{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "CS464_HW3_Spring20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulHamidDabboussi/Ocular-Disease-Recognition/blob/master/CS464_HW3_Spring20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5yghM_BMmkQ",
        "colab_type": "text"
      },
      "source": [
        "<h1><center>CS 464</center></h1>\n",
        "<h1><center>Introduction to Machine Learning</center></h1>\n",
        "<h1><center>Spring 2020</center></h1>\n",
        "<h1><center>Homework 3</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds6L6MMXMmkR",
        "colab_type": "text"
      },
      "source": [
        "<h3><center>Due: May 19, 2020 23:55 (GMT+3)</center></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCsDGpxqMmkT",
        "colab_type": "text"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "<ul>\n",
        "    <li>\n",
        "    This homework contains both written and programming questions about neural networks. You should implement programming questions on this notebook. Each programming question has its own cell for your answer. You can implement your code directly in these cells, or you can call required functions which are defined in a different location for the given question. <b>Any other programming enviroment will NOT be accepted.</b>\n",
        "    </li>\n",
        "    <li>\n",
        "    For questions that you need to plot, your plot results have to be included in both cell output. For written questions, you may provide them either as comments in code cells or as seperate text cells. \n",
        "    </li>\n",
        "    <li>\n",
        "    You are <b>ONLY ALLOWED</b> to use libraries given below:\n",
        "        <ul>\n",
        "        <i>>google.colab.drive</i><br>\n",
        "         <i>>pandas</i><br>\n",
        "         <i>>numpy</i><br>\n",
        "         <i>>libraries included in Python standard library (time, os, sys etc.)</i><br>\n",
        "         <i>>libraries included in PyTorch framework (torch, torchvision etc.)</i><br>\n",
        "         <i>>PIL.Image</i><br>\n",
        "         <i>>matplotlib</i>\n",
        "        </ul>\n",
        "    </li>\n",
        "    <li>\n",
        "    It is <b>NOT ALLOWED</b> to use a different deep learning framework than PyTorch.\n",
        "    </li>\n",
        "    <li>\n",
        "    You will submit only a single compressed file for this homework. Compress your notebook(\".ipynb\") and model (\".pth\") files as a gzipped TAR file or a ZIP file with the name CS464_HW3_Section#_Firstname_Lastname. Do not use any Turkish letters for any of your files including code files and model files. Upload your homework to the related section on Moodle.\n",
        "    </li>\n",
        "    <li>\n",
        "    This is an individual assignment for each student. That is, you are NOT allowed to share your workwith your classmates.</li>\n",
        "    <li> \n",
        "    If you do not follow the submission routes, deadlines and specifications, it will lead to a significant grade deduction.\n",
        "    </li>\n",
        "    <li> \n",
        "    For any question regarding this assignment, contact <b>ilayda.beyreli@bilkent.edu.tr</b>.\n",
        "    </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE7TyUOpMmkU",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDBMnGMFlc50",
        "colab_type": "text"
      },
      "source": [
        "You may use both anaconda or pip to install PyTorch to your own computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wbzHKKbMmkV",
        "colab_type": "text"
      },
      "source": [
        "### Anaconda Installation\n",
        "\n",
        "<ul>\n",
        "    <li>Download anaconda from https://www.anaconda.com/download</li>\n",
        "    <li>Follow the instructions provided in https://conda.io/docs/user-guide/install/index.html#regular-installation</li>\n",
        "</ul>\n",
        "\n",
        "#### Creation of Virtual Environment\n",
        "\n",
        "<ul>\n",
        "    <li>Create python3.7 virtual environment for your hw3 using follow command from the command line<br>\n",
        "        <i>> conda create -n HW3 python=3.7 anaconda</i></li>\n",
        "    <li>Activate your virtual environment<br>\n",
        "        <i>> source activate HW3</i></li>\n",
        "    <li>When you create your virtual environment with \"anaconda\" metapackage, jupyter notebook should be installed. Try:<br>\n",
        "         <i>> jupyter notebook</i>\n",
        "</ul>\n",
        "\n",
        "\n",
        "#### Pytorch Installation with Anaconda\n",
        "\n",
        "You should install PyTorch to your virtual environment which is created for the hw3. Therefore, you should activate your homework virtual environment before to start PyTorch installation.\n",
        "<li>> source activate HW3</li>\n",
        "\n",
        "After you have activated the virtual environment, then use one of the following commands to install pytorch for CPU for your system. See https://pytorch.org/ for help.\n",
        "<ul>\n",
        "<li>For MacOS:<br>\n",
        "    <i>> conda install pytorch torchvision -c pytorch</i>\n",
        "</li>\n",
        "<li>For Linux:<br>\n",
        "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i>\n",
        "</li>\n",
        "<li>For Windows:<br>\n",
        "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i><br>\n",
        "</li></ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oV6Va_SmV5i",
        "colab_type": "text"
      },
      "source": [
        "###Pip3 Installation\n",
        "<ul>\n",
        "    <li>Download pip3 from https://pip.pypa.io/en/stable/installing/</li>\n",
        "    <li>If you are using Windows, you may need to add Python to your enviroment variables. You may use the following tutorial to install Python and pip.\n",
        "    https://phoenixnap.com/kb/how-to-install-python-3-windows</li>\n",
        "</ul>\n",
        "\n",
        "#### PyTorch Installation with Pip\n",
        "<ul>\n",
        "<li>For MacOS:<br>\n",
        "    <i>> pip3 install torch torchvision</i>\n",
        "</li>\n",
        "<li>For Linux:<br>\n",
        "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i>\n",
        "</li>\n",
        "<li>For Windows:<br>\n",
        "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i><br>\n",
        "</li>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGRjkX0pMmkY",
        "colab_type": "text"
      },
      "source": [
        "## Question 1 Decison Trees [30 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BcwAjaoMmkZ",
        "colab_type": "text"
      },
      "source": [
        "![Dataset](https://drive.google.com/uc?export=view&id=1k6-cF0zIJ2TFtey1WZ8Y5wevWcDtKckd)\n",
        "\n",
        "  Consider the dataset shown above. You will train a binary decision tree using information gain as the splitting criteria. Consider the following stopping criteria (ie. early pruning criteria): If the entropy of a node is below a predefined threshold, $T$, stop splitting that node, and set it as a leaf.<br>\n",
        "\n",
        " Reminder: You are not allowed to put scanned images. Hence, you need to use drawing tools such as draw.io, Paint, Microsoft Powerpoint etc.<br>\n",
        "\n",
        "**a) [10 pts]** Draw the decision tree that is trained on this dataset without any pruning (i.e. $T=0$). Use <b>ID3 algorithm</b> and <b>entropy</b> as your impurity measure to construct your tree. Show your calculation for each split decision and justify your answer clearly.<br>\n",
        "\n",
        "\n",
        "**b) [5 pts]** Draw the decision boundaries resulting from the decision tree you have drawn for part a.<br>\n",
        "\n",
        "![image](https://drive.google.com/open?id=1xJVVjWN_UbywOPDzpATFs6m_D-8Vjd9p)\n",
        "\n",
        "Image is not showing but the link will take you to the image.\n",
        "\n",
        "**c) [5 pts]** Assume we have a very large balanced dataset with 2 classes. Draw a hypothetical plot, which shows training and test errors (Y-axis) as $T$ changes from 1 to 0 (X-axis). Explain your rationale explicitly.<br>\n",
        "\n",
        "![Image](https://drive.google.com/open?id=1yW9hVKt0w8dsdXeQpgyUcf2A7cf3-e-0)\n",
        "\n",
        "**d) [5 pts]** Is ID3 algorithm optimal? If yes, provide an intuition of optimality. If not, explain the reason explicitly.<br>\n",
        "\n",
        "No, ID3 does not garauntee an optimal solution as it might converge on a local optimum due to it's selection of a greedy choice, which is not always the most optimal.\n",
        "\n",
        "**e) [5 pts]** If you used Gini Index as your impurity measure instead of entropy, would you obtain the same tree as in part a? Explain your reasoning clearly.<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmN0va8GMmka",
        "colab_type": "text"
      },
      "source": [
        "## Question 2 [70 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3mspgJlMmkb",
        "colab_type": "text"
      },
      "source": [
        "In this question you are asked to perform binary classification on Ocular Disease Recognition, ODIR5k, dataset. First, you will implement a three-layer neural network, and then a 3 layer convolutional neural network (CNN) to classify retinal images of 3500 patiens as either \"normal\" or \"abnormal/disease\".<br><br>\n",
        "The dataset has been preprocessed in such a way that the right and the left retinal images from the same patient are combined, downsized and stored as H:128 X W:256 RGB images. The label for each patient is also given to you in another .xlsx file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgzLrMkW8xxU",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Multi Layer Perceptron (MLP) [30 pts]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8ngH8qolz1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3316b000-13cd-463a-fd66-39a4e2a059b1"
      },
      "source": [
        "!pip install torch torchvision\n",
        "import os\n",
        "# os._exit(00)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iPPPGOCgrrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import glob\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGq8mbeminFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd8237f2-5523-415d-a7b3-4af6c5553f7e"
      },
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWjH22A7fOlf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "2d1d6607-eeef-4086-ca6c-9b6e753ddcb6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive/My\\ Drive/Bilkent/Year\\ 3/CS464/hw3\n",
        "\n",
        "!unzip '/gdrive/My Drive/Bilkent/Year 3/CS464/hw3/data.zip' -d '/temp'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "data.zip  labels.xlsx\n",
            "Archive:  /gdrive/My Drive/Bilkent/Year 3/CS464/hw3/data.zip\n",
            "replace /temp/data/images/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7HMeC2puxA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = '/gdrive/My Drive/Bilkent/Year 3/CS464/hw3'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yzlbHxVMmkc",
        "colab_type": "text"
      },
      "source": [
        "#### Data Loader [7pts]\n",
        "\n",
        "An important part of such a task is to implement your own data loader. In this homework, a partial loader is provided to you. This loader is going to be based on a base class named \"Dataset\", provided in PyTorch library. You need to complete the code below to create your custom \"OcularDataset\" class which will be able to load your dataset. <br><br>\n",
        "Implement the functions whose proptotypes are given. Follow the TODO notes below. You have to divide the files into three sets as <b>train (5/7)</b>, <b>validation (1/7)</b> and **test (1/7)** sets.  These non-overlapping splits, which are subsets of OcularDataset, should be retrieved using the \"get_dataset\" function. Here, you are also supposed to flatten the image into a vector (also to grayscale) to be compatible with MLP. Note that the pixel values also needs to be normalized to [0,1] range.\n",
        "<br><br>\n",
        "\n",
        "Hint: The dataset is not normalized and your results will heavily depend on your input.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKA_ylTx6SVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OcularDataset(Dataset):\n",
        "    \n",
        "    # TODO:\n",
        "    # Define constructor for OcularDataset class\n",
        "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "\n",
        "        \n",
        "    '''This function should return sample count in the dataset'''\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
        "    def __getitem__(self, index):\n",
        "        _x = self.data[index,:-1]\n",
        "        _y = self.data[index, -1]\n",
        "        return _x, _y\n",
        "\n",
        "def normalize(column):\n",
        "        # mean = np.mean(column)\n",
        "        # std = np.std(column)\n",
        "        # if std == 0.0:\n",
        "        #     return column\n",
        "        # return (column - mean) / std\n",
        "        return np.divide((column - np.min(column)),(np.max(column - np.min(column))))\n",
        "\n",
        "def get_dataset(root):\n",
        "    # TODO: \n",
        "    # Read dataset files\n",
        "    # Construct training, validation and test sets\n",
        "    # Normalize & flatten datasets\n",
        "\n",
        "    labels_path = os.path.join(root, 'labels.xlsx') \n",
        "    labels = pd.read_excel(labels_path)\n",
        "    y = np.array(labels[1])\n",
        "    y = y.reshape(3500, -1)\n",
        "\n",
        "    images = []\n",
        "    # image_path = (os.path.join(root, 'data/images/*.jpg'))\n",
        "    image_path = '/temp/data/images/*.jpg'\n",
        "    \n",
        "    regex = re.compile(r'\\d+')\n",
        "    for im in glob.iglob(image_path):\n",
        "        num = int(regex.search(im).group(0))\n",
        "        images.append(np.append((np.asarray(Image.open(im).convert(\"L\"))).flatten(), num))\n",
        "        \n",
        "    images = np.asarray(images)\n",
        "    images = images[images[:,-1].argsort()][:,:-1]\n",
        "    images = np.apply_along_axis(normalize, 0, images)\n",
        "    data = np.append(images, y, axis=1)\n",
        "\n",
        "    #np.random.seed(42)\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    train_set, val_set, test_set = np.array_split(data, [int(data.shape[0]*(5/7)), int(data.shape[0]*(5/7.0)+data.shape[0]*(1/7))])\n",
        "\n",
        "\n",
        "\n",
        "    train_set = OcularDataset(train_set)\n",
        "    val_set = OcularDataset(val_set)\n",
        "    test_set = OcularDataset(test_set)\n",
        "\n",
        "    #return train_dataset, val_dataset, test_dataset\n",
        "    return train_set, val_set, test_set\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkDmMyZ4UyoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set, val_set, test_set = get_dataset(root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMfyonGRMmke",
        "colab_type": "text"
      },
      "source": [
        "#### Neural Network [4 pts]\n",
        "\n",
        "Now, implement your three hidden layer neural network. FNet class will represent your neural network. The layer descriptions are as follows:<ul>\n",
        "    <i>> Input layer will have ReLU activation. You should decide the number of input neurons.</i><br>\n",
        "    <i>> First hidden layer will have 1024 neuros with ReLU activation </i><br>\n",
        "    <i>> Second hidden layer will have 256 neuros with ReLU activation </i><br>\n",
        "    <i> You should decide the number of output neurons and pick a proper activation function for the output layer. </i><br>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adM6pBIp6cIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FNet(nn.Module):\n",
        "    '''Define your neural network'''\n",
        "    def __init__(self, **kwargs): \n",
        "    # you can add any additional parameters you want \n",
        "    # TODO:\n",
        "    # You should create your neural network here\n",
        "        super(FNet, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(32768, 1024), #input\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256), #hidden 1\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 32), #hidden 2\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1))\n",
        "        #no activation function here because BCEWithLogitsLoss is used as loss function. Sigmoid is applied later.\n",
        "    \n",
        "    def forward(self, X): \n",
        "    # you can add any additional parameters you want\n",
        "    # TODO:\n",
        "    # Forward propagation implementation should be here\n",
        "        return self.network(X) \n",
        "    #simple forward code because sequence was specified above using nn.Sequential\n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4-GhJe0Mmkf",
        "colab_type": "text"
      },
      "source": [
        "#### Training [10 pts]\n",
        "\n",
        "Complete the code snippet below to train your network. You need to carefully select the appropriate loss function and tune hyper-parameters. Use SGD optimizer for this question.<br>\n",
        "So far, you should have created three dataset splits for train, validation and test. You will need to load these splits at this phase. Make sure that you shuffle the samples in the training split. Save training loss and training accuracy of each iteration (each batch) and also save validation loss and accuracy at each epoch to use them in the next part for plotting.<br>\n",
        "Your model is going to run upto at most 100 epochs. Pick the best model so far as your final model and save this model as a \".pth\" file. <br>\n",
        "Note that the best accuracy does not always imply the best model. Try to track losses instead of accuracies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlkS5jVR6kNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HINT: note that your training time should not take many days.\n",
        "\n",
        "#TODO:\n",
        "#Pick your hyper parameters\n",
        "train_batch = 256\n",
        "val_batch = 128\n",
        "test_batch = 128\n",
        "max_epoch = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 256 128 0.005 -> 0.37\n",
        "# 256 128 0.01 -> 0.69\n",
        "# 256 128 0.01 -> 0.69 better\n",
        "\n",
        "\n",
        "#use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_acc = []\n",
        "train_loss = []\n",
        "val_acc = []\n",
        "val_loss = []\n",
        "best_path = 'FNet.pth'\n",
        "\n",
        "def main(): # you are free to change parameters\n",
        "\n",
        "    # Create train dataset loader\n",
        "    train_loader = DataLoader(train_set, batch_size=train_batch, shuffle=True)\n",
        "    # Create validation dataset loader\n",
        "    val_loader = DataLoader(val_set, batch_size=val_batch)\n",
        "    # Create test dataset loader\n",
        "    test_loader = DataLoader(test_set, test_batch)\n",
        "    # initialize your GENet neural network\n",
        "    model = FNet()\n",
        "    model.to(device)\n",
        "\n",
        "    # define your loss function\n",
        "    loss = nn.BCEWithLogitsLoss().to(device)\n",
        "    \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
        "    # start training\n",
        "    # for each epoch calculate validation performance\n",
        "    # save best model according to validation performance\n",
        "    best_loss = 10\n",
        "    \n",
        "\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "       train_a, train_l = train(epoch, model, loss, optimizer, train_loader)\n",
        "       train_acc.append(train_a)\n",
        "       train_loss.append(train_l)\n",
        "       val_a, val_l = test(model, val_loader, loss)\n",
        "       val_acc.append(val_a)\n",
        "       val_loss.append(val_l)\n",
        "       if val_l < best_loss: #minimize loss\n",
        "          torch.save(model.state_dict(), best_path)\n",
        "          best_loss = val_l\n",
        "    print(best_loss)\n",
        "    \n",
        "''' Train your network for a one epoch '''\n",
        "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    end = time.time()\n",
        "    for batch_idx, (data, labels) in enumerate(loader):\n",
        "        # TODO:\n",
        "        # Implement training code for a one iteration\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data.float())\n",
        "        # output = torch.round(output)\n",
        "        # output = torch.flatten(output)\n",
        "\n",
        "        loss = criterion(output, labels.unsqueeze(1))\n",
        "        \n",
        "        output = torch.round(torch.sigmoid(output))\n",
        "        output = torch.flatten(output)\n",
        "        prec = accuracy(output, labels)\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        accuracies.update(prec, data.size(0))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "            'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
        "            epoch + 1, batch_idx + 1, len(loader), \n",
        "            batch_time=batch_time,\n",
        "            data_time=data_time, \n",
        "            loss=losses,\n",
        "            acc=accuracies))\n",
        "        \n",
        "    return accuracies.avg, losses.avg\n",
        "        \n",
        "\n",
        "''' Test&Validate your network '''\n",
        "def test(model, loader, criterion): # you are free to change parameters\n",
        "    batch_time = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        end = time.time()\n",
        "        for batch_idx, (data, labels) in enumerate(loader):\n",
        "            # TODO:\n",
        "            # Implement test code\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            output = model(data.float())\n",
        "            # print(output[:10])\n",
        "            # output = torch.round(output)\n",
        "            # output = torch.flatten(output)\n",
        "            # print(output[:10])\n",
        "\n",
        "            loss = criterion(output, labels.unsqueeze(1)) \n",
        "            output = torch.round(torch.sigmoid(output))\n",
        "            output = torch.flatten(output)  \n",
        "            prec = accuracy(output, labels)\n",
        "            losses.update(loss.item(), data.size(0))\n",
        "            accuracies.update(prec, data.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            \n",
        "        print('Time {batch_time.avg:.3f}\\t'\n",
        "              'Accu {acc.avg:.4f}\\t'\n",
        "              'Loss {loss.avg:.4f}\\t'.format(\n",
        "               batch_time=batch_time, \n",
        "               acc=accuracies,\n",
        "               loss=losses))\n",
        "        \n",
        "    return accuracies.avg, losses.avg\n",
        "\n",
        "\"\"\"Class from pytorch example code\"\"\"        \n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\"\"\" function from pytorch example code\"\"\"\n",
        "def accuracy(preds, labels):\n",
        "    return float((preds==labels).sum())/labels.shape[0]\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_RCjYvCxqtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42e3ab7b-17dc-4af6-9bce-aec19fa76c80"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1][1/10]\tTime 0.336 (0.336)\tData 0.1639 (0.1639)\tLoss 0.6776 (0.6776)\tAccu 0.7266 (0.7266)\t\n",
            "Epoch: [1][2/10]\tTime 0.158 (0.247)\tData 0.1228 (0.1433)\tLoss 0.6744 (0.6760)\tAccu 0.7227 (0.7246)\t\n",
            "Epoch: [1][3/10]\tTime 0.157 (0.217)\tData 0.1269 (0.1379)\tLoss 0.6742 (0.6754)\tAccu 0.6641 (0.7044)\t\n",
            "Epoch: [1][4/10]\tTime 0.158 (0.202)\tData 0.1270 (0.1352)\tLoss 0.6671 (0.6733)\tAccu 0.6797 (0.6982)\t\n",
            "Epoch: [1][5/10]\tTime 0.160 (0.194)\tData 0.1293 (0.1340)\tLoss 0.6582 (0.6703)\tAccu 0.6875 (0.6961)\t\n",
            "Epoch: [1][6/10]\tTime 0.154 (0.187)\tData 0.1239 (0.1323)\tLoss 0.6541 (0.6676)\tAccu 0.6797 (0.6934)\t\n",
            "Epoch: [1][7/10]\tTime 0.158 (0.183)\tData 0.1276 (0.1316)\tLoss 0.6553 (0.6658)\tAccu 0.6523 (0.6875)\t\n",
            "Epoch: [1][8/10]\tTime 0.154 (0.179)\tData 0.1243 (0.1307)\tLoss 0.6565 (0.6647)\tAccu 0.6328 (0.6807)\t\n",
            "Epoch: [1][9/10]\tTime 0.159 (0.177)\tData 0.1288 (0.1305)\tLoss 0.6497 (0.6630)\tAccu 0.6367 (0.6758)\t\n",
            "Epoch: [1][10/10]\tTime 0.123 (0.172)\tData 0.0943 (0.1269)\tLoss 0.6372 (0.6610)\tAccu 0.6633 (0.6748)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6237\t\n",
            "Epoch: [2][1/10]\tTime 0.204 (0.204)\tData 0.1735 (0.1735)\tLoss 0.6145 (0.6145)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [2][2/10]\tTime 0.158 (0.181)\tData 0.1276 (0.1505)\tLoss 0.6535 (0.6340)\tAccu 0.6211 (0.6582)\t\n",
            "Epoch: [2][3/10]\tTime 0.158 (0.173)\tData 0.1271 (0.1427)\tLoss 0.6060 (0.6246)\tAccu 0.7031 (0.6732)\t\n",
            "Epoch: [2][4/10]\tTime 0.162 (0.170)\tData 0.1318 (0.1400)\tLoss 0.5901 (0.6160)\tAccu 0.7266 (0.6865)\t\n",
            "Epoch: [2][5/10]\tTime 0.165 (0.169)\tData 0.1351 (0.1390)\tLoss 0.6366 (0.6201)\tAccu 0.6562 (0.6805)\t\n",
            "Epoch: [2][6/10]\tTime 0.152 (0.166)\tData 0.1212 (0.1361)\tLoss 0.6030 (0.6173)\tAccu 0.6992 (0.6836)\t\n",
            "Epoch: [2][7/10]\tTime 0.157 (0.165)\tData 0.1274 (0.1348)\tLoss 0.6513 (0.6221)\tAccu 0.6523 (0.6791)\t\n",
            "Epoch: [2][8/10]\tTime 0.155 (0.164)\tData 0.1238 (0.1334)\tLoss 0.6356 (0.6238)\tAccu 0.6719 (0.6782)\t\n",
            "Epoch: [2][9/10]\tTime 0.157 (0.163)\tData 0.1272 (0.1327)\tLoss 0.6505 (0.6268)\tAccu 0.6562 (0.6758)\t\n",
            "Epoch: [2][10/10]\tTime 0.126 (0.159)\tData 0.0955 (0.1290)\tLoss 0.6575 (0.6292)\tAccu 0.6633 (0.6748)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6226\t\n",
            "Epoch: [3][1/10]\tTime 0.163 (0.163)\tData 0.1323 (0.1323)\tLoss 0.6616 (0.6616)\tAccu 0.6445 (0.6445)\t\n",
            "Epoch: [3][2/10]\tTime 0.148 (0.156)\tData 0.1188 (0.1256)\tLoss 0.6032 (0.6324)\tAccu 0.6875 (0.6660)\t\n",
            "Epoch: [3][3/10]\tTime 0.160 (0.157)\tData 0.1294 (0.1268)\tLoss 0.6005 (0.6218)\tAccu 0.6992 (0.6771)\t\n",
            "Epoch: [3][4/10]\tTime 0.154 (0.156)\tData 0.1243 (0.1262)\tLoss 0.6309 (0.6241)\tAccu 0.6602 (0.6729)\t\n",
            "Epoch: [3][5/10]\tTime 0.158 (0.157)\tData 0.1267 (0.1263)\tLoss 0.6515 (0.6296)\tAccu 0.6445 (0.6672)\t\n",
            "Epoch: [3][6/10]\tTime 0.160 (0.157)\tData 0.1296 (0.1269)\tLoss 0.6115 (0.6266)\tAccu 0.6914 (0.6712)\t\n",
            "Epoch: [3][7/10]\tTime 0.158 (0.157)\tData 0.1273 (0.1269)\tLoss 0.6191 (0.6255)\tAccu 0.6758 (0.6719)\t\n",
            "Epoch: [3][8/10]\tTime 0.155 (0.157)\tData 0.1251 (0.1267)\tLoss 0.6125 (0.6239)\tAccu 0.7031 (0.6758)\t\n",
            "Epoch: [3][9/10]\tTime 0.163 (0.158)\tData 0.1322 (0.1273)\tLoss 0.6216 (0.6236)\tAccu 0.6875 (0.6771)\t\n",
            "Epoch: [3][10/10]\tTime 0.126 (0.154)\tData 0.0966 (0.1242)\tLoss 0.6509 (0.6257)\tAccu 0.6480 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6219\t\n",
            "Epoch: [4][1/10]\tTime 0.155 (0.155)\tData 0.1253 (0.1253)\tLoss 0.6317 (0.6317)\tAccu 0.6562 (0.6562)\t\n",
            "Epoch: [4][2/10]\tTime 0.158 (0.157)\tData 0.1282 (0.1267)\tLoss 0.6285 (0.6301)\tAccu 0.6758 (0.6660)\t\n",
            "Epoch: [4][3/10]\tTime 0.160 (0.158)\tData 0.1291 (0.1275)\tLoss 0.6178 (0.6260)\tAccu 0.6836 (0.6719)\t\n",
            "Epoch: [4][4/10]\tTime 0.151 (0.156)\tData 0.1215 (0.1260)\tLoss 0.6237 (0.6254)\tAccu 0.6758 (0.6729)\t\n",
            "Epoch: [4][5/10]\tTime 0.165 (0.158)\tData 0.1352 (0.1279)\tLoss 0.6566 (0.6316)\tAccu 0.6250 (0.6633)\t\n",
            "Epoch: [4][6/10]\tTime 0.166 (0.159)\tData 0.1358 (0.1292)\tLoss 0.6280 (0.6310)\tAccu 0.6719 (0.6647)\t\n",
            "Epoch: [4][7/10]\tTime 0.164 (0.160)\tData 0.1339 (0.1299)\tLoss 0.6389 (0.6322)\tAccu 0.6680 (0.6652)\t\n",
            "Epoch: [4][8/10]\tTime 0.156 (0.160)\tData 0.1259 (0.1294)\tLoss 0.5976 (0.6278)\tAccu 0.7227 (0.6724)\t\n",
            "Epoch: [4][9/10]\tTime 0.159 (0.160)\tData 0.1295 (0.1294)\tLoss 0.5993 (0.6247)\tAccu 0.7070 (0.6762)\t\n",
            "Epoch: [4][10/10]\tTime 0.133 (0.157)\tData 0.1020 (0.1266)\tLoss 0.6338 (0.6254)\tAccu 0.6582 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6197\t\n",
            "Epoch: [5][1/10]\tTime 0.153 (0.153)\tData 0.1229 (0.1229)\tLoss 0.6114 (0.6114)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [5][2/10]\tTime 0.189 (0.171)\tData 0.1579 (0.1404)\tLoss 0.6533 (0.6323)\tAccu 0.6367 (0.6660)\t\n",
            "Epoch: [5][3/10]\tTime 0.161 (0.168)\tData 0.1310 (0.1373)\tLoss 0.6144 (0.6263)\tAccu 0.6758 (0.6693)\t\n",
            "Epoch: [5][4/10]\tTime 0.157 (0.165)\tData 0.1252 (0.1343)\tLoss 0.5962 (0.6188)\tAccu 0.7070 (0.6787)\t\n",
            "Epoch: [5][5/10]\tTime 0.158 (0.164)\tData 0.1280 (0.1330)\tLoss 0.6054 (0.6161)\tAccu 0.6914 (0.6813)\t\n",
            "Epoch: [5][6/10]\tTime 0.164 (0.164)\tData 0.1324 (0.1329)\tLoss 0.6479 (0.6214)\tAccu 0.6523 (0.6764)\t\n",
            "Epoch: [5][7/10]\tTime 0.158 (0.163)\tData 0.1282 (0.1322)\tLoss 0.6174 (0.6208)\tAccu 0.6836 (0.6775)\t\n",
            "Epoch: [5][8/10]\tTime 0.154 (0.162)\tData 0.1244 (0.1313)\tLoss 0.6399 (0.6232)\tAccu 0.6641 (0.6758)\t\n",
            "Epoch: [5][9/10]\tTime 0.160 (0.162)\tData 0.1294 (0.1311)\tLoss 0.6579 (0.6271)\tAccu 0.6445 (0.6723)\t\n",
            "Epoch: [5][10/10]\tTime 0.128 (0.158)\tData 0.0989 (0.1278)\tLoss 0.5994 (0.6249)\tAccu 0.7041 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6195\t\n",
            "Epoch: [6][1/10]\tTime 0.154 (0.154)\tData 0.1236 (0.1236)\tLoss 0.6014 (0.6014)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [6][2/10]\tTime 0.152 (0.153)\tData 0.1219 (0.1228)\tLoss 0.6284 (0.6149)\tAccu 0.6680 (0.6836)\t\n",
            "Epoch: [6][3/10]\tTime 0.156 (0.154)\tData 0.1257 (0.1237)\tLoss 0.6443 (0.6247)\tAccu 0.6484 (0.6719)\t\n",
            "Epoch: [6][4/10]\tTime 0.151 (0.153)\tData 0.1209 (0.1230)\tLoss 0.6102 (0.6211)\tAccu 0.7031 (0.6797)\t\n",
            "Epoch: [6][5/10]\tTime 0.156 (0.154)\tData 0.1257 (0.1236)\tLoss 0.6084 (0.6185)\tAccu 0.6992 (0.6836)\t\n",
            "Epoch: [6][6/10]\tTime 0.156 (0.154)\tData 0.1248 (0.1238)\tLoss 0.6322 (0.6208)\tAccu 0.6602 (0.6797)\t\n",
            "Epoch: [6][7/10]\tTime 0.154 (0.154)\tData 0.1244 (0.1239)\tLoss 0.6366 (0.6231)\tAccu 0.6562 (0.6763)\t\n",
            "Epoch: [6][8/10]\tTime 0.153 (0.154)\tData 0.1221 (0.1236)\tLoss 0.6481 (0.6262)\tAccu 0.6367 (0.6714)\t\n",
            "Epoch: [6][9/10]\tTime 0.155 (0.154)\tData 0.1254 (0.1238)\tLoss 0.6070 (0.6241)\tAccu 0.6992 (0.6745)\t\n",
            "Epoch: [6][10/10]\tTime 0.127 (0.151)\tData 0.0978 (0.1212)\tLoss 0.6253 (0.6241)\tAccu 0.6786 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6198\t\n",
            "Epoch: [7][1/10]\tTime 0.151 (0.151)\tData 0.1216 (0.1216)\tLoss 0.6251 (0.6251)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [7][2/10]\tTime 0.167 (0.159)\tData 0.1364 (0.1290)\tLoss 0.6295 (0.6273)\tAccu 0.6641 (0.6680)\t\n",
            "Epoch: [7][3/10]\tTime 0.151 (0.156)\tData 0.1213 (0.1264)\tLoss 0.6066 (0.6204)\tAccu 0.7031 (0.6797)\t\n",
            "Epoch: [7][4/10]\tTime 0.153 (0.155)\tData 0.1238 (0.1258)\tLoss 0.6285 (0.6224)\tAccu 0.6641 (0.6758)\t\n",
            "Epoch: [7][5/10]\tTime 0.151 (0.155)\tData 0.1214 (0.1249)\tLoss 0.5976 (0.6175)\tAccu 0.7148 (0.6836)\t\n",
            "Epoch: [7][6/10]\tTime 0.150 (0.154)\tData 0.1201 (0.1241)\tLoss 0.6235 (0.6185)\tAccu 0.6680 (0.6810)\t\n",
            "Epoch: [7][7/10]\tTime 0.152 (0.154)\tData 0.1225 (0.1239)\tLoss 0.6336 (0.6206)\tAccu 0.6680 (0.6791)\t\n",
            "Epoch: [7][8/10]\tTime 0.152 (0.153)\tData 0.1209 (0.1235)\tLoss 0.6248 (0.6212)\tAccu 0.6719 (0.6782)\t\n",
            "Epoch: [7][9/10]\tTime 0.158 (0.154)\tData 0.1284 (0.1241)\tLoss 0.6359 (0.6228)\tAccu 0.6719 (0.6775)\t\n",
            "Epoch: [7][10/10]\tTime 0.121 (0.151)\tData 0.0921 (0.1209)\tLoss 0.6423 (0.6243)\tAccu 0.6429 (0.6748)\t\n",
            "Time 0.042\tAccu 0.6800\tLoss 0.6192\t\n",
            "Epoch: [8][1/10]\tTime 0.158 (0.158)\tData 0.1288 (0.1288)\tLoss 0.6508 (0.6508)\tAccu 0.6445 (0.6445)\t\n",
            "Epoch: [8][2/10]\tTime 0.147 (0.153)\tData 0.1167 (0.1227)\tLoss 0.6210 (0.6359)\tAccu 0.6680 (0.6562)\t\n",
            "Epoch: [8][3/10]\tTime 0.176 (0.161)\tData 0.1425 (0.1293)\tLoss 0.6769 (0.6496)\tAccu 0.6055 (0.6393)\t\n",
            "Epoch: [8][4/10]\tTime 0.156 (0.159)\tData 0.1256 (0.1284)\tLoss 0.6315 (0.6451)\tAccu 0.6680 (0.6465)\t\n",
            "Epoch: [8][5/10]\tTime 0.156 (0.159)\tData 0.1267 (0.1280)\tLoss 0.5844 (0.6329)\tAccu 0.7266 (0.6625)\t\n",
            "Epoch: [8][6/10]\tTime 0.154 (0.158)\tData 0.1234 (0.1273)\tLoss 0.6025 (0.6279)\tAccu 0.7148 (0.6712)\t\n",
            "Epoch: [8][7/10]\tTime 0.156 (0.158)\tData 0.1256 (0.1270)\tLoss 0.6384 (0.6294)\tAccu 0.6406 (0.6669)\t\n",
            "Epoch: [8][8/10]\tTime 0.154 (0.157)\tData 0.1233 (0.1266)\tLoss 0.6337 (0.6299)\tAccu 0.6680 (0.6670)\t\n",
            "Epoch: [8][9/10]\tTime 0.159 (0.157)\tData 0.1296 (0.1269)\tLoss 0.5854 (0.6250)\tAccu 0.7266 (0.6736)\t\n",
            "Epoch: [8][10/10]\tTime 0.124 (0.154)\tData 0.0949 (0.1237)\tLoss 0.6184 (0.6245)\tAccu 0.6888 (0.6748)\t\n",
            "Time 0.042\tAccu 0.6800\tLoss 0.6193\t\n",
            "Epoch: [9][1/10]\tTime 0.155 (0.155)\tData 0.1253 (0.1253)\tLoss 0.5939 (0.5939)\tAccu 0.7188 (0.7188)\t\n",
            "Epoch: [9][2/10]\tTime 0.157 (0.156)\tData 0.1261 (0.1257)\tLoss 0.6183 (0.6061)\tAccu 0.6758 (0.6973)\t\n",
            "Epoch: [9][3/10]\tTime 0.155 (0.156)\tData 0.1256 (0.1257)\tLoss 0.6216 (0.6113)\tAccu 0.6758 (0.6901)\t\n",
            "Epoch: [9][4/10]\tTime 0.155 (0.156)\tData 0.1246 (0.1254)\tLoss 0.6409 (0.6187)\tAccu 0.6562 (0.6816)\t\n",
            "Epoch: [9][5/10]\tTime 0.160 (0.156)\tData 0.1280 (0.1259)\tLoss 0.6108 (0.6171)\tAccu 0.6875 (0.6828)\t\n",
            "Epoch: [9][6/10]\tTime 0.153 (0.156)\tData 0.1233 (0.1255)\tLoss 0.6376 (0.6205)\tAccu 0.6797 (0.6823)\t\n",
            "Epoch: [9][7/10]\tTime 0.152 (0.155)\tData 0.1227 (0.1251)\tLoss 0.6362 (0.6228)\tAccu 0.6562 (0.6786)\t\n",
            "Epoch: [9][8/10]\tTime 0.158 (0.156)\tData 0.1276 (0.1254)\tLoss 0.6056 (0.6206)\tAccu 0.6953 (0.6807)\t\n",
            "Epoch: [9][9/10]\tTime 0.160 (0.156)\tData 0.1301 (0.1259)\tLoss 0.6576 (0.6247)\tAccu 0.6328 (0.6753)\t\n",
            "Epoch: [9][10/10]\tTime 0.129 (0.153)\tData 0.0990 (0.1232)\tLoss 0.6190 (0.6243)\tAccu 0.6684 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6189\t\n",
            "Epoch: [10][1/10]\tTime 0.156 (0.156)\tData 0.1262 (0.1262)\tLoss 0.5967 (0.5967)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [10][2/10]\tTime 0.153 (0.155)\tData 0.1229 (0.1245)\tLoss 0.6484 (0.6225)\tAccu 0.6445 (0.6797)\t\n",
            "Epoch: [10][3/10]\tTime 0.163 (0.158)\tData 0.1330 (0.1274)\tLoss 0.6402 (0.6284)\tAccu 0.6641 (0.6745)\t\n",
            "Epoch: [10][4/10]\tTime 0.163 (0.159)\tData 0.1313 (0.1283)\tLoss 0.5937 (0.6197)\tAccu 0.7227 (0.6865)\t\n",
            "Epoch: [10][5/10]\tTime 0.165 (0.160)\tData 0.1342 (0.1295)\tLoss 0.6259 (0.6210)\tAccu 0.6602 (0.6813)\t\n",
            "Epoch: [10][6/10]\tTime 0.163 (0.161)\tData 0.1329 (0.1301)\tLoss 0.6560 (0.6268)\tAccu 0.6406 (0.6745)\t\n",
            "Epoch: [10][7/10]\tTime 0.158 (0.160)\tData 0.1271 (0.1297)\tLoss 0.6181 (0.6256)\tAccu 0.6680 (0.6735)\t\n",
            "Epoch: [10][8/10]\tTime 0.153 (0.159)\tData 0.1226 (0.1288)\tLoss 0.6345 (0.6267)\tAccu 0.6641 (0.6724)\t\n",
            "Epoch: [10][9/10]\tTime 0.159 (0.159)\tData 0.1289 (0.1288)\tLoss 0.6051 (0.6243)\tAccu 0.6914 (0.6745)\t\n",
            "Epoch: [10][10/10]\tTime 0.127 (0.156)\tData 0.0980 (0.1257)\tLoss 0.6160 (0.6236)\tAccu 0.6786 (0.6748)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6196\t\n",
            "Epoch: [11][1/10]\tTime 0.155 (0.155)\tData 0.1234 (0.1234)\tLoss 0.6239 (0.6239)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [11][2/10]\tTime 0.157 (0.156)\tData 0.1273 (0.1253)\tLoss 0.6440 (0.6340)\tAccu 0.6484 (0.6602)\t\n",
            "Epoch: [11][3/10]\tTime 0.157 (0.156)\tData 0.1270 (0.1259)\tLoss 0.6045 (0.6242)\tAccu 0.7031 (0.6745)\t\n",
            "Epoch: [11][4/10]\tTime 0.156 (0.156)\tData 0.1258 (0.1259)\tLoss 0.6334 (0.6265)\tAccu 0.6680 (0.6729)\t\n",
            "Epoch: [11][5/10]\tTime 0.155 (0.156)\tData 0.1249 (0.1257)\tLoss 0.6267 (0.6265)\tAccu 0.6562 (0.6695)\t\n",
            "Epoch: [11][6/10]\tTime 0.156 (0.156)\tData 0.1262 (0.1258)\tLoss 0.6394 (0.6287)\tAccu 0.6484 (0.6660)\t\n",
            "Epoch: [11][7/10]\tTime 0.152 (0.155)\tData 0.1219 (0.1252)\tLoss 0.6351 (0.6296)\tAccu 0.6641 (0.6657)\t\n",
            "Epoch: [11][8/10]\tTime 0.162 (0.156)\tData 0.1314 (0.1260)\tLoss 0.6202 (0.6284)\tAccu 0.6836 (0.6680)\t\n",
            "Epoch: [11][9/10]\tTime 0.155 (0.156)\tData 0.1251 (0.1259)\tLoss 0.5967 (0.6249)\tAccu 0.7148 (0.6732)\t\n",
            "Epoch: [11][10/10]\tTime 0.124 (0.153)\tData 0.0957 (0.1229)\tLoss 0.6034 (0.6232)\tAccu 0.6939 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6187\t\n",
            "Epoch: [12][1/10]\tTime 0.165 (0.165)\tData 0.1351 (0.1351)\tLoss 0.6198 (0.6198)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [12][2/10]\tTime 0.160 (0.163)\tData 0.1294 (0.1322)\tLoss 0.6095 (0.6147)\tAccu 0.6953 (0.6895)\t\n",
            "Epoch: [12][3/10]\tTime 0.160 (0.162)\tData 0.1289 (0.1311)\tLoss 0.5895 (0.6063)\tAccu 0.6992 (0.6927)\t\n",
            "Epoch: [12][4/10]\tTime 0.159 (0.161)\tData 0.1295 (0.1307)\tLoss 0.6483 (0.6168)\tAccu 0.6641 (0.6855)\t\n",
            "Epoch: [12][5/10]\tTime 0.165 (0.162)\tData 0.1325 (0.1311)\tLoss 0.6279 (0.6190)\tAccu 0.6641 (0.6813)\t\n",
            "Epoch: [12][6/10]\tTime 0.159 (0.161)\tData 0.1282 (0.1306)\tLoss 0.6039 (0.6165)\tAccu 0.7031 (0.6849)\t\n",
            "Epoch: [12][7/10]\tTime 0.160 (0.161)\tData 0.1307 (0.1306)\tLoss 0.6178 (0.6167)\tAccu 0.6797 (0.6842)\t\n",
            "Epoch: [12][8/10]\tTime 0.153 (0.160)\tData 0.1233 (0.1297)\tLoss 0.6580 (0.6219)\tAccu 0.6289 (0.6772)\t\n",
            "Epoch: [12][9/10]\tTime 0.159 (0.160)\tData 0.1292 (0.1297)\tLoss 0.6512 (0.6251)\tAccu 0.6406 (0.6732)\t\n",
            "Epoch: [12][10/10]\tTime 0.125 (0.157)\tData 0.0958 (0.1263)\tLoss 0.6067 (0.6237)\tAccu 0.6939 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6186\t\n",
            "Epoch: [13][1/10]\tTime 0.161 (0.161)\tData 0.1296 (0.1296)\tLoss 0.6261 (0.6261)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [13][2/10]\tTime 0.156 (0.158)\tData 0.1251 (0.1273)\tLoss 0.6291 (0.6276)\tAccu 0.6641 (0.6680)\t\n",
            "Epoch: [13][3/10]\tTime 0.157 (0.158)\tData 0.1251 (0.1266)\tLoss 0.6050 (0.6201)\tAccu 0.7031 (0.6797)\t\n",
            "Epoch: [13][4/10]\tTime 0.155 (0.157)\tData 0.1245 (0.1261)\tLoss 0.5722 (0.6081)\tAccu 0.7344 (0.6934)\t\n",
            "Epoch: [13][5/10]\tTime 0.154 (0.156)\tData 0.1230 (0.1254)\tLoss 0.6435 (0.6152)\tAccu 0.6523 (0.6852)\t\n",
            "Epoch: [13][6/10]\tTime 0.150 (0.155)\tData 0.1201 (0.1246)\tLoss 0.5770 (0.6088)\tAccu 0.7305 (0.6927)\t\n",
            "Epoch: [13][7/10]\tTime 0.153 (0.155)\tData 0.1233 (0.1244)\tLoss 0.6841 (0.6196)\tAccu 0.5977 (0.6791)\t\n",
            "Epoch: [13][8/10]\tTime 0.157 (0.155)\tData 0.1264 (0.1246)\tLoss 0.6018 (0.6173)\tAccu 0.7070 (0.6826)\t\n",
            "Epoch: [13][9/10]\tTime 0.156 (0.155)\tData 0.1257 (0.1247)\tLoss 0.6439 (0.6203)\tAccu 0.6367 (0.6775)\t\n",
            "Epoch: [13][10/10]\tTime 0.134 (0.153)\tData 0.1053 (0.1228)\tLoss 0.6560 (0.6231)\tAccu 0.6429 (0.6748)\t\n",
            "Time 0.042\tAccu 0.6800\tLoss 0.6187\t\n",
            "Epoch: [14][1/10]\tTime 0.155 (0.155)\tData 0.1245 (0.1245)\tLoss 0.6279 (0.6279)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [14][2/10]\tTime 0.156 (0.155)\tData 0.1255 (0.1250)\tLoss 0.6128 (0.6203)\tAccu 0.6836 (0.6777)\t\n",
            "Epoch: [14][3/10]\tTime 0.150 (0.153)\tData 0.1199 (0.1233)\tLoss 0.5753 (0.6053)\tAccu 0.7461 (0.7005)\t\n",
            "Epoch: [14][4/10]\tTime 0.157 (0.154)\tData 0.1269 (0.1242)\tLoss 0.6369 (0.6132)\tAccu 0.6602 (0.6904)\t\n",
            "Epoch: [14][5/10]\tTime 0.153 (0.154)\tData 0.1235 (0.1241)\tLoss 0.6198 (0.6146)\tAccu 0.6797 (0.6883)\t\n",
            "Epoch: [14][6/10]\tTime 0.161 (0.155)\tData 0.1301 (0.1251)\tLoss 0.6063 (0.6132)\tAccu 0.6953 (0.6895)\t\n",
            "Epoch: [14][7/10]\tTime 0.155 (0.155)\tData 0.1246 (0.1250)\tLoss 0.6560 (0.6193)\tAccu 0.6406 (0.6825)\t\n",
            "Epoch: [14][8/10]\tTime 0.163 (0.156)\tData 0.1320 (0.1259)\tLoss 0.6195 (0.6193)\tAccu 0.6641 (0.6802)\t\n",
            "Epoch: [14][9/10]\tTime 0.156 (0.156)\tData 0.1260 (0.1259)\tLoss 0.6125 (0.6186)\tAccu 0.6758 (0.6797)\t\n",
            "Epoch: [14][10/10]\tTime 0.127 (0.153)\tData 0.0965 (0.1229)\tLoss 0.6745 (0.6229)\tAccu 0.6173 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6185\t\n",
            "Epoch: [15][1/10]\tTime 0.156 (0.156)\tData 0.1261 (0.1261)\tLoss 0.6265 (0.6265)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [15][2/10]\tTime 0.153 (0.154)\tData 0.1221 (0.1241)\tLoss 0.6213 (0.6239)\tAccu 0.6758 (0.6719)\t\n",
            "Epoch: [15][3/10]\tTime 0.164 (0.158)\tData 0.1330 (0.1271)\tLoss 0.6132 (0.6203)\tAccu 0.6914 (0.6784)\t\n",
            "Epoch: [15][4/10]\tTime 0.157 (0.158)\tData 0.1249 (0.1265)\tLoss 0.6080 (0.6172)\tAccu 0.6953 (0.6826)\t\n",
            "Epoch: [15][5/10]\tTime 0.158 (0.158)\tData 0.1275 (0.1267)\tLoss 0.6007 (0.6139)\tAccu 0.6914 (0.6844)\t\n",
            "Epoch: [15][6/10]\tTime 0.161 (0.158)\tData 0.1303 (0.1273)\tLoss 0.6113 (0.6135)\tAccu 0.6875 (0.6849)\t\n",
            "Epoch: [15][7/10]\tTime 0.158 (0.158)\tData 0.1278 (0.1274)\tLoss 0.6408 (0.6174)\tAccu 0.6562 (0.6808)\t\n",
            "Epoch: [15][8/10]\tTime 0.157 (0.158)\tData 0.1270 (0.1273)\tLoss 0.6338 (0.6194)\tAccu 0.6523 (0.6772)\t\n",
            "Epoch: [15][9/10]\tTime 0.159 (0.158)\tData 0.1289 (0.1275)\tLoss 0.6443 (0.6222)\tAccu 0.6523 (0.6745)\t\n",
            "Epoch: [15][10/10]\tTime 0.125 (0.155)\tData 0.0964 (0.1244)\tLoss 0.6277 (0.6226)\tAccu 0.6786 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6183\t\n",
            "Epoch: [16][1/10]\tTime 0.154 (0.154)\tData 0.1239 (0.1239)\tLoss 0.6232 (0.6232)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [16][2/10]\tTime 0.151 (0.153)\tData 0.1215 (0.1227)\tLoss 0.6126 (0.6179)\tAccu 0.6875 (0.6797)\t\n",
            "Epoch: [16][3/10]\tTime 0.154 (0.153)\tData 0.1244 (0.1232)\tLoss 0.6231 (0.6196)\tAccu 0.6758 (0.6784)\t\n",
            "Epoch: [16][4/10]\tTime 0.154 (0.153)\tData 0.1244 (0.1235)\tLoss 0.6269 (0.6215)\tAccu 0.6719 (0.6768)\t\n",
            "Epoch: [16][5/10]\tTime 0.160 (0.155)\tData 0.1294 (0.1247)\tLoss 0.5802 (0.6132)\tAccu 0.7305 (0.6875)\t\n",
            "Epoch: [16][6/10]\tTime 0.153 (0.155)\tData 0.1232 (0.1244)\tLoss 0.6219 (0.6146)\tAccu 0.6797 (0.6862)\t\n",
            "Epoch: [16][7/10]\tTime 0.159 (0.155)\tData 0.1283 (0.1250)\tLoss 0.6493 (0.6196)\tAccu 0.6406 (0.6797)\t\n",
            "Epoch: [16][8/10]\tTime 0.155 (0.155)\tData 0.1236 (0.1248)\tLoss 0.6394 (0.6221)\tAccu 0.6523 (0.6763)\t\n",
            "Epoch: [16][9/10]\tTime 0.159 (0.155)\tData 0.1285 (0.1252)\tLoss 0.6344 (0.6234)\tAccu 0.6484 (0.6732)\t\n",
            "Epoch: [16][10/10]\tTime 0.125 (0.152)\tData 0.0960 (0.1223)\tLoss 0.6087 (0.6223)\tAccu 0.6939 (0.6748)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6182\t\n",
            "Epoch: [17][1/10]\tTime 0.168 (0.168)\tData 0.1369 (0.1369)\tLoss 0.6358 (0.6358)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [17][2/10]\tTime 0.152 (0.160)\tData 0.1220 (0.1294)\tLoss 0.6079 (0.6219)\tAccu 0.6953 (0.6777)\t\n",
            "Epoch: [17][3/10]\tTime 0.154 (0.158)\tData 0.1244 (0.1278)\tLoss 0.6165 (0.6201)\tAccu 0.6758 (0.6771)\t\n",
            "Epoch: [17][4/10]\tTime 0.155 (0.157)\tData 0.1244 (0.1269)\tLoss 0.6503 (0.6276)\tAccu 0.6328 (0.6660)\t\n",
            "Epoch: [17][5/10]\tTime 0.161 (0.158)\tData 0.1305 (0.1276)\tLoss 0.6111 (0.6243)\tAccu 0.6953 (0.6719)\t\n",
            "Epoch: [17][6/10]\tTime 0.161 (0.158)\tData 0.1311 (0.1282)\tLoss 0.6139 (0.6226)\tAccu 0.6797 (0.6732)\t\n",
            "Epoch: [17][7/10]\tTime 0.156 (0.158)\tData 0.1251 (0.1278)\tLoss 0.6398 (0.6250)\tAccu 0.6484 (0.6696)\t\n",
            "Epoch: [17][8/10]\tTime 0.159 (0.158)\tData 0.1289 (0.1279)\tLoss 0.6117 (0.6234)\tAccu 0.6797 (0.6709)\t\n",
            "Epoch: [17][9/10]\tTime 0.156 (0.158)\tData 0.1267 (0.1278)\tLoss 0.5904 (0.6197)\tAccu 0.7305 (0.6775)\t\n",
            "Epoch: [17][10/10]\tTime 0.128 (0.155)\tData 0.0986 (0.1249)\tLoss 0.6481 (0.6219)\tAccu 0.6429 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6181\t\n",
            "Epoch: [18][1/10]\tTime 0.163 (0.163)\tData 0.1332 (0.1332)\tLoss 0.6325 (0.6325)\tAccu 0.6641 (0.6641)\t\n",
            "Epoch: [18][2/10]\tTime 0.162 (0.163)\tData 0.1310 (0.1321)\tLoss 0.6196 (0.6260)\tAccu 0.6641 (0.6641)\t\n",
            "Epoch: [18][3/10]\tTime 0.164 (0.163)\tData 0.1310 (0.1317)\tLoss 0.6376 (0.6299)\tAccu 0.6602 (0.6628)\t\n",
            "Epoch: [18][4/10]\tTime 0.153 (0.161)\tData 0.1230 (0.1296)\tLoss 0.6291 (0.6297)\tAccu 0.6797 (0.6670)\t\n",
            "Epoch: [18][5/10]\tTime 0.160 (0.161)\tData 0.1301 (0.1297)\tLoss 0.6075 (0.6252)\tAccu 0.6836 (0.6703)\t\n",
            "Epoch: [18][6/10]\tTime 0.159 (0.160)\tData 0.1246 (0.1288)\tLoss 0.6322 (0.6264)\tAccu 0.6562 (0.6680)\t\n",
            "Epoch: [18][7/10]\tTime 0.159 (0.160)\tData 0.1290 (0.1288)\tLoss 0.6066 (0.6236)\tAccu 0.6953 (0.6719)\t\n",
            "Epoch: [18][8/10]\tTime 0.153 (0.159)\tData 0.1227 (0.1281)\tLoss 0.5805 (0.6182)\tAccu 0.7305 (0.6792)\t\n",
            "Epoch: [18][9/10]\tTime 0.162 (0.160)\tData 0.1318 (0.1285)\tLoss 0.6284 (0.6193)\tAccu 0.6680 (0.6780)\t\n",
            "Epoch: [18][10/10]\tTime 0.125 (0.156)\tData 0.0964 (0.1253)\tLoss 0.6511 (0.6218)\tAccu 0.6378 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6178\t\n",
            "Epoch: [19][1/10]\tTime 0.174 (0.174)\tData 0.1438 (0.1438)\tLoss 0.5800 (0.5800)\tAccu 0.7305 (0.7305)\t\n",
            "Epoch: [19][2/10]\tTime 0.166 (0.170)\tData 0.1352 (0.1395)\tLoss 0.6375 (0.6087)\tAccu 0.6602 (0.6953)\t\n",
            "Epoch: [19][3/10]\tTime 0.153 (0.164)\tData 0.1232 (0.1341)\tLoss 0.6539 (0.6238)\tAccu 0.6406 (0.6771)\t\n",
            "Epoch: [19][4/10]\tTime 0.156 (0.162)\tData 0.1247 (0.1317)\tLoss 0.6133 (0.6212)\tAccu 0.6797 (0.6777)\t\n",
            "Epoch: [19][5/10]\tTime 0.164 (0.162)\tData 0.1323 (0.1318)\tLoss 0.6560 (0.6281)\tAccu 0.6211 (0.6664)\t\n",
            "Epoch: [19][6/10]\tTime 0.154 (0.161)\tData 0.1241 (0.1305)\tLoss 0.6157 (0.6261)\tAccu 0.6797 (0.6686)\t\n",
            "Epoch: [19][7/10]\tTime 0.164 (0.162)\tData 0.1331 (0.1309)\tLoss 0.6138 (0.6243)\tAccu 0.6875 (0.6713)\t\n",
            "Epoch: [19][8/10]\tTime 0.154 (0.161)\tData 0.1234 (0.1300)\tLoss 0.6027 (0.6216)\tAccu 0.7070 (0.6758)\t\n",
            "Epoch: [19][9/10]\tTime 0.161 (0.161)\tData 0.1300 (0.1300)\tLoss 0.6058 (0.6199)\tAccu 0.6953 (0.6780)\t\n",
            "Epoch: [19][10/10]\tTime 0.126 (0.157)\tData 0.0969 (0.1267)\tLoss 0.6436 (0.6217)\tAccu 0.6378 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6180\t\n",
            "Epoch: [20][1/10]\tTime 0.156 (0.156)\tData 0.1259 (0.1259)\tLoss 0.6679 (0.6679)\tAccu 0.6250 (0.6250)\t\n",
            "Epoch: [20][2/10]\tTime 0.153 (0.155)\tData 0.1228 (0.1244)\tLoss 0.5999 (0.6339)\tAccu 0.6953 (0.6602)\t\n",
            "Epoch: [20][3/10]\tTime 0.158 (0.156)\tData 0.1284 (0.1257)\tLoss 0.6094 (0.6258)\tAccu 0.6953 (0.6719)\t\n",
            "Epoch: [20][4/10]\tTime 0.156 (0.156)\tData 0.1257 (0.1257)\tLoss 0.6177 (0.6237)\tAccu 0.6797 (0.6738)\t\n",
            "Epoch: [20][5/10]\tTime 0.162 (0.157)\tData 0.1318 (0.1269)\tLoss 0.6403 (0.6271)\tAccu 0.6523 (0.6695)\t\n",
            "Epoch: [20][6/10]\tTime 0.159 (0.158)\tData 0.1276 (0.1270)\tLoss 0.6064 (0.6236)\tAccu 0.7031 (0.6751)\t\n",
            "Epoch: [20][7/10]\tTime 0.159 (0.158)\tData 0.1269 (0.1270)\tLoss 0.6142 (0.6223)\tAccu 0.6758 (0.6752)\t\n",
            "Epoch: [20][8/10]\tTime 0.159 (0.158)\tData 0.1277 (0.1271)\tLoss 0.6303 (0.6233)\tAccu 0.6641 (0.6738)\t\n",
            "Epoch: [20][9/10]\tTime 0.159 (0.158)\tData 0.1286 (0.1273)\tLoss 0.5906 (0.6196)\tAccu 0.7031 (0.6771)\t\n",
            "Epoch: [20][10/10]\tTime 0.126 (0.155)\tData 0.0960 (0.1241)\tLoss 0.6398 (0.6212)\tAccu 0.6480 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6176\t\n",
            "Epoch: [21][1/10]\tTime 0.158 (0.158)\tData 0.1267 (0.1267)\tLoss 0.6060 (0.6060)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [21][2/10]\tTime 0.159 (0.159)\tData 0.1290 (0.1278)\tLoss 0.6566 (0.6313)\tAccu 0.6367 (0.6602)\t\n",
            "Epoch: [21][3/10]\tTime 0.168 (0.162)\tData 0.1382 (0.1313)\tLoss 0.5694 (0.6107)\tAccu 0.7344 (0.6849)\t\n",
            "Epoch: [21][4/10]\tTime 0.159 (0.161)\tData 0.1281 (0.1305)\tLoss 0.5920 (0.6060)\tAccu 0.7031 (0.6895)\t\n",
            "Epoch: [21][5/10]\tTime 0.160 (0.161)\tData 0.1301 (0.1304)\tLoss 0.6053 (0.6059)\tAccu 0.6875 (0.6891)\t\n",
            "Epoch: [21][6/10]\tTime 0.159 (0.161)\tData 0.1274 (0.1299)\tLoss 0.6718 (0.6168)\tAccu 0.6172 (0.6771)\t\n",
            "Epoch: [21][7/10]\tTime 0.156 (0.160)\tData 0.1264 (0.1294)\tLoss 0.6377 (0.6198)\tAccu 0.6641 (0.6752)\t\n",
            "Epoch: [21][8/10]\tTime 0.158 (0.160)\tData 0.1277 (0.1292)\tLoss 0.5947 (0.6167)\tAccu 0.7148 (0.6802)\t\n",
            "Epoch: [21][9/10]\tTime 0.158 (0.160)\tData 0.1264 (0.1289)\tLoss 0.6490 (0.6203)\tAccu 0.6445 (0.6762)\t\n",
            "Epoch: [21][10/10]\tTime 0.129 (0.157)\tData 0.1003 (0.1260)\tLoss 0.6351 (0.6214)\tAccu 0.6582 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6177\t\n",
            "Epoch: [22][1/10]\tTime 0.152 (0.152)\tData 0.1222 (0.1222)\tLoss 0.6031 (0.6031)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [22][2/10]\tTime 0.155 (0.154)\tData 0.1255 (0.1238)\tLoss 0.6072 (0.6051)\tAccu 0.6836 (0.6875)\t\n",
            "Epoch: [22][3/10]\tTime 0.154 (0.154)\tData 0.1240 (0.1239)\tLoss 0.6391 (0.6165)\tAccu 0.6523 (0.6758)\t\n",
            "Epoch: [22][4/10]\tTime 0.157 (0.155)\tData 0.1272 (0.1247)\tLoss 0.6167 (0.6165)\tAccu 0.6836 (0.6777)\t\n",
            "Epoch: [22][5/10]\tTime 0.154 (0.155)\tData 0.1220 (0.1242)\tLoss 0.6436 (0.6219)\tAccu 0.6484 (0.6719)\t\n",
            "Epoch: [22][6/10]\tTime 0.157 (0.155)\tData 0.1239 (0.1241)\tLoss 0.6380 (0.6246)\tAccu 0.6406 (0.6667)\t\n",
            "Epoch: [22][7/10]\tTime 0.157 (0.155)\tData 0.1274 (0.1246)\tLoss 0.5743 (0.6174)\tAccu 0.7578 (0.6797)\t\n",
            "Epoch: [22][8/10]\tTime 0.155 (0.155)\tData 0.1252 (0.1247)\tLoss 0.6366 (0.6198)\tAccu 0.6641 (0.6777)\t\n",
            "Epoch: [22][9/10]\tTime 0.151 (0.155)\tData 0.1213 (0.1243)\tLoss 0.6132 (0.6191)\tAccu 0.6758 (0.6775)\t\n",
            "Epoch: [22][10/10]\tTime 0.128 (0.152)\tData 0.0993 (0.1218)\tLoss 0.6442 (0.6210)\tAccu 0.6429 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6177\t\n",
            "Epoch: [23][1/10]\tTime 0.159 (0.159)\tData 0.1280 (0.1280)\tLoss 0.6554 (0.6554)\tAccu 0.6289 (0.6289)\t\n",
            "Epoch: [23][2/10]\tTime 0.156 (0.157)\tData 0.1259 (0.1270)\tLoss 0.6086 (0.6320)\tAccu 0.6836 (0.6562)\t\n",
            "Epoch: [23][3/10]\tTime 0.154 (0.156)\tData 0.1220 (0.1253)\tLoss 0.5631 (0.6090)\tAccu 0.7578 (0.6901)\t\n",
            "Epoch: [23][4/10]\tTime 0.155 (0.156)\tData 0.1243 (0.1250)\tLoss 0.5952 (0.6056)\tAccu 0.7109 (0.6953)\t\n",
            "Epoch: [23][5/10]\tTime 0.157 (0.156)\tData 0.1253 (0.1251)\tLoss 0.5948 (0.6034)\tAccu 0.7188 (0.7000)\t\n",
            "Epoch: [23][6/10]\tTime 0.156 (0.156)\tData 0.1250 (0.1251)\tLoss 0.6423 (0.6099)\tAccu 0.6406 (0.6901)\t\n",
            "Epoch: [23][7/10]\tTime 0.157 (0.156)\tData 0.1260 (0.1252)\tLoss 0.6332 (0.6132)\tAccu 0.6523 (0.6847)\t\n",
            "Epoch: [23][8/10]\tTime 0.160 (0.157)\tData 0.1283 (0.1256)\tLoss 0.6599 (0.6191)\tAccu 0.6211 (0.6768)\t\n",
            "Epoch: [23][9/10]\tTime 0.158 (0.157)\tData 0.1276 (0.1258)\tLoss 0.6257 (0.6198)\tAccu 0.6797 (0.6771)\t\n",
            "Epoch: [23][10/10]\tTime 0.127 (0.154)\tData 0.0977 (0.1230)\tLoss 0.6456 (0.6218)\tAccu 0.6480 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6173\t\n",
            "Epoch: [24][1/10]\tTime 0.163 (0.163)\tData 0.1325 (0.1325)\tLoss 0.6168 (0.6168)\tAccu 0.6641 (0.6641)\t\n",
            "Epoch: [24][2/10]\tTime 0.164 (0.164)\tData 0.1334 (0.1329)\tLoss 0.6763 (0.6466)\tAccu 0.6211 (0.6426)\t\n",
            "Epoch: [24][3/10]\tTime 0.154 (0.160)\tData 0.1230 (0.1296)\tLoss 0.6188 (0.6373)\tAccu 0.6914 (0.6589)\t\n",
            "Epoch: [24][4/10]\tTime 0.158 (0.160)\tData 0.1276 (0.1291)\tLoss 0.5990 (0.6277)\tAccu 0.7070 (0.6709)\t\n",
            "Epoch: [24][5/10]\tTime 0.158 (0.159)\tData 0.1277 (0.1288)\tLoss 0.6033 (0.6228)\tAccu 0.7109 (0.6789)\t\n",
            "Epoch: [24][6/10]\tTime 0.163 (0.160)\tData 0.1318 (0.1293)\tLoss 0.5964 (0.6184)\tAccu 0.7266 (0.6868)\t\n",
            "Epoch: [24][7/10]\tTime 0.160 (0.160)\tData 0.1303 (0.1295)\tLoss 0.6335 (0.6206)\tAccu 0.6523 (0.6819)\t\n",
            "Epoch: [24][8/10]\tTime 0.160 (0.160)\tData 0.1297 (0.1295)\tLoss 0.6046 (0.6186)\tAccu 0.6797 (0.6816)\t\n",
            "Epoch: [24][9/10]\tTime 0.158 (0.160)\tData 0.1283 (0.1294)\tLoss 0.6400 (0.6210)\tAccu 0.6406 (0.6771)\t\n",
            "Epoch: [24][10/10]\tTime 0.122 (0.156)\tData 0.0927 (0.1257)\tLoss 0.6386 (0.6224)\tAccu 0.6480 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6175\t\n",
            "Epoch: [25][1/10]\tTime 0.157 (0.157)\tData 0.1264 (0.1264)\tLoss 0.6428 (0.6428)\tAccu 0.6484 (0.6484)\t\n",
            "Epoch: [25][2/10]\tTime 0.157 (0.157)\tData 0.1270 (0.1267)\tLoss 0.6467 (0.6447)\tAccu 0.6445 (0.6465)\t\n",
            "Epoch: [25][3/10]\tTime 0.157 (0.157)\tData 0.1256 (0.1263)\tLoss 0.5808 (0.6234)\tAccu 0.7266 (0.6732)\t\n",
            "Epoch: [25][4/10]\tTime 0.162 (0.158)\tData 0.1314 (0.1276)\tLoss 0.6212 (0.6229)\tAccu 0.6797 (0.6748)\t\n",
            "Epoch: [25][5/10]\tTime 0.157 (0.158)\tData 0.1265 (0.1274)\tLoss 0.6128 (0.6209)\tAccu 0.6914 (0.6781)\t\n",
            "Epoch: [25][6/10]\tTime 0.156 (0.158)\tData 0.1264 (0.1272)\tLoss 0.6595 (0.6273)\tAccu 0.6211 (0.6686)\t\n",
            "Epoch: [25][7/10]\tTime 0.151 (0.157)\tData 0.1212 (0.1264)\tLoss 0.6436 (0.6296)\tAccu 0.6406 (0.6646)\t\n",
            "Epoch: [25][8/10]\tTime 0.165 (0.158)\tData 0.1348 (0.1274)\tLoss 0.6078 (0.6269)\tAccu 0.6875 (0.6675)\t\n",
            "Epoch: [25][9/10]\tTime 0.155 (0.157)\tData 0.1243 (0.1271)\tLoss 0.6069 (0.6247)\tAccu 0.6797 (0.6688)\t\n",
            "Epoch: [25][10/10]\tTime 0.136 (0.155)\tData 0.1064 (0.1250)\tLoss 0.5654 (0.6200)\tAccu 0.7449 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6173\t\n",
            "Epoch: [26][1/10]\tTime 0.159 (0.159)\tData 0.1276 (0.1276)\tLoss 0.5981 (0.5981)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [26][2/10]\tTime 0.152 (0.156)\tData 0.1222 (0.1249)\tLoss 0.6318 (0.6149)\tAccu 0.6719 (0.6816)\t\n",
            "Epoch: [26][3/10]\tTime 0.158 (0.156)\tData 0.1265 (0.1254)\tLoss 0.6114 (0.6137)\tAccu 0.6836 (0.6823)\t\n",
            "Epoch: [26][4/10]\tTime 0.155 (0.156)\tData 0.1246 (0.1252)\tLoss 0.6318 (0.6183)\tAccu 0.6602 (0.6768)\t\n",
            "Epoch: [26][5/10]\tTime 0.160 (0.157)\tData 0.1294 (0.1261)\tLoss 0.6134 (0.6173)\tAccu 0.6836 (0.6781)\t\n",
            "Epoch: [26][6/10]\tTime 0.164 (0.158)\tData 0.1325 (0.1271)\tLoss 0.6528 (0.6232)\tAccu 0.6562 (0.6745)\t\n",
            "Epoch: [26][7/10]\tTime 0.158 (0.158)\tData 0.1274 (0.1272)\tLoss 0.6411 (0.6258)\tAccu 0.6523 (0.6713)\t\n",
            "Epoch: [26][8/10]\tTime 0.156 (0.158)\tData 0.1255 (0.1270)\tLoss 0.6021 (0.6228)\tAccu 0.6953 (0.6743)\t\n",
            "Epoch: [26][9/10]\tTime 0.158 (0.158)\tData 0.1276 (0.1270)\tLoss 0.5991 (0.6202)\tAccu 0.6875 (0.6758)\t\n",
            "Epoch: [26][10/10]\tTime 0.125 (0.154)\tData 0.0962 (0.1239)\tLoss 0.6207 (0.6202)\tAccu 0.6633 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6175\t\n",
            "Epoch: [27][1/10]\tTime 0.153 (0.153)\tData 0.1221 (0.1221)\tLoss 0.6041 (0.6041)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [27][2/10]\tTime 0.155 (0.154)\tData 0.1247 (0.1234)\tLoss 0.5979 (0.6010)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [27][3/10]\tTime 0.150 (0.153)\tData 0.1206 (0.1224)\tLoss 0.6415 (0.6145)\tAccu 0.6445 (0.6862)\t\n",
            "Epoch: [27][4/10]\tTime 0.152 (0.153)\tData 0.1220 (0.1223)\tLoss 0.6159 (0.6149)\tAccu 0.6836 (0.6855)\t\n",
            "Epoch: [27][5/10]\tTime 0.157 (0.153)\tData 0.1262 (0.1231)\tLoss 0.6333 (0.6185)\tAccu 0.6523 (0.6789)\t\n",
            "Epoch: [27][6/10]\tTime 0.159 (0.154)\tData 0.1280 (0.1239)\tLoss 0.6197 (0.6187)\tAccu 0.6758 (0.6784)\t\n",
            "Epoch: [27][7/10]\tTime 0.157 (0.155)\tData 0.1262 (0.1243)\tLoss 0.6403 (0.6218)\tAccu 0.6406 (0.6730)\t\n",
            "Epoch: [27][8/10]\tTime 0.162 (0.156)\tData 0.1321 (0.1252)\tLoss 0.6088 (0.6202)\tAccu 0.6797 (0.6738)\t\n",
            "Epoch: [27][9/10]\tTime 0.157 (0.156)\tData 0.1255 (0.1253)\tLoss 0.6056 (0.6186)\tAccu 0.6914 (0.6758)\t\n",
            "Epoch: [27][10/10]\tTime 0.125 (0.153)\tData 0.0957 (0.1223)\tLoss 0.6275 (0.6193)\tAccu 0.6633 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6174\t\n",
            "Epoch: [28][1/10]\tTime 0.164 (0.164)\tData 0.1328 (0.1328)\tLoss 0.6306 (0.6306)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [28][2/10]\tTime 0.157 (0.160)\tData 0.1263 (0.1295)\tLoss 0.6130 (0.6218)\tAccu 0.6797 (0.6738)\t\n",
            "Epoch: [28][3/10]\tTime 0.160 (0.160)\tData 0.1285 (0.1292)\tLoss 0.5930 (0.6122)\tAccu 0.6992 (0.6823)\t\n",
            "Epoch: [28][4/10]\tTime 0.154 (0.159)\tData 0.1243 (0.1280)\tLoss 0.6167 (0.6133)\tAccu 0.6797 (0.6816)\t\n",
            "Epoch: [28][5/10]\tTime 0.157 (0.158)\tData 0.1270 (0.1278)\tLoss 0.6490 (0.6205)\tAccu 0.6484 (0.6750)\t\n",
            "Epoch: [28][6/10]\tTime 0.154 (0.158)\tData 0.1237 (0.1271)\tLoss 0.6178 (0.6200)\tAccu 0.6797 (0.6758)\t\n",
            "Epoch: [28][7/10]\tTime 0.155 (0.157)\tData 0.1221 (0.1264)\tLoss 0.6089 (0.6184)\tAccu 0.6914 (0.6780)\t\n",
            "Epoch: [28][8/10]\tTime 0.157 (0.157)\tData 0.1263 (0.1264)\tLoss 0.5753 (0.6130)\tAccu 0.7305 (0.6846)\t\n",
            "Epoch: [28][9/10]\tTime 0.158 (0.157)\tData 0.1282 (0.1266)\tLoss 0.6587 (0.6181)\tAccu 0.6133 (0.6766)\t\n",
            "Epoch: [28][10/10]\tTime 0.130 (0.155)\tData 0.1016 (0.1241)\tLoss 0.6305 (0.6191)\tAccu 0.6531 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6171\t\n",
            "Epoch: [29][1/10]\tTime 0.157 (0.157)\tData 0.1259 (0.1259)\tLoss 0.6041 (0.6041)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [29][2/10]\tTime 0.161 (0.159)\tData 0.1299 (0.1279)\tLoss 0.5601 (0.5821)\tAccu 0.7539 (0.7305)\t\n",
            "Epoch: [29][3/10]\tTime 0.162 (0.160)\tData 0.1292 (0.1284)\tLoss 0.6533 (0.6058)\tAccu 0.6328 (0.6979)\t\n",
            "Epoch: [29][4/10]\tTime 0.161 (0.160)\tData 0.1308 (0.1290)\tLoss 0.6533 (0.6177)\tAccu 0.6367 (0.6826)\t\n",
            "Epoch: [29][5/10]\tTime 0.165 (0.161)\tData 0.1347 (0.1301)\tLoss 0.5896 (0.6121)\tAccu 0.7031 (0.6867)\t\n",
            "Epoch: [29][6/10]\tTime 0.157 (0.160)\tData 0.1260 (0.1294)\tLoss 0.6439 (0.6174)\tAccu 0.6250 (0.6764)\t\n",
            "Epoch: [29][7/10]\tTime 0.159 (0.160)\tData 0.1283 (0.1293)\tLoss 0.6190 (0.6176)\tAccu 0.6797 (0.6769)\t\n",
            "Epoch: [29][8/10]\tTime 0.160 (0.160)\tData 0.1291 (0.1293)\tLoss 0.6042 (0.6159)\tAccu 0.6953 (0.6792)\t\n",
            "Epoch: [29][9/10]\tTime 0.157 (0.160)\tData 0.1268 (0.1290)\tLoss 0.6528 (0.6200)\tAccu 0.6289 (0.6736)\t\n",
            "Epoch: [29][10/10]\tTime 0.125 (0.156)\tData 0.0957 (0.1256)\tLoss 0.6035 (0.6187)\tAccu 0.6888 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6179\t\n",
            "Epoch: [30][1/10]\tTime 0.152 (0.152)\tData 0.1214 (0.1214)\tLoss 0.6191 (0.6191)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [30][2/10]\tTime 0.150 (0.151)\tData 0.1202 (0.1208)\tLoss 0.6286 (0.6238)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [30][3/10]\tTime 0.149 (0.150)\tData 0.1187 (0.1201)\tLoss 0.6088 (0.6188)\tAccu 0.6875 (0.6745)\t\n",
            "Epoch: [30][4/10]\tTime 0.153 (0.151)\tData 0.1229 (0.1208)\tLoss 0.6157 (0.6181)\tAccu 0.6797 (0.6758)\t\n",
            "Epoch: [30][5/10]\tTime 0.157 (0.152)\tData 0.1267 (0.1220)\tLoss 0.6296 (0.6204)\tAccu 0.6562 (0.6719)\t\n",
            "Epoch: [30][6/10]\tTime 0.154 (0.152)\tData 0.1225 (0.1221)\tLoss 0.6047 (0.6177)\tAccu 0.6875 (0.6745)\t\n",
            "Epoch: [30][7/10]\tTime 0.151 (0.152)\tData 0.1209 (0.1219)\tLoss 0.6152 (0.6174)\tAccu 0.6641 (0.6730)\t\n",
            "Epoch: [30][8/10]\tTime 0.148 (0.152)\tData 0.1181 (0.1214)\tLoss 0.6456 (0.6209)\tAccu 0.6602 (0.6714)\t\n",
            "Epoch: [30][9/10]\tTime 0.153 (0.152)\tData 0.1229 (0.1216)\tLoss 0.6349 (0.6225)\tAccu 0.6484 (0.6688)\t\n",
            "Epoch: [30][10/10]\tTime 0.128 (0.149)\tData 0.0962 (0.1191)\tLoss 0.5692 (0.6183)\tAccu 0.7449 (0.6748)\t\n",
            "Time 0.042\tAccu 0.6800\tLoss 0.6170\t\n",
            "Epoch: [31][1/10]\tTime 0.156 (0.156)\tData 0.1256 (0.1256)\tLoss 0.5819 (0.5819)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [31][2/10]\tTime 0.151 (0.153)\tData 0.1211 (0.1234)\tLoss 0.6085 (0.5952)\tAccu 0.6797 (0.6973)\t\n",
            "Epoch: [31][3/10]\tTime 0.157 (0.154)\tData 0.1246 (0.1238)\tLoss 0.6208 (0.6038)\tAccu 0.6836 (0.6927)\t\n",
            "Epoch: [31][4/10]\tTime 0.152 (0.154)\tData 0.1219 (0.1233)\tLoss 0.6371 (0.6121)\tAccu 0.6602 (0.6846)\t\n",
            "Epoch: [31][5/10]\tTime 0.158 (0.155)\tData 0.1282 (0.1243)\tLoss 0.6349 (0.6167)\tAccu 0.6680 (0.6813)\t\n",
            "Epoch: [31][6/10]\tTime 0.157 (0.155)\tData 0.1266 (0.1247)\tLoss 0.5690 (0.6087)\tAccu 0.7188 (0.6875)\t\n",
            "Epoch: [31][7/10]\tTime 0.149 (0.154)\tData 0.1189 (0.1239)\tLoss 0.6346 (0.6124)\tAccu 0.6680 (0.6847)\t\n",
            "Epoch: [31][8/10]\tTime 0.147 (0.153)\tData 0.1167 (0.1230)\tLoss 0.6644 (0.6189)\tAccu 0.6133 (0.6758)\t\n",
            "Epoch: [31][9/10]\tTime 0.160 (0.154)\tData 0.1289 (0.1236)\tLoss 0.6175 (0.6188)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [31][10/10]\tTime 0.128 (0.151)\tData 0.0991 (0.1212)\tLoss 0.6169 (0.6186)\tAccu 0.6633 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6187\t\n",
            "Epoch: [32][1/10]\tTime 0.156 (0.156)\tData 0.1252 (0.1252)\tLoss 0.6719 (0.6719)\tAccu 0.6094 (0.6094)\t\n",
            "Epoch: [32][2/10]\tTime 0.164 (0.160)\tData 0.1335 (0.1293)\tLoss 0.6162 (0.6441)\tAccu 0.6875 (0.6484)\t\n",
            "Epoch: [32][3/10]\tTime 0.154 (0.158)\tData 0.1243 (0.1277)\tLoss 0.6437 (0.6440)\tAccu 0.6328 (0.6432)\t\n",
            "Epoch: [32][4/10]\tTime 0.157 (0.158)\tData 0.1264 (0.1274)\tLoss 0.6254 (0.6393)\tAccu 0.6484 (0.6445)\t\n",
            "Epoch: [32][5/10]\tTime 0.156 (0.157)\tData 0.1240 (0.1267)\tLoss 0.6235 (0.6361)\tAccu 0.6758 (0.6508)\t\n",
            "Epoch: [32][6/10]\tTime 0.160 (0.158)\tData 0.1288 (0.1270)\tLoss 0.6024 (0.6305)\tAccu 0.7070 (0.6602)\t\n",
            "Epoch: [32][7/10]\tTime 0.156 (0.157)\tData 0.1256 (0.1268)\tLoss 0.5933 (0.6252)\tAccu 0.7305 (0.6702)\t\n",
            "Epoch: [32][8/10]\tTime 0.155 (0.157)\tData 0.1243 (0.1265)\tLoss 0.6152 (0.6239)\tAccu 0.6875 (0.6724)\t\n",
            "Epoch: [32][9/10]\tTime 0.154 (0.157)\tData 0.1238 (0.1262)\tLoss 0.5795 (0.6190)\tAccu 0.7148 (0.6771)\t\n",
            "Epoch: [32][10/10]\tTime 0.124 (0.153)\tData 0.0952 (0.1231)\tLoss 0.6338 (0.6202)\tAccu 0.6480 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6203\t\n",
            "Epoch: [33][1/10]\tTime 0.160 (0.160)\tData 0.1297 (0.1297)\tLoss 0.5911 (0.5911)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [33][2/10]\tTime 0.154 (0.157)\tData 0.1233 (0.1265)\tLoss 0.5995 (0.5953)\tAccu 0.6875 (0.6914)\t\n",
            "Epoch: [33][3/10]\tTime 0.155 (0.156)\tData 0.1245 (0.1258)\tLoss 0.6489 (0.6131)\tAccu 0.6484 (0.6771)\t\n",
            "Epoch: [33][4/10]\tTime 0.158 (0.157)\tData 0.1281 (0.1264)\tLoss 0.6322 (0.6179)\tAccu 0.6602 (0.6729)\t\n",
            "Epoch: [33][5/10]\tTime 0.155 (0.156)\tData 0.1252 (0.1261)\tLoss 0.6601 (0.6263)\tAccu 0.6406 (0.6664)\t\n",
            "Epoch: [33][6/10]\tTime 0.151 (0.156)\tData 0.1207 (0.1252)\tLoss 0.6231 (0.6258)\tAccu 0.6797 (0.6686)\t\n",
            "Epoch: [33][7/10]\tTime 0.157 (0.156)\tData 0.1273 (0.1255)\tLoss 0.6270 (0.6260)\tAccu 0.6836 (0.6708)\t\n",
            "Epoch: [33][8/10]\tTime 0.155 (0.156)\tData 0.1250 (0.1255)\tLoss 0.6119 (0.6242)\tAccu 0.6758 (0.6714)\t\n",
            "Epoch: [33][9/10]\tTime 0.157 (0.156)\tData 0.1268 (0.1256)\tLoss 0.6232 (0.6241)\tAccu 0.6719 (0.6714)\t\n",
            "Epoch: [33][10/10]\tTime 0.126 (0.153)\tData 0.0966 (0.1227)\tLoss 0.6041 (0.6225)\tAccu 0.7143 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6238\t\n",
            "Epoch: [34][1/10]\tTime 0.156 (0.156)\tData 0.1254 (0.1254)\tLoss 0.5993 (0.5993)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [34][2/10]\tTime 0.157 (0.156)\tData 0.1257 (0.1256)\tLoss 0.6372 (0.6182)\tAccu 0.6406 (0.6777)\t\n",
            "Epoch: [34][3/10]\tTime 0.160 (0.158)\tData 0.1290 (0.1267)\tLoss 0.6020 (0.6128)\tAccu 0.6992 (0.6849)\t\n",
            "Epoch: [34][4/10]\tTime 0.162 (0.159)\tData 0.1310 (0.1278)\tLoss 0.6259 (0.6161)\tAccu 0.6562 (0.6777)\t\n",
            "Epoch: [34][5/10]\tTime 0.153 (0.158)\tData 0.1231 (0.1268)\tLoss 0.6303 (0.6189)\tAccu 0.6484 (0.6719)\t\n",
            "Epoch: [34][6/10]\tTime 0.161 (0.158)\tData 0.1307 (0.1275)\tLoss 0.6079 (0.6171)\tAccu 0.7070 (0.6777)\t\n",
            "Epoch: [34][7/10]\tTime 0.154 (0.158)\tData 0.1239 (0.1270)\tLoss 0.6624 (0.6236)\tAccu 0.6211 (0.6696)\t\n",
            "Epoch: [34][8/10]\tTime 0.156 (0.157)\tData 0.1249 (0.1267)\tLoss 0.6165 (0.6227)\tAccu 0.6758 (0.6704)\t\n",
            "Epoch: [34][9/10]\tTime 0.154 (0.157)\tData 0.1237 (0.1264)\tLoss 0.6067 (0.6209)\tAccu 0.7031 (0.6740)\t\n",
            "Epoch: [34][10/10]\tTime 0.130 (0.154)\tData 0.1009 (0.1238)\tLoss 0.6040 (0.6196)\tAccu 0.6837 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6172\t\n",
            "Epoch: [35][1/10]\tTime 0.154 (0.154)\tData 0.1241 (0.1241)\tLoss 0.5886 (0.5886)\tAccu 0.7266 (0.7266)\t\n",
            "Epoch: [35][2/10]\tTime 0.158 (0.156)\tData 0.1281 (0.1261)\tLoss 0.5978 (0.5932)\tAccu 0.6875 (0.7070)\t\n",
            "Epoch: [35][3/10]\tTime 0.155 (0.156)\tData 0.1252 (0.1258)\tLoss 0.6463 (0.6109)\tAccu 0.6445 (0.6862)\t\n",
            "Epoch: [35][4/10]\tTime 0.154 (0.155)\tData 0.1239 (0.1253)\tLoss 0.6689 (0.6254)\tAccu 0.6172 (0.6689)\t\n",
            "Epoch: [35][5/10]\tTime 0.164 (0.157)\tData 0.1342 (0.1271)\tLoss 0.6117 (0.6227)\tAccu 0.6953 (0.6742)\t\n",
            "Epoch: [35][6/10]\tTime 0.155 (0.157)\tData 0.1249 (0.1267)\tLoss 0.6151 (0.6214)\tAccu 0.6797 (0.6751)\t\n",
            "Epoch: [35][7/10]\tTime 0.157 (0.157)\tData 0.1262 (0.1266)\tLoss 0.6189 (0.6210)\tAccu 0.6797 (0.6758)\t\n",
            "Epoch: [35][8/10]\tTime 0.156 (0.157)\tData 0.1242 (0.1263)\tLoss 0.6194 (0.6208)\tAccu 0.6484 (0.6724)\t\n",
            "Epoch: [35][9/10]\tTime 0.154 (0.157)\tData 0.1244 (0.1261)\tLoss 0.6094 (0.6196)\tAccu 0.6797 (0.6732)\t\n",
            "Epoch: [35][10/10]\tTime 0.122 (0.153)\tData 0.0924 (0.1228)\tLoss 0.6042 (0.6184)\tAccu 0.6939 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6188\t\n",
            "Epoch: [36][1/10]\tTime 0.158 (0.158)\tData 0.1277 (0.1277)\tLoss 0.5898 (0.5898)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [36][2/10]\tTime 0.156 (0.157)\tData 0.1262 (0.1270)\tLoss 0.6090 (0.5994)\tAccu 0.6914 (0.7012)\t\n",
            "Epoch: [36][3/10]\tTime 0.156 (0.157)\tData 0.1257 (0.1265)\tLoss 0.6696 (0.6228)\tAccu 0.6289 (0.6771)\t\n",
            "Epoch: [36][4/10]\tTime 0.164 (0.159)\tData 0.1339 (0.1284)\tLoss 0.6165 (0.6212)\tAccu 0.6680 (0.6748)\t\n",
            "Epoch: [36][5/10]\tTime 0.152 (0.157)\tData 0.1219 (0.1271)\tLoss 0.6105 (0.6191)\tAccu 0.6875 (0.6773)\t\n",
            "Epoch: [36][6/10]\tTime 0.159 (0.158)\tData 0.1280 (0.1272)\tLoss 0.6219 (0.6195)\tAccu 0.6602 (0.6745)\t\n",
            "Epoch: [36][7/10]\tTime 0.160 (0.158)\tData 0.1291 (0.1275)\tLoss 0.6366 (0.6220)\tAccu 0.6641 (0.6730)\t\n",
            "Epoch: [36][8/10]\tTime 0.163 (0.159)\tData 0.1329 (0.1282)\tLoss 0.6032 (0.6196)\tAccu 0.6680 (0.6724)\t\n",
            "Epoch: [36][9/10]\tTime 0.155 (0.158)\tData 0.1248 (0.1278)\tLoss 0.5930 (0.6167)\tAccu 0.7109 (0.6766)\t\n",
            "Epoch: [36][10/10]\tTime 0.127 (0.155)\tData 0.0979 (0.1248)\tLoss 0.6269 (0.6175)\tAccu 0.6531 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6171\t\n",
            "Epoch: [37][1/10]\tTime 0.156 (0.156)\tData 0.1253 (0.1253)\tLoss 0.6280 (0.6280)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [37][2/10]\tTime 0.150 (0.153)\tData 0.1206 (0.1229)\tLoss 0.6165 (0.6222)\tAccu 0.6758 (0.6680)\t\n",
            "Epoch: [37][3/10]\tTime 0.163 (0.156)\tData 0.1331 (0.1263)\tLoss 0.6069 (0.6171)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [37][4/10]\tTime 0.157 (0.157)\tData 0.1260 (0.1263)\tLoss 0.6104 (0.6154)\tAccu 0.6719 (0.6689)\t\n",
            "Epoch: [37][5/10]\tTime 0.162 (0.158)\tData 0.1318 (0.1274)\tLoss 0.5732 (0.6070)\tAccu 0.7344 (0.6820)\t\n",
            "Epoch: [37][6/10]\tTime 0.158 (0.158)\tData 0.1286 (0.1276)\tLoss 0.5974 (0.6054)\tAccu 0.6953 (0.6842)\t\n",
            "Epoch: [37][7/10]\tTime 0.161 (0.158)\tData 0.1312 (0.1281)\tLoss 0.6132 (0.6065)\tAccu 0.6875 (0.6847)\t\n",
            "Epoch: [37][8/10]\tTime 0.155 (0.158)\tData 0.1244 (0.1276)\tLoss 0.6413 (0.6108)\tAccu 0.6719 (0.6831)\t\n",
            "Epoch: [37][9/10]\tTime 0.163 (0.158)\tData 0.1325 (0.1282)\tLoss 0.6726 (0.6177)\tAccu 0.6094 (0.6749)\t\n",
            "Epoch: [37][10/10]\tTime 0.124 (0.155)\tData 0.0947 (0.1248)\tLoss 0.6194 (0.6178)\tAccu 0.6735 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6173\t\n",
            "Epoch: [38][1/10]\tTime 0.153 (0.153)\tData 0.1224 (0.1224)\tLoss 0.6246 (0.6246)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [38][2/10]\tTime 0.163 (0.158)\tData 0.1322 (0.1273)\tLoss 0.6272 (0.6259)\tAccu 0.6562 (0.6582)\t\n",
            "Epoch: [38][3/10]\tTime 0.152 (0.156)\tData 0.1221 (0.1256)\tLoss 0.6006 (0.6175)\tAccu 0.6875 (0.6680)\t\n",
            "Epoch: [38][4/10]\tTime 0.159 (0.157)\tData 0.1288 (0.1264)\tLoss 0.6171 (0.6174)\tAccu 0.6836 (0.6719)\t\n",
            "Epoch: [38][5/10]\tTime 0.157 (0.157)\tData 0.1267 (0.1264)\tLoss 0.6091 (0.6157)\tAccu 0.7148 (0.6805)\t\n",
            "Epoch: [38][6/10]\tTime 0.155 (0.157)\tData 0.1252 (0.1262)\tLoss 0.6041 (0.6138)\tAccu 0.6992 (0.6836)\t\n",
            "Epoch: [38][7/10]\tTime 0.154 (0.156)\tData 0.1239 (0.1259)\tLoss 0.6517 (0.6192)\tAccu 0.6172 (0.6741)\t\n",
            "Epoch: [38][8/10]\tTime 0.161 (0.157)\tData 0.1297 (0.1264)\tLoss 0.6067 (0.6176)\tAccu 0.6836 (0.6753)\t\n",
            "Epoch: [38][9/10]\tTime 0.159 (0.157)\tData 0.1287 (0.1266)\tLoss 0.6540 (0.6217)\tAccu 0.6367 (0.6710)\t\n",
            "Epoch: [38][10/10]\tTime 0.128 (0.154)\tData 0.0969 (0.1237)\tLoss 0.5732 (0.6179)\tAccu 0.7194 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6172\t\n",
            "Epoch: [39][1/10]\tTime 0.155 (0.155)\tData 0.1239 (0.1239)\tLoss 0.5863 (0.5863)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [39][2/10]\tTime 0.158 (0.157)\tData 0.1275 (0.1257)\tLoss 0.6259 (0.6061)\tAccu 0.6602 (0.6855)\t\n",
            "Epoch: [39][3/10]\tTime 0.155 (0.156)\tData 0.1251 (0.1255)\tLoss 0.6344 (0.6155)\tAccu 0.6641 (0.6784)\t\n",
            "Epoch: [39][4/10]\tTime 0.160 (0.157)\tData 0.1294 (0.1265)\tLoss 0.6132 (0.6150)\tAccu 0.6758 (0.6777)\t\n",
            "Epoch: [39][5/10]\tTime 0.157 (0.157)\tData 0.1265 (0.1265)\tLoss 0.6406 (0.6201)\tAccu 0.6367 (0.6695)\t\n",
            "Epoch: [39][6/10]\tTime 0.161 (0.158)\tData 0.1295 (0.1270)\tLoss 0.6107 (0.6185)\tAccu 0.6641 (0.6686)\t\n",
            "Epoch: [39][7/10]\tTime 0.163 (0.159)\tData 0.1321 (0.1277)\tLoss 0.6191 (0.6186)\tAccu 0.6836 (0.6708)\t\n",
            "Epoch: [39][8/10]\tTime 0.155 (0.158)\tData 0.1245 (0.1273)\tLoss 0.6128 (0.6179)\tAccu 0.6914 (0.6733)\t\n",
            "Epoch: [39][9/10]\tTime 0.159 (0.158)\tData 0.1279 (0.1274)\tLoss 0.6272 (0.6189)\tAccu 0.6562 (0.6714)\t\n",
            "Epoch: [39][10/10]\tTime 0.127 (0.155)\tData 0.0969 (0.1243)\tLoss 0.5981 (0.6173)\tAccu 0.7143 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6192\t\n",
            "Epoch: [40][1/10]\tTime 0.157 (0.157)\tData 0.1260 (0.1260)\tLoss 0.5944 (0.5944)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [40][2/10]\tTime 0.164 (0.161)\tData 0.1337 (0.1298)\tLoss 0.6182 (0.6063)\tAccu 0.6797 (0.6855)\t\n",
            "Epoch: [40][3/10]\tTime 0.151 (0.158)\tData 0.1209 (0.1268)\tLoss 0.6190 (0.6105)\tAccu 0.6719 (0.6810)\t\n",
            "Epoch: [40][4/10]\tTime 0.156 (0.157)\tData 0.1261 (0.1267)\tLoss 0.6061 (0.6094)\tAccu 0.6797 (0.6807)\t\n",
            "Epoch: [40][5/10]\tTime 0.155 (0.157)\tData 0.1245 (0.1262)\tLoss 0.6184 (0.6112)\tAccu 0.6836 (0.6813)\t\n",
            "Epoch: [40][6/10]\tTime 0.162 (0.158)\tData 0.1319 (0.1272)\tLoss 0.6327 (0.6148)\tAccu 0.6484 (0.6758)\t\n",
            "Epoch: [40][7/10]\tTime 0.152 (0.157)\tData 0.1226 (0.1265)\tLoss 0.6524 (0.6202)\tAccu 0.6484 (0.6719)\t\n",
            "Epoch: [40][8/10]\tTime 0.159 (0.157)\tData 0.1275 (0.1266)\tLoss 0.5988 (0.6175)\tAccu 0.6875 (0.6738)\t\n",
            "Epoch: [40][9/10]\tTime 0.158 (0.157)\tData 0.1273 (0.1267)\tLoss 0.6119 (0.6169)\tAccu 0.6797 (0.6745)\t\n",
            "Epoch: [40][10/10]\tTime 0.135 (0.155)\tData 0.1030 (0.1243)\tLoss 0.6176 (0.6169)\tAccu 0.6786 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6183\t\n",
            "Epoch: [41][1/10]\tTime 0.159 (0.159)\tData 0.1286 (0.1286)\tLoss 0.6155 (0.6155)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [41][2/10]\tTime 0.156 (0.157)\tData 0.1252 (0.1269)\tLoss 0.6242 (0.6199)\tAccu 0.6523 (0.6660)\t\n",
            "Epoch: [41][3/10]\tTime 0.160 (0.158)\tData 0.1293 (0.1277)\tLoss 0.5843 (0.6080)\tAccu 0.7109 (0.6810)\t\n",
            "Epoch: [41][4/10]\tTime 0.159 (0.158)\tData 0.1285 (0.1279)\tLoss 0.5996 (0.6059)\tAccu 0.7148 (0.6895)\t\n",
            "Epoch: [41][5/10]\tTime 0.155 (0.158)\tData 0.1244 (0.1272)\tLoss 0.6059 (0.6059)\tAccu 0.6914 (0.6898)\t\n",
            "Epoch: [41][6/10]\tTime 0.155 (0.157)\tData 0.1237 (0.1266)\tLoss 0.6423 (0.6120)\tAccu 0.6484 (0.6829)\t\n",
            "Epoch: [41][7/10]\tTime 0.160 (0.158)\tData 0.1298 (0.1271)\tLoss 0.6534 (0.6179)\tAccu 0.6250 (0.6747)\t\n",
            "Epoch: [41][8/10]\tTime 0.156 (0.157)\tData 0.1260 (0.1269)\tLoss 0.6251 (0.6188)\tAccu 0.6680 (0.6738)\t\n",
            "Epoch: [41][9/10]\tTime 0.158 (0.157)\tData 0.1274 (0.1270)\tLoss 0.6009 (0.6168)\tAccu 0.6836 (0.6749)\t\n",
            "Epoch: [41][10/10]\tTime 0.127 (0.154)\tData 0.0972 (0.1240)\tLoss 0.6075 (0.6161)\tAccu 0.6735 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6171\t\n",
            "Epoch: [42][1/10]\tTime 0.152 (0.152)\tData 0.1224 (0.1224)\tLoss 0.6361 (0.6361)\tAccu 0.6445 (0.6445)\t\n",
            "Epoch: [42][2/10]\tTime 0.164 (0.158)\tData 0.1343 (0.1283)\tLoss 0.6246 (0.6304)\tAccu 0.6641 (0.6543)\t\n",
            "Epoch: [42][3/10]\tTime 0.153 (0.157)\tData 0.1216 (0.1261)\tLoss 0.6225 (0.6277)\tAccu 0.6641 (0.6576)\t\n",
            "Epoch: [42][4/10]\tTime 0.155 (0.156)\tData 0.1257 (0.1260)\tLoss 0.5846 (0.6169)\tAccu 0.7148 (0.6719)\t\n",
            "Epoch: [42][5/10]\tTime 0.149 (0.155)\tData 0.1191 (0.1246)\tLoss 0.6209 (0.6177)\tAccu 0.6758 (0.6727)\t\n",
            "Epoch: [42][6/10]\tTime 0.161 (0.156)\tData 0.1296 (0.1254)\tLoss 0.6142 (0.6171)\tAccu 0.6641 (0.6712)\t\n",
            "Epoch: [42][7/10]\tTime 0.164 (0.157)\tData 0.1336 (0.1266)\tLoss 0.6505 (0.6219)\tAccu 0.6523 (0.6685)\t\n",
            "Epoch: [42][8/10]\tTime 0.154 (0.157)\tData 0.1227 (0.1261)\tLoss 0.6022 (0.6194)\tAccu 0.6836 (0.6704)\t\n",
            "Epoch: [42][9/10]\tTime 0.164 (0.157)\tData 0.1330 (0.1269)\tLoss 0.5965 (0.6169)\tAccu 0.6953 (0.6732)\t\n",
            "Epoch: [42][10/10]\tTime 0.134 (0.155)\tData 0.1054 (0.1247)\tLoss 0.5943 (0.6151)\tAccu 0.6939 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6170\t\n",
            "Epoch: [43][1/10]\tTime 0.159 (0.159)\tData 0.1276 (0.1276)\tLoss 0.5868 (0.5868)\tAccu 0.7266 (0.7266)\t\n",
            "Epoch: [43][2/10]\tTime 0.158 (0.159)\tData 0.1277 (0.1277)\tLoss 0.6724 (0.6296)\tAccu 0.6016 (0.6641)\t\n",
            "Epoch: [43][3/10]\tTime 0.160 (0.159)\tData 0.1298 (0.1284)\tLoss 0.6368 (0.6320)\tAccu 0.6367 (0.6549)\t\n",
            "Epoch: [43][4/10]\tTime 0.154 (0.158)\tData 0.1241 (0.1273)\tLoss 0.6097 (0.6264)\tAccu 0.6836 (0.6621)\t\n",
            "Epoch: [43][5/10]\tTime 0.154 (0.157)\tData 0.1234 (0.1265)\tLoss 0.5925 (0.6197)\tAccu 0.6992 (0.6695)\t\n",
            "Epoch: [43][6/10]\tTime 0.160 (0.158)\tData 0.1301 (0.1271)\tLoss 0.5859 (0.6140)\tAccu 0.7109 (0.6764)\t\n",
            "Epoch: [43][7/10]\tTime 0.159 (0.158)\tData 0.1290 (0.1274)\tLoss 0.6057 (0.6128)\tAccu 0.6953 (0.6791)\t\n",
            "Epoch: [43][8/10]\tTime 0.158 (0.158)\tData 0.1277 (0.1274)\tLoss 0.6358 (0.6157)\tAccu 0.6562 (0.6763)\t\n",
            "Epoch: [43][9/10]\tTime 0.154 (0.157)\tData 0.1235 (0.1270)\tLoss 0.6176 (0.6159)\tAccu 0.6523 (0.6736)\t\n",
            "Epoch: [43][10/10]\tTime 0.127 (0.154)\tData 0.0972 (0.1240)\tLoss 0.6071 (0.6152)\tAccu 0.6888 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6171\t\n",
            "Epoch: [44][1/10]\tTime 0.161 (0.161)\tData 0.1302 (0.1302)\tLoss 0.5667 (0.5667)\tAccu 0.7266 (0.7266)\t\n",
            "Epoch: [44][2/10]\tTime 0.159 (0.160)\tData 0.1286 (0.1294)\tLoss 0.6082 (0.5875)\tAccu 0.6836 (0.7051)\t\n",
            "Epoch: [44][3/10]\tTime 0.155 (0.158)\tData 0.1248 (0.1278)\tLoss 0.6235 (0.5995)\tAccu 0.6680 (0.6927)\t\n",
            "Epoch: [44][4/10]\tTime 0.160 (0.159)\tData 0.1295 (0.1283)\tLoss 0.6022 (0.6001)\tAccu 0.6875 (0.6914)\t\n",
            "Epoch: [44][5/10]\tTime 0.155 (0.158)\tData 0.1241 (0.1274)\tLoss 0.6261 (0.6053)\tAccu 0.6719 (0.6875)\t\n",
            "Epoch: [44][6/10]\tTime 0.156 (0.158)\tData 0.1252 (0.1271)\tLoss 0.6010 (0.6046)\tAccu 0.6953 (0.6888)\t\n",
            "Epoch: [44][7/10]\tTime 0.153 (0.157)\tData 0.1232 (0.1265)\tLoss 0.6413 (0.6099)\tAccu 0.6289 (0.6802)\t\n",
            "Epoch: [44][8/10]\tTime 0.160 (0.157)\tData 0.1285 (0.1268)\tLoss 0.6177 (0.6108)\tAccu 0.6680 (0.6787)\t\n",
            "Epoch: [44][9/10]\tTime 0.158 (0.157)\tData 0.1275 (0.1268)\tLoss 0.6308 (0.6131)\tAccu 0.6602 (0.6766)\t\n",
            "Epoch: [44][10/10]\tTime 0.126 (0.154)\tData 0.0965 (0.1238)\tLoss 0.6247 (0.6140)\tAccu 0.6531 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6202\t\n",
            "Epoch: [45][1/10]\tTime 0.155 (0.155)\tData 0.1239 (0.1239)\tLoss 0.6075 (0.6075)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [45][2/10]\tTime 0.157 (0.156)\tData 0.1267 (0.1253)\tLoss 0.5908 (0.5991)\tAccu 0.6992 (0.6895)\t\n",
            "Epoch: [45][3/10]\tTime 0.155 (0.155)\tData 0.1236 (0.1247)\tLoss 0.6100 (0.6027)\tAccu 0.7109 (0.6966)\t\n",
            "Epoch: [45][4/10]\tTime 0.155 (0.155)\tData 0.1241 (0.1246)\tLoss 0.5813 (0.5974)\tAccu 0.7227 (0.7031)\t\n",
            "Epoch: [45][5/10]\tTime 0.156 (0.155)\tData 0.1258 (0.1248)\tLoss 0.6133 (0.6006)\tAccu 0.6523 (0.6930)\t\n",
            "Epoch: [45][6/10]\tTime 0.164 (0.157)\tData 0.1334 (0.1263)\tLoss 0.6232 (0.6043)\tAccu 0.6562 (0.6868)\t\n",
            "Epoch: [45][7/10]\tTime 0.160 (0.157)\tData 0.1296 (0.1267)\tLoss 0.6503 (0.6109)\tAccu 0.6445 (0.6808)\t\n",
            "Epoch: [45][8/10]\tTime 0.156 (0.157)\tData 0.1253 (0.1266)\tLoss 0.6709 (0.6184)\tAccu 0.6289 (0.6743)\t\n",
            "Epoch: [45][9/10]\tTime 0.157 (0.157)\tData 0.1259 (0.1265)\tLoss 0.5818 (0.6143)\tAccu 0.7148 (0.6788)\t\n",
            "Epoch: [45][10/10]\tTime 0.132 (0.155)\tData 0.1010 (0.1239)\tLoss 0.6494 (0.6171)\tAccu 0.6276 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6222\t\n",
            "Epoch: [46][1/10]\tTime 0.157 (0.157)\tData 0.1257 (0.1257)\tLoss 0.5982 (0.5982)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [46][2/10]\tTime 0.154 (0.155)\tData 0.1233 (0.1245)\tLoss 0.6330 (0.6156)\tAccu 0.6719 (0.6816)\t\n",
            "Epoch: [46][3/10]\tTime 0.157 (0.156)\tData 0.1268 (0.1252)\tLoss 0.6292 (0.6201)\tAccu 0.6758 (0.6797)\t\n",
            "Epoch: [46][4/10]\tTime 0.154 (0.155)\tData 0.1243 (0.1250)\tLoss 0.5890 (0.6123)\tAccu 0.7266 (0.6914)\t\n",
            "Epoch: [46][5/10]\tTime 0.159 (0.156)\tData 0.1275 (0.1255)\tLoss 0.6149 (0.6128)\tAccu 0.6602 (0.6852)\t\n",
            "Epoch: [46][6/10]\tTime 0.161 (0.157)\tData 0.1314 (0.1265)\tLoss 0.6262 (0.6151)\tAccu 0.6680 (0.6823)\t\n",
            "Epoch: [46][7/10]\tTime 0.155 (0.157)\tData 0.1255 (0.1263)\tLoss 0.6130 (0.6148)\tAccu 0.6641 (0.6797)\t\n",
            "Epoch: [46][8/10]\tTime 0.160 (0.157)\tData 0.1298 (0.1268)\tLoss 0.6096 (0.6141)\tAccu 0.6836 (0.6802)\t\n",
            "Epoch: [46][9/10]\tTime 0.155 (0.157)\tData 0.1246 (0.1265)\tLoss 0.6205 (0.6148)\tAccu 0.6680 (0.6788)\t\n",
            "Epoch: [46][10/10]\tTime 0.126 (0.154)\tData 0.0962 (0.1235)\tLoss 0.6435 (0.6171)\tAccu 0.6276 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6169\t\n",
            "Epoch: [47][1/10]\tTime 0.154 (0.154)\tData 0.1244 (0.1244)\tLoss 0.5570 (0.5570)\tAccu 0.7344 (0.7344)\t\n",
            "Epoch: [47][2/10]\tTime 0.160 (0.157)\tData 0.1290 (0.1267)\tLoss 0.6380 (0.5975)\tAccu 0.6641 (0.6992)\t\n",
            "Epoch: [47][3/10]\tTime 0.160 (0.158)\tData 0.1306 (0.1280)\tLoss 0.6153 (0.6034)\tAccu 0.6875 (0.6953)\t\n",
            "Epoch: [47][4/10]\tTime 0.155 (0.157)\tData 0.1244 (0.1271)\tLoss 0.5979 (0.6020)\tAccu 0.6719 (0.6895)\t\n",
            "Epoch: [47][5/10]\tTime 0.159 (0.158)\tData 0.1289 (0.1274)\tLoss 0.6153 (0.6047)\tAccu 0.6680 (0.6852)\t\n",
            "Epoch: [47][6/10]\tTime 0.164 (0.159)\tData 0.1325 (0.1283)\tLoss 0.6548 (0.6130)\tAccu 0.6445 (0.6784)\t\n",
            "Epoch: [47][7/10]\tTime 0.161 (0.159)\tData 0.1309 (0.1286)\tLoss 0.6140 (0.6132)\tAccu 0.6680 (0.6769)\t\n",
            "Epoch: [47][8/10]\tTime 0.157 (0.159)\tData 0.1258 (0.1283)\tLoss 0.6183 (0.6138)\tAccu 0.6602 (0.6748)\t\n",
            "Epoch: [47][9/10]\tTime 0.160 (0.159)\tData 0.1299 (0.1285)\tLoss 0.6158 (0.6140)\tAccu 0.6523 (0.6723)\t\n",
            "Epoch: [47][10/10]\tTime 0.127 (0.156)\tData 0.0978 (0.1254)\tLoss 0.5955 (0.6126)\tAccu 0.7041 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6170\t\n",
            "Epoch: [48][1/10]\tTime 0.157 (0.157)\tData 0.1261 (0.1261)\tLoss 0.6079 (0.6079)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [48][2/10]\tTime 0.171 (0.164)\tData 0.1412 (0.1336)\tLoss 0.5912 (0.5995)\tAccu 0.6914 (0.6836)\t\n",
            "Epoch: [48][3/10]\tTime 0.154 (0.161)\tData 0.1233 (0.1302)\tLoss 0.5963 (0.5984)\tAccu 0.7031 (0.6901)\t\n",
            "Epoch: [48][4/10]\tTime 0.165 (0.162)\tData 0.1333 (0.1310)\tLoss 0.5988 (0.5985)\tAccu 0.6875 (0.6895)\t\n",
            "Epoch: [48][5/10]\tTime 0.160 (0.161)\tData 0.1266 (0.1301)\tLoss 0.6427 (0.6074)\tAccu 0.6758 (0.6867)\t\n",
            "Epoch: [48][6/10]\tTime 0.170 (0.163)\tData 0.1374 (0.1313)\tLoss 0.6174 (0.6090)\tAccu 0.6758 (0.6849)\t\n",
            "Epoch: [48][7/10]\tTime 0.155 (0.162)\tData 0.1249 (0.1304)\tLoss 0.6154 (0.6100)\tAccu 0.6680 (0.6825)\t\n",
            "Epoch: [48][8/10]\tTime 0.158 (0.161)\tData 0.1279 (0.1301)\tLoss 0.6235 (0.6116)\tAccu 0.6484 (0.6782)\t\n",
            "Epoch: [48][9/10]\tTime 0.150 (0.160)\tData 0.1203 (0.1290)\tLoss 0.6258 (0.6132)\tAccu 0.6445 (0.6745)\t\n",
            "Epoch: [48][10/10]\tTime 0.121 (0.156)\tData 0.0913 (0.1252)\tLoss 0.6271 (0.6143)\tAccu 0.6786 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6282\t\n",
            "Epoch: [49][1/10]\tTime 0.151 (0.151)\tData 0.1209 (0.1209)\tLoss 0.6232 (0.6232)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [49][2/10]\tTime 0.150 (0.150)\tData 0.1206 (0.1207)\tLoss 0.6118 (0.6175)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [49][3/10]\tTime 0.155 (0.152)\tData 0.1251 (0.1222)\tLoss 0.5999 (0.6116)\tAccu 0.6914 (0.6862)\t\n",
            "Epoch: [49][4/10]\tTime 0.156 (0.153)\tData 0.1248 (0.1228)\tLoss 0.5921 (0.6067)\tAccu 0.6875 (0.6865)\t\n",
            "Epoch: [49][5/10]\tTime 0.154 (0.153)\tData 0.1240 (0.1231)\tLoss 0.6427 (0.6139)\tAccu 0.6367 (0.6766)\t\n",
            "Epoch: [49][6/10]\tTime 0.152 (0.153)\tData 0.1223 (0.1229)\tLoss 0.6576 (0.6212)\tAccu 0.6367 (0.6699)\t\n",
            "Epoch: [49][7/10]\tTime 0.156 (0.153)\tData 0.1261 (0.1234)\tLoss 0.6007 (0.6183)\tAccu 0.6914 (0.6730)\t\n",
            "Epoch: [49][8/10]\tTime 0.151 (0.153)\tData 0.1212 (0.1231)\tLoss 0.6792 (0.6259)\tAccu 0.6406 (0.6689)\t\n",
            "Epoch: [49][9/10]\tTime 0.155 (0.153)\tData 0.1243 (0.1232)\tLoss 0.5867 (0.6215)\tAccu 0.7070 (0.6732)\t\n",
            "Epoch: [49][10/10]\tTime 0.122 (0.150)\tData 0.0930 (0.1202)\tLoss 0.5878 (0.6189)\tAccu 0.6939 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6189\t\n",
            "Epoch: [50][1/10]\tTime 0.148 (0.148)\tData 0.1176 (0.1176)\tLoss 0.6546 (0.6546)\tAccu 0.6328 (0.6328)\t\n",
            "Epoch: [50][2/10]\tTime 0.157 (0.152)\tData 0.1265 (0.1221)\tLoss 0.5813 (0.6180)\tAccu 0.7070 (0.6699)\t\n",
            "Epoch: [50][3/10]\tTime 0.151 (0.152)\tData 0.1215 (0.1219)\tLoss 0.6301 (0.6220)\tAccu 0.6367 (0.6589)\t\n",
            "Epoch: [50][4/10]\tTime 0.152 (0.152)\tData 0.1206 (0.1216)\tLoss 0.6086 (0.6187)\tAccu 0.6680 (0.6611)\t\n",
            "Epoch: [50][5/10]\tTime 0.153 (0.152)\tData 0.1227 (0.1218)\tLoss 0.6246 (0.6199)\tAccu 0.6641 (0.6617)\t\n",
            "Epoch: [50][6/10]\tTime 0.153 (0.152)\tData 0.1213 (0.1217)\tLoss 0.6041 (0.6172)\tAccu 0.6914 (0.6667)\t\n",
            "Epoch: [50][7/10]\tTime 0.148 (0.152)\tData 0.1184 (0.1212)\tLoss 0.6232 (0.6181)\tAccu 0.6758 (0.6680)\t\n",
            "Epoch: [50][8/10]\tTime 0.151 (0.152)\tData 0.1203 (0.1211)\tLoss 0.6007 (0.6159)\tAccu 0.6797 (0.6694)\t\n",
            "Epoch: [50][9/10]\tTime 0.153 (0.152)\tData 0.1222 (0.1212)\tLoss 0.5699 (0.6108)\tAccu 0.7383 (0.6771)\t\n",
            "Epoch: [50][10/10]\tTime 0.120 (0.149)\tData 0.0911 (0.1182)\tLoss 0.6276 (0.6121)\tAccu 0.6480 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6172\t\n",
            "Epoch: [51][1/10]\tTime 0.154 (0.154)\tData 0.1245 (0.1245)\tLoss 0.6378 (0.6378)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [51][2/10]\tTime 0.160 (0.157)\tData 0.1296 (0.1270)\tLoss 0.5906 (0.6142)\tAccu 0.6914 (0.6797)\t\n",
            "Epoch: [51][3/10]\tTime 0.152 (0.156)\tData 0.1227 (0.1256)\tLoss 0.5925 (0.6070)\tAccu 0.6875 (0.6823)\t\n",
            "Epoch: [51][4/10]\tTime 0.158 (0.156)\tData 0.1284 (0.1263)\tLoss 0.5968 (0.6045)\tAccu 0.6797 (0.6816)\t\n",
            "Epoch: [51][5/10]\tTime 0.152 (0.155)\tData 0.1217 (0.1254)\tLoss 0.6230 (0.6082)\tAccu 0.6523 (0.6758)\t\n",
            "Epoch: [51][6/10]\tTime 0.155 (0.155)\tData 0.1250 (0.1253)\tLoss 0.6185 (0.6099)\tAccu 0.6641 (0.6738)\t\n",
            "Epoch: [51][7/10]\tTime 0.153 (0.155)\tData 0.1228 (0.1249)\tLoss 0.5943 (0.6077)\tAccu 0.7109 (0.6791)\t\n",
            "Epoch: [51][8/10]\tTime 0.159 (0.156)\tData 0.1276 (0.1253)\tLoss 0.6348 (0.6110)\tAccu 0.6367 (0.6738)\t\n",
            "Epoch: [51][9/10]\tTime 0.159 (0.156)\tData 0.1289 (0.1257)\tLoss 0.6186 (0.6119)\tAccu 0.6680 (0.6732)\t\n",
            "Epoch: [51][10/10]\tTime 0.124 (0.153)\tData 0.0947 (0.1226)\tLoss 0.6084 (0.6116)\tAccu 0.6939 (0.6748)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6166\t\n",
            "Epoch: [52][1/10]\tTime 0.156 (0.156)\tData 0.1266 (0.1266)\tLoss 0.6338 (0.6338)\tAccu 0.6484 (0.6484)\t\n",
            "Epoch: [52][2/10]\tTime 0.157 (0.157)\tData 0.1260 (0.1263)\tLoss 0.6200 (0.6269)\tAccu 0.6523 (0.6504)\t\n",
            "Epoch: [52][3/10]\tTime 0.163 (0.159)\tData 0.1330 (0.1285)\tLoss 0.6203 (0.6247)\tAccu 0.6484 (0.6497)\t\n",
            "Epoch: [52][4/10]\tTime 0.158 (0.159)\tData 0.1267 (0.1281)\tLoss 0.6275 (0.6254)\tAccu 0.6680 (0.6543)\t\n",
            "Epoch: [52][5/10]\tTime 0.156 (0.158)\tData 0.1261 (0.1277)\tLoss 0.5934 (0.6190)\tAccu 0.6914 (0.6617)\t\n",
            "Epoch: [52][6/10]\tTime 0.155 (0.157)\tData 0.1247 (0.1272)\tLoss 0.5650 (0.6100)\tAccu 0.7344 (0.6738)\t\n",
            "Epoch: [52][7/10]\tTime 0.157 (0.157)\tData 0.1262 (0.1270)\tLoss 0.6544 (0.6163)\tAccu 0.6328 (0.6680)\t\n",
            "Epoch: [52][8/10]\tTime 0.159 (0.158)\tData 0.1290 (0.1273)\tLoss 0.6060 (0.6150)\tAccu 0.6914 (0.6709)\t\n",
            "Epoch: [52][9/10]\tTime 0.163 (0.158)\tData 0.1304 (0.1276)\tLoss 0.5483 (0.6076)\tAccu 0.7461 (0.6793)\t\n",
            "Epoch: [52][10/10]\tTime 0.136 (0.156)\tData 0.1076 (0.1256)\tLoss 0.6602 (0.6117)\tAccu 0.6224 (0.6748)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6170\t\n",
            "Epoch: [53][1/10]\tTime 0.152 (0.152)\tData 0.1218 (0.1218)\tLoss 0.6331 (0.6331)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [53][2/10]\tTime 0.163 (0.157)\tData 0.1328 (0.1273)\tLoss 0.5407 (0.5869)\tAccu 0.7578 (0.7129)\t\n",
            "Epoch: [53][3/10]\tTime 0.154 (0.156)\tData 0.1240 (0.1262)\tLoss 0.6080 (0.5939)\tAccu 0.6719 (0.6992)\t\n",
            "Epoch: [53][4/10]\tTime 0.156 (0.156)\tData 0.1249 (0.1259)\tLoss 0.6464 (0.6070)\tAccu 0.6172 (0.6787)\t\n",
            "Epoch: [53][5/10]\tTime 0.159 (0.157)\tData 0.1287 (0.1265)\tLoss 0.6025 (0.6061)\tAccu 0.6875 (0.6805)\t\n",
            "Epoch: [53][6/10]\tTime 0.157 (0.157)\tData 0.1272 (0.1266)\tLoss 0.6310 (0.6103)\tAccu 0.6445 (0.6745)\t\n",
            "Epoch: [53][7/10]\tTime 0.156 (0.157)\tData 0.1257 (0.1264)\tLoss 0.5955 (0.6082)\tAccu 0.6953 (0.6775)\t\n",
            "Epoch: [53][8/10]\tTime 0.158 (0.157)\tData 0.1270 (0.1265)\tLoss 0.6524 (0.6137)\tAccu 0.6406 (0.6729)\t\n",
            "Epoch: [53][9/10]\tTime 0.155 (0.157)\tData 0.1247 (0.1263)\tLoss 0.5971 (0.6119)\tAccu 0.6836 (0.6740)\t\n",
            "Epoch: [53][10/10]\tTime 0.131 (0.154)\tData 0.1004 (0.1237)\tLoss 0.5988 (0.6108)\tAccu 0.6837 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6160\t\n",
            "Epoch: [54][1/10]\tTime 0.169 (0.169)\tData 0.1389 (0.1389)\tLoss 0.5905 (0.5905)\tAccu 0.6875 (0.6875)\t\n",
            "Epoch: [54][2/10]\tTime 0.149 (0.159)\tData 0.1197 (0.1293)\tLoss 0.6113 (0.6009)\tAccu 0.6680 (0.6777)\t\n",
            "Epoch: [54][3/10]\tTime 0.155 (0.158)\tData 0.1248 (0.1278)\tLoss 0.6004 (0.6007)\tAccu 0.6836 (0.6797)\t\n",
            "Epoch: [54][4/10]\tTime 0.168 (0.160)\tData 0.1378 (0.1303)\tLoss 0.6398 (0.6105)\tAccu 0.6523 (0.6729)\t\n",
            "Epoch: [54][5/10]\tTime 0.156 (0.160)\tData 0.1261 (0.1295)\tLoss 0.6330 (0.6150)\tAccu 0.6484 (0.6680)\t\n",
            "Epoch: [54][6/10]\tTime 0.160 (0.160)\tData 0.1293 (0.1294)\tLoss 0.5919 (0.6112)\tAccu 0.7031 (0.6738)\t\n",
            "Epoch: [54][7/10]\tTime 0.168 (0.161)\tData 0.1377 (0.1306)\tLoss 0.5921 (0.6084)\tAccu 0.7109 (0.6791)\t\n",
            "Epoch: [54][8/10]\tTime 0.154 (0.160)\tData 0.1235 (0.1297)\tLoss 0.5932 (0.6065)\tAccu 0.7188 (0.6841)\t\n",
            "Epoch: [54][9/10]\tTime 0.158 (0.160)\tData 0.1264 (0.1294)\tLoss 0.6695 (0.6135)\tAccu 0.5977 (0.6745)\t\n",
            "Epoch: [54][10/10]\tTime 0.139 (0.158)\tData 0.1104 (0.1275)\tLoss 0.5986 (0.6124)\tAccu 0.6786 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6174\t\n",
            "Epoch: [55][1/10]\tTime 0.159 (0.159)\tData 0.1282 (0.1282)\tLoss 0.6191 (0.6191)\tAccu 0.6484 (0.6484)\t\n",
            "Epoch: [55][2/10]\tTime 0.159 (0.159)\tData 0.1279 (0.1281)\tLoss 0.6148 (0.6169)\tAccu 0.6719 (0.6602)\t\n",
            "Epoch: [55][3/10]\tTime 0.158 (0.158)\tData 0.1281 (0.1281)\tLoss 0.6072 (0.6137)\tAccu 0.6875 (0.6693)\t\n",
            "Epoch: [55][4/10]\tTime 0.158 (0.158)\tData 0.1282 (0.1281)\tLoss 0.5946 (0.6089)\tAccu 0.7070 (0.6787)\t\n",
            "Epoch: [55][5/10]\tTime 0.154 (0.158)\tData 0.1244 (0.1274)\tLoss 0.6435 (0.6158)\tAccu 0.6445 (0.6719)\t\n",
            "Epoch: [55][6/10]\tTime 0.159 (0.158)\tData 0.1282 (0.1275)\tLoss 0.6134 (0.6154)\tAccu 0.6641 (0.6706)\t\n",
            "Epoch: [55][7/10]\tTime 0.152 (0.157)\tData 0.1224 (0.1268)\tLoss 0.6442 (0.6195)\tAccu 0.6367 (0.6657)\t\n",
            "Epoch: [55][8/10]\tTime 0.165 (0.158)\tData 0.1330 (0.1275)\tLoss 0.5823 (0.6149)\tAccu 0.7305 (0.6738)\t\n",
            "Epoch: [55][9/10]\tTime 0.158 (0.158)\tData 0.1272 (0.1275)\tLoss 0.6151 (0.6149)\tAccu 0.6523 (0.6714)\t\n",
            "Epoch: [55][10/10]\tTime 0.125 (0.155)\tData 0.0961 (0.1244)\tLoss 0.5643 (0.6109)\tAccu 0.7143 (0.6748)\t\n",
            "Time 0.042\tAccu 0.6800\tLoss 0.6157\t\n",
            "Epoch: [56][1/10]\tTime 0.156 (0.156)\tData 0.1257 (0.1257)\tLoss 0.6106 (0.6106)\tAccu 0.6562 (0.6562)\t\n",
            "Epoch: [56][2/10]\tTime 0.162 (0.159)\tData 0.1327 (0.1292)\tLoss 0.6128 (0.6117)\tAccu 0.6562 (0.6562)\t\n",
            "Epoch: [56][3/10]\tTime 0.165 (0.161)\tData 0.1347 (0.1310)\tLoss 0.6364 (0.6199)\tAccu 0.6523 (0.6549)\t\n",
            "Epoch: [56][4/10]\tTime 0.158 (0.160)\tData 0.1275 (0.1301)\tLoss 0.6273 (0.6218)\tAccu 0.6484 (0.6533)\t\n",
            "Epoch: [56][5/10]\tTime 0.158 (0.160)\tData 0.1267 (0.1295)\tLoss 0.5986 (0.6171)\tAccu 0.6875 (0.6602)\t\n",
            "Epoch: [56][6/10]\tTime 0.157 (0.159)\tData 0.1265 (0.1290)\tLoss 0.5893 (0.6125)\tAccu 0.7031 (0.6673)\t\n",
            "Epoch: [56][7/10]\tTime 0.156 (0.159)\tData 0.1258 (0.1285)\tLoss 0.6157 (0.6129)\tAccu 0.6875 (0.6702)\t\n",
            "Epoch: [56][8/10]\tTime 0.157 (0.159)\tData 0.1273 (0.1284)\tLoss 0.5592 (0.6062)\tAccu 0.7422 (0.6792)\t\n",
            "Epoch: [56][9/10]\tTime 0.155 (0.158)\tData 0.1249 (0.1280)\tLoss 0.6483 (0.6109)\tAccu 0.6406 (0.6749)\t\n",
            "Epoch: [56][10/10]\tTime 0.126 (0.155)\tData 0.0977 (0.1249)\tLoss 0.6118 (0.6110)\tAccu 0.6735 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6177\t\n",
            "Epoch: [57][1/10]\tTime 0.157 (0.157)\tData 0.1264 (0.1264)\tLoss 0.5922 (0.5922)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [57][2/10]\tTime 0.156 (0.156)\tData 0.1250 (0.1257)\tLoss 0.5872 (0.5897)\tAccu 0.6953 (0.6875)\t\n",
            "Epoch: [57][3/10]\tTime 0.155 (0.156)\tData 0.1242 (0.1252)\tLoss 0.5625 (0.5806)\tAccu 0.7227 (0.6992)\t\n",
            "Epoch: [57][4/10]\tTime 0.163 (0.157)\tData 0.1322 (0.1270)\tLoss 0.5967 (0.5847)\tAccu 0.6953 (0.6982)\t\n",
            "Epoch: [57][5/10]\tTime 0.157 (0.157)\tData 0.1264 (0.1268)\tLoss 0.6360 (0.5949)\tAccu 0.6523 (0.6891)\t\n",
            "Epoch: [57][6/10]\tTime 0.158 (0.157)\tData 0.1275 (0.1269)\tLoss 0.6420 (0.6028)\tAccu 0.6445 (0.6816)\t\n",
            "Epoch: [57][7/10]\tTime 0.156 (0.157)\tData 0.1249 (0.1267)\tLoss 0.6332 (0.6071)\tAccu 0.6367 (0.6752)\t\n",
            "Epoch: [57][8/10]\tTime 0.155 (0.157)\tData 0.1248 (0.1264)\tLoss 0.6020 (0.6065)\tAccu 0.6953 (0.6777)\t\n",
            "Epoch: [57][9/10]\tTime 0.155 (0.157)\tData 0.1244 (0.1262)\tLoss 0.6241 (0.6084)\tAccu 0.6445 (0.6740)\t\n",
            "Epoch: [57][10/10]\tTime 0.127 (0.154)\tData 0.0978 (0.1234)\tLoss 0.6020 (0.6079)\tAccu 0.6837 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6170\t\n",
            "Epoch: [58][1/10]\tTime 0.161 (0.161)\tData 0.1310 (0.1310)\tLoss 0.6028 (0.6028)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [58][2/10]\tTime 0.156 (0.158)\tData 0.1247 (0.1278)\tLoss 0.5893 (0.5960)\tAccu 0.7070 (0.6895)\t\n",
            "Epoch: [58][3/10]\tTime 0.158 (0.158)\tData 0.1279 (0.1279)\tLoss 0.6209 (0.6043)\tAccu 0.6758 (0.6849)\t\n",
            "Epoch: [58][4/10]\tTime 0.156 (0.158)\tData 0.1226 (0.1265)\tLoss 0.6199 (0.6082)\tAccu 0.6680 (0.6807)\t\n",
            "Epoch: [58][5/10]\tTime 0.161 (0.158)\tData 0.1308 (0.1274)\tLoss 0.6148 (0.6095)\tAccu 0.6758 (0.6797)\t\n",
            "Epoch: [58][6/10]\tTime 0.163 (0.159)\tData 0.1318 (0.1281)\tLoss 0.6382 (0.6143)\tAccu 0.6328 (0.6719)\t\n",
            "Epoch: [58][7/10]\tTime 0.160 (0.159)\tData 0.1299 (0.1284)\tLoss 0.6012 (0.6124)\tAccu 0.6914 (0.6747)\t\n",
            "Epoch: [58][8/10]\tTime 0.158 (0.159)\tData 0.1278 (0.1283)\tLoss 0.6176 (0.6131)\tAccu 0.6484 (0.6714)\t\n",
            "Epoch: [58][9/10]\tTime 0.159 (0.159)\tData 0.1276 (0.1282)\tLoss 0.6011 (0.6117)\tAccu 0.7031 (0.6749)\t\n",
            "Epoch: [58][10/10]\tTime 0.122 (0.155)\tData 0.0921 (0.1246)\tLoss 0.6133 (0.6119)\tAccu 0.6735 (0.6748)\t\n",
            "Time 0.042\tAccu 0.6800\tLoss 0.6148\t\n",
            "Epoch: [59][1/10]\tTime 0.154 (0.154)\tData 0.1223 (0.1223)\tLoss 0.6160 (0.6160)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [59][2/10]\tTime 0.157 (0.156)\tData 0.1258 (0.1241)\tLoss 0.5682 (0.5921)\tAccu 0.7070 (0.6953)\t\n",
            "Epoch: [59][3/10]\tTime 0.153 (0.155)\tData 0.1229 (0.1237)\tLoss 0.6202 (0.6015)\tAccu 0.6680 (0.6862)\t\n",
            "Epoch: [59][4/10]\tTime 0.165 (0.157)\tData 0.1339 (0.1262)\tLoss 0.6098 (0.6036)\tAccu 0.6797 (0.6846)\t\n",
            "Epoch: [59][5/10]\tTime 0.157 (0.157)\tData 0.1261 (0.1262)\tLoss 0.5932 (0.6015)\tAccu 0.6602 (0.6797)\t\n",
            "Epoch: [59][6/10]\tTime 0.155 (0.157)\tData 0.1249 (0.1260)\tLoss 0.6272 (0.6058)\tAccu 0.6562 (0.6758)\t\n",
            "Epoch: [59][7/10]\tTime 0.161 (0.158)\tData 0.1304 (0.1266)\tLoss 0.6166 (0.6073)\tAccu 0.6680 (0.6747)\t\n",
            "Epoch: [59][8/10]\tTime 0.157 (0.157)\tData 0.1274 (0.1267)\tLoss 0.6166 (0.6085)\tAccu 0.6758 (0.6748)\t\n",
            "Epoch: [59][9/10]\tTime 0.158 (0.158)\tData 0.1268 (0.1267)\tLoss 0.6362 (0.6116)\tAccu 0.6641 (0.6736)\t\n",
            "Epoch: [59][10/10]\tTime 0.129 (0.155)\tData 0.0997 (0.1240)\tLoss 0.6149 (0.6118)\tAccu 0.6888 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6144\t\n",
            "Epoch: [60][1/10]\tTime 0.187 (0.187)\tData 0.1560 (0.1560)\tLoss 0.6189 (0.6189)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [60][2/10]\tTime 0.150 (0.169)\tData 0.1201 (0.1381)\tLoss 0.5987 (0.6088)\tAccu 0.7148 (0.6875)\t\n",
            "Epoch: [60][3/10]\tTime 0.155 (0.164)\tData 0.1247 (0.1336)\tLoss 0.5885 (0.6021)\tAccu 0.6992 (0.6914)\t\n",
            "Epoch: [60][4/10]\tTime 0.151 (0.161)\tData 0.1209 (0.1304)\tLoss 0.6373 (0.6109)\tAccu 0.6445 (0.6797)\t\n",
            "Epoch: [60][5/10]\tTime 0.156 (0.160)\tData 0.1249 (0.1293)\tLoss 0.5580 (0.6003)\tAccu 0.7148 (0.6867)\t\n",
            "Epoch: [60][6/10]\tTime 0.150 (0.158)\tData 0.1197 (0.1277)\tLoss 0.6497 (0.6085)\tAccu 0.6406 (0.6790)\t\n",
            "Epoch: [60][7/10]\tTime 0.157 (0.158)\tData 0.1269 (0.1276)\tLoss 0.5964 (0.6068)\tAccu 0.6875 (0.6802)\t\n",
            "Epoch: [60][8/10]\tTime 0.157 (0.158)\tData 0.1267 (0.1275)\tLoss 0.6139 (0.6077)\tAccu 0.6641 (0.6782)\t\n",
            "Epoch: [60][9/10]\tTime 0.153 (0.157)\tData 0.1222 (0.1269)\tLoss 0.6216 (0.6092)\tAccu 0.6797 (0.6784)\t\n",
            "Epoch: [60][10/10]\tTime 0.122 (0.154)\tData 0.0930 (0.1235)\tLoss 0.6216 (0.6102)\tAccu 0.6327 (0.6748)\t\n",
            "Time 0.042\tAccu 0.6800\tLoss 0.6161\t\n",
            "Epoch: [61][1/10]\tTime 0.149 (0.149)\tData 0.1195 (0.1195)\tLoss 0.5875 (0.5875)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [61][2/10]\tTime 0.154 (0.152)\tData 0.1235 (0.1215)\tLoss 0.6297 (0.6086)\tAccu 0.6484 (0.6738)\t\n",
            "Epoch: [61][3/10]\tTime 0.157 (0.153)\tData 0.1263 (0.1231)\tLoss 0.5975 (0.6049)\tAccu 0.6523 (0.6667)\t\n",
            "Epoch: [61][4/10]\tTime 0.164 (0.156)\tData 0.1333 (0.1257)\tLoss 0.6358 (0.6126)\tAccu 0.6758 (0.6689)\t\n",
            "Epoch: [61][5/10]\tTime 0.156 (0.156)\tData 0.1257 (0.1257)\tLoss 0.6163 (0.6134)\tAccu 0.6875 (0.6727)\t\n",
            "Epoch: [61][6/10]\tTime 0.153 (0.155)\tData 0.1223 (0.1251)\tLoss 0.6256 (0.6154)\tAccu 0.6641 (0.6712)\t\n",
            "Epoch: [61][7/10]\tTime 0.153 (0.155)\tData 0.1231 (0.1248)\tLoss 0.6169 (0.6156)\tAccu 0.6484 (0.6680)\t\n",
            "Epoch: [61][8/10]\tTime 0.153 (0.155)\tData 0.1224 (0.1245)\tLoss 0.5988 (0.6135)\tAccu 0.6836 (0.6699)\t\n",
            "Epoch: [61][9/10]\tTime 0.154 (0.155)\tData 0.1220 (0.1242)\tLoss 0.5983 (0.6118)\tAccu 0.6953 (0.6727)\t\n",
            "Epoch: [61][10/10]\tTime 0.127 (0.152)\tData 0.0966 (0.1215)\tLoss 0.5941 (0.6104)\tAccu 0.6990 (0.6748)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6147\t\n",
            "Epoch: [62][1/10]\tTime 0.153 (0.153)\tData 0.1236 (0.1236)\tLoss 0.6123 (0.6123)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [62][2/10]\tTime 0.150 (0.152)\tData 0.1202 (0.1219)\tLoss 0.5813 (0.5968)\tAccu 0.7148 (0.6875)\t\n",
            "Epoch: [62][3/10]\tTime 0.152 (0.152)\tData 0.1227 (0.1222)\tLoss 0.6024 (0.5987)\tAccu 0.6992 (0.6914)\t\n",
            "Epoch: [62][4/10]\tTime 0.151 (0.152)\tData 0.1191 (0.1214)\tLoss 0.6154 (0.6028)\tAccu 0.6680 (0.6855)\t\n",
            "Epoch: [62][5/10]\tTime 0.154 (0.152)\tData 0.1240 (0.1219)\tLoss 0.6063 (0.6035)\tAccu 0.6797 (0.6844)\t\n",
            "Epoch: [62][6/10]\tTime 0.149 (0.152)\tData 0.1198 (0.1216)\tLoss 0.6059 (0.6039)\tAccu 0.6719 (0.6823)\t\n",
            "Epoch: [62][7/10]\tTime 0.154 (0.152)\tData 0.1245 (0.1220)\tLoss 0.6092 (0.6047)\tAccu 0.6445 (0.6769)\t\n",
            "Epoch: [62][8/10]\tTime 0.152 (0.152)\tData 0.1217 (0.1220)\tLoss 0.6355 (0.6085)\tAccu 0.6602 (0.6748)\t\n",
            "Epoch: [62][9/10]\tTime 0.156 (0.152)\tData 0.1261 (0.1224)\tLoss 0.6137 (0.6091)\tAccu 0.6992 (0.6775)\t\n",
            "Epoch: [62][10/10]\tTime 0.119 (0.149)\tData 0.0902 (0.1192)\tLoss 0.6264 (0.6105)\tAccu 0.6276 (0.6736)\t\n",
            "Time 0.042\tAccu 0.6800\tLoss 0.6138\t\n",
            "Epoch: [63][1/10]\tTime 0.151 (0.151)\tData 0.1217 (0.1217)\tLoss 0.6861 (0.6861)\tAccu 0.5664 (0.5664)\t\n",
            "Epoch: [63][2/10]\tTime 0.154 (0.153)\tData 0.1238 (0.1227)\tLoss 0.6106 (0.6484)\tAccu 0.6836 (0.6250)\t\n",
            "Epoch: [63][3/10]\tTime 0.158 (0.155)\tData 0.1280 (0.1245)\tLoss 0.5748 (0.6238)\tAccu 0.7305 (0.6602)\t\n",
            "Epoch: [63][4/10]\tTime 0.153 (0.154)\tData 0.1220 (0.1239)\tLoss 0.6091 (0.6202)\tAccu 0.6680 (0.6621)\t\n",
            "Epoch: [63][5/10]\tTime 0.155 (0.154)\tData 0.1251 (0.1241)\tLoss 0.5907 (0.6143)\tAccu 0.6914 (0.6680)\t\n",
            "Epoch: [63][6/10]\tTime 0.155 (0.154)\tData 0.1252 (0.1243)\tLoss 0.5635 (0.6058)\tAccu 0.7188 (0.6764)\t\n",
            "Epoch: [63][7/10]\tTime 0.159 (0.155)\tData 0.1292 (0.1250)\tLoss 0.6458 (0.6115)\tAccu 0.6289 (0.6696)\t\n",
            "Epoch: [63][8/10]\tTime 0.153 (0.155)\tData 0.1231 (0.1248)\tLoss 0.5612 (0.6052)\tAccu 0.7148 (0.6753)\t\n",
            "Epoch: [63][9/10]\tTime 0.166 (0.156)\tData 0.1357 (0.1260)\tLoss 0.6308 (0.6081)\tAccu 0.6445 (0.6719)\t\n",
            "Epoch: [63][10/10]\tTime 0.127 (0.153)\tData 0.0977 (0.1231)\tLoss 0.5825 (0.6061)\tAccu 0.7041 (0.6744)\t\n",
            "Time 0.044\tAccu 0.6780\tLoss 0.6148\t\n",
            "Epoch: [64][1/10]\tTime 0.153 (0.153)\tData 0.1229 (0.1229)\tLoss 0.5835 (0.5835)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [64][2/10]\tTime 0.163 (0.158)\tData 0.1322 (0.1275)\tLoss 0.5950 (0.5893)\tAccu 0.6602 (0.6797)\t\n",
            "Epoch: [64][3/10]\tTime 0.154 (0.157)\tData 0.1238 (0.1263)\tLoss 0.6149 (0.5978)\tAccu 0.6719 (0.6771)\t\n",
            "Epoch: [64][4/10]\tTime 0.159 (0.157)\tData 0.1291 (0.1270)\tLoss 0.5787 (0.5930)\tAccu 0.7266 (0.6895)\t\n",
            "Epoch: [64][5/10]\tTime 0.155 (0.157)\tData 0.1245 (0.1265)\tLoss 0.6471 (0.6038)\tAccu 0.6211 (0.6758)\t\n",
            "Epoch: [64][6/10]\tTime 0.159 (0.157)\tData 0.1290 (0.1269)\tLoss 0.6113 (0.6051)\tAccu 0.6641 (0.6738)\t\n",
            "Epoch: [64][7/10]\tTime 0.155 (0.157)\tData 0.1243 (0.1265)\tLoss 0.5817 (0.6017)\tAccu 0.7109 (0.6791)\t\n",
            "Epoch: [64][8/10]\tTime 0.160 (0.157)\tData 0.1283 (0.1268)\tLoss 0.6167 (0.6036)\tAccu 0.6562 (0.6763)\t\n",
            "Epoch: [64][9/10]\tTime 0.161 (0.158)\tData 0.1306 (0.1272)\tLoss 0.5927 (0.6024)\tAccu 0.6914 (0.6780)\t\n",
            "Epoch: [64][10/10]\tTime 0.127 (0.155)\tData 0.0986 (0.1243)\tLoss 0.6478 (0.6060)\tAccu 0.6327 (0.6744)\t\n",
            "Time 0.042\tAccu 0.6780\tLoss 0.6139\t\n",
            "Epoch: [65][1/10]\tTime 0.155 (0.155)\tData 0.1253 (0.1253)\tLoss 0.5745 (0.5745)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [65][2/10]\tTime 0.153 (0.154)\tData 0.1225 (0.1239)\tLoss 0.5948 (0.5847)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [65][3/10]\tTime 0.157 (0.155)\tData 0.1268 (0.1249)\tLoss 0.5902 (0.5865)\tAccu 0.6680 (0.6966)\t\n",
            "Epoch: [65][4/10]\tTime 0.156 (0.155)\tData 0.1246 (0.1248)\tLoss 0.6304 (0.5975)\tAccu 0.6523 (0.6855)\t\n",
            "Epoch: [65][5/10]\tTime 0.157 (0.156)\tData 0.1265 (0.1251)\tLoss 0.6133 (0.6007)\tAccu 0.6641 (0.6813)\t\n",
            "Epoch: [65][6/10]\tTime 0.154 (0.155)\tData 0.1231 (0.1248)\tLoss 0.6123 (0.6026)\tAccu 0.6875 (0.6823)\t\n",
            "Epoch: [65][7/10]\tTime 0.156 (0.155)\tData 0.1262 (0.1250)\tLoss 0.5841 (0.6000)\tAccu 0.7109 (0.6864)\t\n",
            "Epoch: [65][8/10]\tTime 0.152 (0.155)\tData 0.1225 (0.1247)\tLoss 0.6181 (0.6022)\tAccu 0.6445 (0.6812)\t\n",
            "Epoch: [65][9/10]\tTime 0.155 (0.155)\tData 0.1244 (0.1247)\tLoss 0.5961 (0.6016)\tAccu 0.6758 (0.6806)\t\n",
            "Epoch: [65][10/10]\tTime 0.123 (0.152)\tData 0.0937 (0.1216)\tLoss 0.6587 (0.6060)\tAccu 0.6173 (0.6756)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6133\t\n",
            "Epoch: [66][1/10]\tTime 0.155 (0.155)\tData 0.1242 (0.1242)\tLoss 0.5706 (0.5706)\tAccu 0.7031 (0.7031)\t\n",
            "Epoch: [66][2/10]\tTime 0.159 (0.157)\tData 0.1288 (0.1265)\tLoss 0.5589 (0.5648)\tAccu 0.7227 (0.7129)\t\n",
            "Epoch: [66][3/10]\tTime 0.158 (0.157)\tData 0.1271 (0.1267)\tLoss 0.6379 (0.5891)\tAccu 0.6445 (0.6901)\t\n",
            "Epoch: [66][4/10]\tTime 0.154 (0.156)\tData 0.1232 (0.1258)\tLoss 0.6158 (0.5958)\tAccu 0.6602 (0.6826)\t\n",
            "Epoch: [66][5/10]\tTime 0.161 (0.157)\tData 0.1300 (0.1267)\tLoss 0.6422 (0.6051)\tAccu 0.6289 (0.6719)\t\n",
            "Epoch: [66][6/10]\tTime 0.151 (0.156)\tData 0.1216 (0.1258)\tLoss 0.5829 (0.6014)\tAccu 0.6914 (0.6751)\t\n",
            "Epoch: [66][7/10]\tTime 0.157 (0.156)\tData 0.1262 (0.1259)\tLoss 0.6335 (0.6060)\tAccu 0.6523 (0.6719)\t\n",
            "Epoch: [66][8/10]\tTime 0.156 (0.156)\tData 0.1257 (0.1258)\tLoss 0.5923 (0.6043)\tAccu 0.6914 (0.6743)\t\n",
            "Epoch: [66][9/10]\tTime 0.155 (0.156)\tData 0.1250 (0.1257)\tLoss 0.5872 (0.6024)\tAccu 0.6953 (0.6766)\t\n",
            "Epoch: [66][10/10]\tTime 0.128 (0.153)\tData 0.0978 (0.1229)\tLoss 0.6168 (0.6035)\tAccu 0.6480 (0.6744)\t\n",
            "Time 0.044\tAccu 0.6800\tLoss 0.6144\t\n",
            "Epoch: [67][1/10]\tTime 0.160 (0.160)\tData 0.1299 (0.1299)\tLoss 0.6285 (0.6285)\tAccu 0.6211 (0.6211)\t\n",
            "Epoch: [67][2/10]\tTime 0.156 (0.158)\tData 0.1259 (0.1279)\tLoss 0.5707 (0.5996)\tAccu 0.7148 (0.6680)\t\n",
            "Epoch: [67][3/10]\tTime 0.155 (0.157)\tData 0.1244 (0.1267)\tLoss 0.6136 (0.6042)\tAccu 0.6602 (0.6654)\t\n",
            "Epoch: [67][4/10]\tTime 0.154 (0.156)\tData 0.1243 (0.1261)\tLoss 0.6125 (0.6063)\tAccu 0.6562 (0.6631)\t\n",
            "Epoch: [67][5/10]\tTime 0.158 (0.157)\tData 0.1265 (0.1262)\tLoss 0.6186 (0.6088)\tAccu 0.6523 (0.6609)\t\n",
            "Epoch: [67][6/10]\tTime 0.160 (0.157)\tData 0.1300 (0.1268)\tLoss 0.5831 (0.6045)\tAccu 0.6797 (0.6641)\t\n",
            "Epoch: [67][7/10]\tTime 0.161 (0.158)\tData 0.1304 (0.1274)\tLoss 0.5534 (0.5972)\tAccu 0.7305 (0.6735)\t\n",
            "Epoch: [67][8/10]\tTime 0.156 (0.158)\tData 0.1258 (0.1272)\tLoss 0.6582 (0.6048)\tAccu 0.6523 (0.6709)\t\n",
            "Epoch: [67][9/10]\tTime 0.154 (0.157)\tData 0.1238 (0.1268)\tLoss 0.6161 (0.6061)\tAccu 0.6719 (0.6710)\t\n",
            "Epoch: [67][10/10]\tTime 0.125 (0.154)\tData 0.0961 (0.1237)\tLoss 0.5870 (0.6046)\tAccu 0.6888 (0.6724)\t\n",
            "Time 0.044\tAccu 0.6760\tLoss 0.6160\t\n",
            "Epoch: [68][1/10]\tTime 0.156 (0.156)\tData 0.1259 (0.1259)\tLoss 0.5996 (0.5996)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [68][2/10]\tTime 0.158 (0.157)\tData 0.1261 (0.1260)\tLoss 0.6181 (0.6089)\tAccu 0.6328 (0.6543)\t\n",
            "Epoch: [68][3/10]\tTime 0.154 (0.156)\tData 0.1240 (0.1253)\tLoss 0.6071 (0.6083)\tAccu 0.6758 (0.6615)\t\n",
            "Epoch: [68][4/10]\tTime 0.160 (0.157)\tData 0.1290 (0.1262)\tLoss 0.5817 (0.6016)\tAccu 0.7109 (0.6738)\t\n",
            "Epoch: [68][5/10]\tTime 0.157 (0.157)\tData 0.1261 (0.1262)\tLoss 0.6105 (0.6034)\tAccu 0.6719 (0.6734)\t\n",
            "Epoch: [68][6/10]\tTime 0.155 (0.157)\tData 0.1246 (0.1259)\tLoss 0.5896 (0.6011)\tAccu 0.6836 (0.6751)\t\n",
            "Epoch: [68][7/10]\tTime 0.158 (0.157)\tData 0.1270 (0.1261)\tLoss 0.5722 (0.5970)\tAccu 0.6914 (0.6775)\t\n",
            "Epoch: [68][8/10]\tTime 0.155 (0.157)\tData 0.1250 (0.1260)\tLoss 0.6439 (0.6028)\tAccu 0.6445 (0.6733)\t\n",
            "Epoch: [68][9/10]\tTime 0.172 (0.158)\tData 0.1417 (0.1277)\tLoss 0.5953 (0.6020)\tAccu 0.7109 (0.6775)\t\n",
            "Epoch: [68][10/10]\tTime 0.123 (0.155)\tData 0.0937 (0.1243)\tLoss 0.6206 (0.6035)\tAccu 0.6633 (0.6764)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6182\t\n",
            "Epoch: [69][1/10]\tTime 0.153 (0.153)\tData 0.1225 (0.1225)\tLoss 0.6129 (0.6129)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [69][2/10]\tTime 0.158 (0.155)\tData 0.1284 (0.1254)\tLoss 0.5921 (0.6025)\tAccu 0.6836 (0.6797)\t\n",
            "Epoch: [69][3/10]\tTime 0.155 (0.155)\tData 0.1243 (0.1251)\tLoss 0.5919 (0.5990)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [69][4/10]\tTime 0.162 (0.157)\tData 0.1298 (0.1263)\tLoss 0.6003 (0.5993)\tAccu 0.6680 (0.6768)\t\n",
            "Epoch: [69][5/10]\tTime 0.162 (0.158)\tData 0.1312 (0.1272)\tLoss 0.5977 (0.5990)\tAccu 0.6602 (0.6734)\t\n",
            "Epoch: [69][6/10]\tTime 0.156 (0.158)\tData 0.1258 (0.1270)\tLoss 0.5997 (0.5991)\tAccu 0.6445 (0.6686)\t\n",
            "Epoch: [69][7/10]\tTime 0.155 (0.157)\tData 0.1254 (0.1268)\tLoss 0.6348 (0.6042)\tAccu 0.6562 (0.6669)\t\n",
            "Epoch: [69][8/10]\tTime 0.156 (0.157)\tData 0.1258 (0.1267)\tLoss 0.6256 (0.6069)\tAccu 0.6719 (0.6675)\t\n",
            "Epoch: [69][9/10]\tTime 0.153 (0.157)\tData 0.1221 (0.1262)\tLoss 0.5810 (0.6040)\tAccu 0.7461 (0.6762)\t\n",
            "Epoch: [69][10/10]\tTime 0.127 (0.154)\tData 0.0980 (0.1233)\tLoss 0.5981 (0.6035)\tAccu 0.6786 (0.6764)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6284\t\n",
            "Epoch: [70][1/10]\tTime 0.155 (0.155)\tData 0.1243 (0.1243)\tLoss 0.6531 (0.6531)\tAccu 0.6367 (0.6367)\t\n",
            "Epoch: [70][2/10]\tTime 0.151 (0.153)\tData 0.1212 (0.1228)\tLoss 0.5748 (0.6139)\tAccu 0.6953 (0.6660)\t\n",
            "Epoch: [70][3/10]\tTime 0.155 (0.154)\tData 0.1250 (0.1235)\tLoss 0.5654 (0.5978)\tAccu 0.7305 (0.6875)\t\n",
            "Epoch: [70][4/10]\tTime 0.154 (0.154)\tData 0.1241 (0.1236)\tLoss 0.6224 (0.6039)\tAccu 0.6562 (0.6797)\t\n",
            "Epoch: [70][5/10]\tTime 0.154 (0.154)\tData 0.1243 (0.1238)\tLoss 0.6299 (0.6091)\tAccu 0.6406 (0.6719)\t\n",
            "Epoch: [70][6/10]\tTime 0.155 (0.154)\tData 0.1247 (0.1239)\tLoss 0.6011 (0.6078)\tAccu 0.6953 (0.6758)\t\n",
            "Epoch: [70][7/10]\tTime 0.163 (0.155)\tData 0.1324 (0.1251)\tLoss 0.5601 (0.6010)\tAccu 0.7695 (0.6892)\t\n",
            "Epoch: [70][8/10]\tTime 0.157 (0.155)\tData 0.1258 (0.1252)\tLoss 0.6241 (0.6039)\tAccu 0.6523 (0.6846)\t\n",
            "Epoch: [70][9/10]\tTime 0.160 (0.156)\tData 0.1296 (0.1257)\tLoss 0.6150 (0.6051)\tAccu 0.6719 (0.6832)\t\n",
            "Epoch: [70][10/10]\tTime 0.126 (0.153)\tData 0.0970 (0.1228)\tLoss 0.6774 (0.6108)\tAccu 0.6224 (0.6784)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6143\t\n",
            "Epoch: [71][1/10]\tTime 0.153 (0.153)\tData 0.1235 (0.1235)\tLoss 0.5943 (0.5943)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [71][2/10]\tTime 0.160 (0.157)\tData 0.1293 (0.1264)\tLoss 0.6062 (0.6002)\tAccu 0.7109 (0.6934)\t\n",
            "Epoch: [71][3/10]\tTime 0.165 (0.160)\tData 0.1338 (0.1289)\tLoss 0.5850 (0.5952)\tAccu 0.7109 (0.6992)\t\n",
            "Epoch: [71][4/10]\tTime 0.158 (0.159)\tData 0.1273 (0.1285)\tLoss 0.5939 (0.5948)\tAccu 0.6914 (0.6973)\t\n",
            "Epoch: [71][5/10]\tTime 0.153 (0.158)\tData 0.1228 (0.1273)\tLoss 0.6111 (0.5981)\tAccu 0.6680 (0.6914)\t\n",
            "Epoch: [71][6/10]\tTime 0.157 (0.158)\tData 0.1274 (0.1273)\tLoss 0.6148 (0.6009)\tAccu 0.6602 (0.6862)\t\n",
            "Epoch: [71][7/10]\tTime 0.152 (0.157)\tData 0.1216 (0.1265)\tLoss 0.6723 (0.6111)\tAccu 0.6367 (0.6791)\t\n",
            "Epoch: [71][8/10]\tTime 0.160 (0.157)\tData 0.1298 (0.1269)\tLoss 0.5869 (0.6081)\tAccu 0.6875 (0.6802)\t\n",
            "Epoch: [71][9/10]\tTime 0.163 (0.158)\tData 0.1325 (0.1276)\tLoss 0.6403 (0.6116)\tAccu 0.6367 (0.6753)\t\n",
            "Epoch: [71][10/10]\tTime 0.128 (0.155)\tData 0.0972 (0.1245)\tLoss 0.6375 (0.6137)\tAccu 0.6684 (0.6748)\t\n",
            "Time 0.042\tAccu 0.6780\tLoss 0.6281\t\n",
            "Epoch: [72][1/10]\tTime 0.151 (0.151)\tData 0.1214 (0.1214)\tLoss 0.5949 (0.5949)\tAccu 0.7266 (0.7266)\t\n",
            "Epoch: [72][2/10]\tTime 0.149 (0.150)\tData 0.1174 (0.1194)\tLoss 0.5849 (0.5899)\tAccu 0.7070 (0.7168)\t\n",
            "Epoch: [72][3/10]\tTime 0.158 (0.153)\tData 0.1278 (0.1222)\tLoss 0.5751 (0.5850)\tAccu 0.7031 (0.7122)\t\n",
            "Epoch: [72][4/10]\tTime 0.159 (0.154)\tData 0.1261 (0.1232)\tLoss 0.7197 (0.6186)\tAccu 0.6133 (0.6875)\t\n",
            "Epoch: [72][5/10]\tTime 0.160 (0.155)\tData 0.1300 (0.1245)\tLoss 0.6759 (0.6301)\tAccu 0.6172 (0.6734)\t\n",
            "Epoch: [72][6/10]\tTime 0.153 (0.155)\tData 0.1221 (0.1241)\tLoss 0.5977 (0.6247)\tAccu 0.6836 (0.6751)\t\n",
            "Epoch: [72][7/10]\tTime 0.160 (0.156)\tData 0.1290 (0.1248)\tLoss 0.6207 (0.6241)\tAccu 0.6562 (0.6724)\t\n",
            "Epoch: [72][8/10]\tTime 0.155 (0.156)\tData 0.1252 (0.1249)\tLoss 0.6375 (0.6258)\tAccu 0.6797 (0.6733)\t\n",
            "Epoch: [72][9/10]\tTime 0.164 (0.156)\tData 0.1329 (0.1258)\tLoss 0.6315 (0.6264)\tAccu 0.6758 (0.6736)\t\n",
            "Epoch: [72][10/10]\tTime 0.124 (0.153)\tData 0.0951 (0.1227)\tLoss 0.6116 (0.6253)\tAccu 0.6786 (0.6740)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6123\t\n",
            "Epoch: [73][1/10]\tTime 0.157 (0.157)\tData 0.1272 (0.1272)\tLoss 0.6204 (0.6204)\tAccu 0.6406 (0.6406)\t\n",
            "Epoch: [73][2/10]\tTime 0.157 (0.157)\tData 0.1258 (0.1265)\tLoss 0.6574 (0.6389)\tAccu 0.6367 (0.6387)\t\n",
            "Epoch: [73][3/10]\tTime 0.159 (0.158)\tData 0.1288 (0.1273)\tLoss 0.6057 (0.6278)\tAccu 0.6758 (0.6510)\t\n",
            "Epoch: [73][4/10]\tTime 0.157 (0.158)\tData 0.1270 (0.1272)\tLoss 0.5688 (0.6131)\tAccu 0.7227 (0.6689)\t\n",
            "Epoch: [73][5/10]\tTime 0.167 (0.160)\tData 0.1339 (0.1285)\tLoss 0.5579 (0.6020)\tAccu 0.7344 (0.6820)\t\n",
            "Epoch: [73][6/10]\tTime 0.157 (0.159)\tData 0.1264 (0.1282)\tLoss 0.6422 (0.6087)\tAccu 0.6289 (0.6732)\t\n",
            "Epoch: [73][7/10]\tTime 0.158 (0.159)\tData 0.1273 (0.1281)\tLoss 0.5980 (0.6072)\tAccu 0.6836 (0.6747)\t\n",
            "Epoch: [73][8/10]\tTime 0.157 (0.159)\tData 0.1261 (0.1278)\tLoss 0.5973 (0.6060)\tAccu 0.6719 (0.6743)\t\n",
            "Epoch: [73][9/10]\tTime 0.159 (0.159)\tData 0.1286 (0.1279)\tLoss 0.5959 (0.6048)\tAccu 0.7070 (0.6780)\t\n",
            "Epoch: [73][10/10]\tTime 0.129 (0.156)\tData 0.1001 (0.1251)\tLoss 0.6351 (0.6072)\tAccu 0.6378 (0.6748)\t\n",
            "Time 0.046\tAccu 0.6800\tLoss 0.6156\t\n",
            "Epoch: [74][1/10]\tTime 0.160 (0.160)\tData 0.1288 (0.1288)\tLoss 0.6406 (0.6406)\tAccu 0.6328 (0.6328)\t\n",
            "Epoch: [74][2/10]\tTime 0.168 (0.164)\tData 0.1371 (0.1329)\tLoss 0.5960 (0.6183)\tAccu 0.6875 (0.6602)\t\n",
            "Epoch: [74][3/10]\tTime 0.155 (0.161)\tData 0.1244 (0.1301)\tLoss 0.5916 (0.6094)\tAccu 0.6797 (0.6667)\t\n",
            "Epoch: [74][4/10]\tTime 0.158 (0.160)\tData 0.1279 (0.1295)\tLoss 0.6210 (0.6123)\tAccu 0.6641 (0.6660)\t\n",
            "Epoch: [74][5/10]\tTime 0.159 (0.160)\tData 0.1279 (0.1292)\tLoss 0.6407 (0.6180)\tAccu 0.6328 (0.6594)\t\n",
            "Epoch: [74][6/10]\tTime 0.159 (0.160)\tData 0.1282 (0.1290)\tLoss 0.5683 (0.6097)\tAccu 0.6992 (0.6660)\t\n",
            "Epoch: [74][7/10]\tTime 0.162 (0.160)\tData 0.1296 (0.1291)\tLoss 0.5920 (0.6072)\tAccu 0.6914 (0.6696)\t\n",
            "Epoch: [74][8/10]\tTime 0.157 (0.160)\tData 0.1267 (0.1288)\tLoss 0.6420 (0.6115)\tAccu 0.6289 (0.6646)\t\n",
            "Epoch: [74][9/10]\tTime 0.157 (0.159)\tData 0.1270 (0.1286)\tLoss 0.5662 (0.6065)\tAccu 0.7305 (0.6719)\t\n",
            "Epoch: [74][10/10]\tTime 0.126 (0.156)\tData 0.0972 (0.1255)\tLoss 0.5900 (0.6052)\tAccu 0.7041 (0.6744)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6138\t\n",
            "Epoch: [75][1/10]\tTime 0.152 (0.152)\tData 0.1201 (0.1201)\tLoss 0.5900 (0.5900)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [75][2/10]\tTime 0.159 (0.155)\tData 0.1283 (0.1242)\tLoss 0.6174 (0.6037)\tAccu 0.6641 (0.6699)\t\n",
            "Epoch: [75][3/10]\tTime 0.163 (0.158)\tData 0.1301 (0.1262)\tLoss 0.6174 (0.6083)\tAccu 0.6562 (0.6654)\t\n",
            "Epoch: [75][4/10]\tTime 0.153 (0.157)\tData 0.1228 (0.1253)\tLoss 0.5862 (0.6027)\tAccu 0.6914 (0.6719)\t\n",
            "Epoch: [75][5/10]\tTime 0.155 (0.156)\tData 0.1256 (0.1254)\tLoss 0.5888 (0.6000)\tAccu 0.6992 (0.6773)\t\n",
            "Epoch: [75][6/10]\tTime 0.156 (0.156)\tData 0.1255 (0.1254)\tLoss 0.6019 (0.6003)\tAccu 0.6875 (0.6790)\t\n",
            "Epoch: [75][7/10]\tTime 0.153 (0.156)\tData 0.1224 (0.1250)\tLoss 0.6054 (0.6010)\tAccu 0.6641 (0.6769)\t\n",
            "Epoch: [75][8/10]\tTime 0.157 (0.156)\tData 0.1273 (0.1253)\tLoss 0.6068 (0.6017)\tAccu 0.6680 (0.6758)\t\n",
            "Epoch: [75][9/10]\tTime 0.158 (0.156)\tData 0.1265 (0.1254)\tLoss 0.6429 (0.6063)\tAccu 0.6484 (0.6727)\t\n",
            "Epoch: [75][10/10]\tTime 0.125 (0.153)\tData 0.0959 (0.1224)\tLoss 0.5767 (0.6040)\tAccu 0.6939 (0.6744)\t\n",
            "Time 0.043\tAccu 0.6760\tLoss 0.6132\t\n",
            "Epoch: [76][1/10]\tTime 0.156 (0.156)\tData 0.1256 (0.1256)\tLoss 0.6217 (0.6217)\tAccu 0.6484 (0.6484)\t\n",
            "Epoch: [76][2/10]\tTime 0.160 (0.158)\tData 0.1286 (0.1271)\tLoss 0.6205 (0.6211)\tAccu 0.6445 (0.6465)\t\n",
            "Epoch: [76][3/10]\tTime 0.155 (0.157)\tData 0.1246 (0.1263)\tLoss 0.5850 (0.6091)\tAccu 0.6836 (0.6589)\t\n",
            "Epoch: [76][4/10]\tTime 0.159 (0.157)\tData 0.1275 (0.1266)\tLoss 0.5917 (0.6047)\tAccu 0.6797 (0.6641)\t\n",
            "Epoch: [76][5/10]\tTime 0.168 (0.159)\tData 0.1373 (0.1287)\tLoss 0.6090 (0.6056)\tAccu 0.6836 (0.6680)\t\n",
            "Epoch: [76][6/10]\tTime 0.154 (0.159)\tData 0.1241 (0.1280)\tLoss 0.5878 (0.6026)\tAccu 0.6875 (0.6712)\t\n",
            "Epoch: [76][7/10]\tTime 0.154 (0.158)\tData 0.1234 (0.1273)\tLoss 0.5663 (0.5974)\tAccu 0.7109 (0.6769)\t\n",
            "Epoch: [76][8/10]\tTime 0.154 (0.157)\tData 0.1240 (0.1269)\tLoss 0.5877 (0.5962)\tAccu 0.7031 (0.6802)\t\n",
            "Epoch: [76][9/10]\tTime 0.153 (0.157)\tData 0.1221 (0.1264)\tLoss 0.6085 (0.5976)\tAccu 0.6875 (0.6810)\t\n",
            "Epoch: [76][10/10]\tTime 0.127 (0.154)\tData 0.0979 (0.1235)\tLoss 0.6559 (0.6021)\tAccu 0.5969 (0.6744)\t\n",
            "Time 0.044\tAccu 0.6740\tLoss 0.6195\t\n",
            "Epoch: [77][1/10]\tTime 0.162 (0.162)\tData 0.1322 (0.1322)\tLoss 0.6060 (0.6060)\tAccu 0.6875 (0.6875)\t\n",
            "Epoch: [77][2/10]\tTime 0.158 (0.160)\tData 0.1278 (0.1300)\tLoss 0.6203 (0.6131)\tAccu 0.6562 (0.6719)\t\n",
            "Epoch: [77][3/10]\tTime 0.160 (0.160)\tData 0.1298 (0.1299)\tLoss 0.6161 (0.6141)\tAccu 0.6836 (0.6758)\t\n",
            "Epoch: [77][4/10]\tTime 0.158 (0.159)\tData 0.1265 (0.1291)\tLoss 0.6200 (0.6156)\tAccu 0.6680 (0.6738)\t\n",
            "Epoch: [77][5/10]\tTime 0.157 (0.159)\tData 0.1271 (0.1287)\tLoss 0.6235 (0.6172)\tAccu 0.6328 (0.6656)\t\n",
            "Epoch: [77][6/10]\tTime 0.150 (0.158)\tData 0.1194 (0.1271)\tLoss 0.5795 (0.6109)\tAccu 0.7109 (0.6732)\t\n",
            "Epoch: [77][7/10]\tTime 0.167 (0.159)\tData 0.1367 (0.1285)\tLoss 0.6127 (0.6111)\tAccu 0.6641 (0.6719)\t\n",
            "Epoch: [77][8/10]\tTime 0.165 (0.160)\tData 0.1349 (0.1293)\tLoss 0.6188 (0.6121)\tAccu 0.6797 (0.6729)\t\n",
            "Epoch: [77][9/10]\tTime 0.157 (0.159)\tData 0.1271 (0.1291)\tLoss 0.5563 (0.6059)\tAccu 0.7383 (0.6801)\t\n",
            "Epoch: [77][10/10]\tTime 0.125 (0.156)\tData 0.0944 (0.1256)\tLoss 0.6091 (0.6061)\tAccu 0.6684 (0.6792)\t\n",
            "Time 0.044\tAccu 0.6820\tLoss 0.6134\t\n",
            "Epoch: [78][1/10]\tTime 0.151 (0.151)\tData 0.1210 (0.1210)\tLoss 0.5793 (0.5793)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [78][2/10]\tTime 0.159 (0.155)\tData 0.1284 (0.1247)\tLoss 0.6420 (0.6106)\tAccu 0.5938 (0.6543)\t\n",
            "Epoch: [78][3/10]\tTime 0.157 (0.156)\tData 0.1268 (0.1254)\tLoss 0.5899 (0.6037)\tAccu 0.6953 (0.6680)\t\n",
            "Epoch: [78][4/10]\tTime 0.154 (0.155)\tData 0.1240 (0.1251)\tLoss 0.6048 (0.6040)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [78][5/10]\tTime 0.155 (0.155)\tData 0.1245 (0.1250)\tLoss 0.6107 (0.6053)\tAccu 0.6953 (0.6734)\t\n",
            "Epoch: [78][6/10]\tTime 0.155 (0.155)\tData 0.1256 (0.1251)\tLoss 0.6216 (0.6080)\tAccu 0.6562 (0.6706)\t\n",
            "Epoch: [78][7/10]\tTime 0.155 (0.155)\tData 0.1246 (0.1250)\tLoss 0.5738 (0.6032)\tAccu 0.6914 (0.6735)\t\n",
            "Epoch: [78][8/10]\tTime 0.153 (0.155)\tData 0.1214 (0.1246)\tLoss 0.5899 (0.6015)\tAccu 0.6719 (0.6733)\t\n",
            "Epoch: [78][9/10]\tTime 0.159 (0.155)\tData 0.1281 (0.1250)\tLoss 0.5913 (0.6004)\tAccu 0.7109 (0.6775)\t\n",
            "Epoch: [78][10/10]\tTime 0.126 (0.153)\tData 0.0978 (0.1222)\tLoss 0.5986 (0.6002)\tAccu 0.6633 (0.6764)\t\n",
            "Time 0.042\tAccu 0.6760\tLoss 0.6161\t\n",
            "Epoch: [79][1/10]\tTime 0.151 (0.151)\tData 0.1211 (0.1211)\tLoss 0.6672 (0.6672)\tAccu 0.6133 (0.6133)\t\n",
            "Epoch: [79][2/10]\tTime 0.157 (0.154)\tData 0.1273 (0.1242)\tLoss 0.6111 (0.6392)\tAccu 0.6836 (0.6484)\t\n",
            "Epoch: [79][3/10]\tTime 0.153 (0.154)\tData 0.1224 (0.1236)\tLoss 0.6257 (0.6347)\tAccu 0.6641 (0.6536)\t\n",
            "Epoch: [79][4/10]\tTime 0.157 (0.155)\tData 0.1261 (0.1242)\tLoss 0.5942 (0.6246)\tAccu 0.6992 (0.6650)\t\n",
            "Epoch: [79][5/10]\tTime 0.160 (0.156)\tData 0.1297 (0.1253)\tLoss 0.5859 (0.6168)\tAccu 0.6914 (0.6703)\t\n",
            "Epoch: [79][6/10]\tTime 0.153 (0.155)\tData 0.1217 (0.1247)\tLoss 0.6859 (0.6283)\tAccu 0.6211 (0.6621)\t\n",
            "Epoch: [79][7/10]\tTime 0.154 (0.155)\tData 0.1240 (0.1246)\tLoss 0.5659 (0.6194)\tAccu 0.7227 (0.6708)\t\n",
            "Epoch: [79][8/10]\tTime 0.151 (0.155)\tData 0.1211 (0.1242)\tLoss 0.5972 (0.6167)\tAccu 0.6875 (0.6729)\t\n",
            "Epoch: [79][9/10]\tTime 0.161 (0.155)\tData 0.1298 (0.1248)\tLoss 0.6324 (0.6184)\tAccu 0.6367 (0.6688)\t\n",
            "Epoch: [79][10/10]\tTime 0.128 (0.153)\tData 0.0986 (0.1222)\tLoss 0.5594 (0.6138)\tAccu 0.7704 (0.6768)\t\n",
            "Time 0.045\tAccu 0.6800\tLoss 0.6208\t\n",
            "Epoch: [80][1/10]\tTime 0.157 (0.157)\tData 0.1272 (0.1272)\tLoss 0.5958 (0.5958)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [80][2/10]\tTime 0.158 (0.158)\tData 0.1277 (0.1274)\tLoss 0.5934 (0.5946)\tAccu 0.6719 (0.6934)\t\n",
            "Epoch: [80][3/10]\tTime 0.157 (0.158)\tData 0.1261 (0.1270)\tLoss 0.5612 (0.5834)\tAccu 0.7109 (0.6992)\t\n",
            "Epoch: [80][4/10]\tTime 0.155 (0.157)\tData 0.1245 (0.1263)\tLoss 0.6384 (0.5972)\tAccu 0.6797 (0.6943)\t\n",
            "Epoch: [80][5/10]\tTime 0.155 (0.157)\tData 0.1249 (0.1261)\tLoss 0.6493 (0.6076)\tAccu 0.6523 (0.6859)\t\n",
            "Epoch: [80][6/10]\tTime 0.156 (0.157)\tData 0.1257 (0.1260)\tLoss 0.5681 (0.6010)\tAccu 0.6953 (0.6875)\t\n",
            "Epoch: [80][7/10]\tTime 0.159 (0.157)\tData 0.1275 (0.1262)\tLoss 0.5962 (0.6003)\tAccu 0.6914 (0.6881)\t\n",
            "Epoch: [80][8/10]\tTime 0.156 (0.157)\tData 0.1252 (0.1261)\tLoss 0.6106 (0.6016)\tAccu 0.6562 (0.6841)\t\n",
            "Epoch: [80][9/10]\tTime 0.157 (0.157)\tData 0.1265 (0.1261)\tLoss 0.6190 (0.6035)\tAccu 0.6562 (0.6810)\t\n",
            "Epoch: [80][10/10]\tTime 0.129 (0.154)\tData 0.0984 (0.1234)\tLoss 0.5987 (0.6032)\tAccu 0.6735 (0.6804)\t\n",
            "Time 0.043\tAccu 0.6820\tLoss 0.6119\t\n",
            "Epoch: [81][1/10]\tTime 0.153 (0.153)\tData 0.1226 (0.1226)\tLoss 0.6052 (0.6052)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [81][2/10]\tTime 0.157 (0.155)\tData 0.1266 (0.1246)\tLoss 0.6076 (0.6064)\tAccu 0.6641 (0.6621)\t\n",
            "Epoch: [81][3/10]\tTime 0.153 (0.154)\tData 0.1230 (0.1241)\tLoss 0.6032 (0.6053)\tAccu 0.6719 (0.6654)\t\n",
            "Epoch: [81][4/10]\tTime 0.155 (0.154)\tData 0.1253 (0.1244)\tLoss 0.6112 (0.6068)\tAccu 0.6289 (0.6562)\t\n",
            "Epoch: [81][5/10]\tTime 0.157 (0.155)\tData 0.1266 (0.1248)\tLoss 0.5651 (0.5985)\tAccu 0.7500 (0.6750)\t\n",
            "Epoch: [81][6/10]\tTime 0.154 (0.155)\tData 0.1242 (0.1247)\tLoss 0.5958 (0.5980)\tAccu 0.6875 (0.6771)\t\n",
            "Epoch: [81][7/10]\tTime 0.157 (0.155)\tData 0.1275 (0.1251)\tLoss 0.5718 (0.5943)\tAccu 0.7266 (0.6842)\t\n",
            "Epoch: [81][8/10]\tTime 0.155 (0.155)\tData 0.1246 (0.1251)\tLoss 0.6147 (0.5968)\tAccu 0.6758 (0.6831)\t\n",
            "Epoch: [81][9/10]\tTime 0.152 (0.155)\tData 0.1228 (0.1248)\tLoss 0.6158 (0.5989)\tAccu 0.6406 (0.6784)\t\n",
            "Epoch: [81][10/10]\tTime 0.125 (0.152)\tData 0.0959 (0.1219)\tLoss 0.6114 (0.5999)\tAccu 0.6429 (0.6756)\t\n",
            "Time 0.042\tAccu 0.6760\tLoss 0.6241\t\n",
            "Epoch: [82][1/10]\tTime 0.149 (0.149)\tData 0.1189 (0.1189)\tLoss 0.6233 (0.6233)\tAccu 0.6562 (0.6562)\t\n",
            "Epoch: [82][2/10]\tTime 0.162 (0.156)\tData 0.1287 (0.1238)\tLoss 0.6080 (0.6157)\tAccu 0.7031 (0.6797)\t\n",
            "Epoch: [82][3/10]\tTime 0.150 (0.154)\tData 0.1201 (0.1226)\tLoss 0.6281 (0.6198)\tAccu 0.6836 (0.6810)\t\n",
            "Epoch: [82][4/10]\tTime 0.162 (0.156)\tData 0.1316 (0.1248)\tLoss 0.6063 (0.6164)\tAccu 0.6602 (0.6758)\t\n",
            "Epoch: [82][5/10]\tTime 0.152 (0.155)\tData 0.1220 (0.1243)\tLoss 0.6031 (0.6138)\tAccu 0.6562 (0.6719)\t\n",
            "Epoch: [82][6/10]\tTime 0.150 (0.154)\tData 0.1204 (0.1236)\tLoss 0.5418 (0.6018)\tAccu 0.7305 (0.6816)\t\n",
            "Epoch: [82][7/10]\tTime 0.156 (0.155)\tData 0.1262 (0.1240)\tLoss 0.6349 (0.6065)\tAccu 0.6250 (0.6735)\t\n",
            "Epoch: [82][8/10]\tTime 0.150 (0.154)\tData 0.1199 (0.1235)\tLoss 0.6146 (0.6075)\tAccu 0.6367 (0.6689)\t\n",
            "Epoch: [82][9/10]\tTime 0.158 (0.154)\tData 0.1277 (0.1240)\tLoss 0.5967 (0.6063)\tAccu 0.7109 (0.6736)\t\n",
            "Epoch: [82][10/10]\tTime 0.124 (0.151)\tData 0.0955 (0.1211)\tLoss 0.6137 (0.6069)\tAccu 0.6480 (0.6716)\t\n",
            "Time 0.043\tAccu 0.6820\tLoss 0.6141\t\n",
            "Epoch: [83][1/10]\tTime 0.155 (0.155)\tData 0.1251 (0.1251)\tLoss 0.6027 (0.6027)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [83][2/10]\tTime 0.151 (0.153)\tData 0.1209 (0.1230)\tLoss 0.6312 (0.6170)\tAccu 0.6523 (0.6680)\t\n",
            "Epoch: [83][3/10]\tTime 0.159 (0.155)\tData 0.1284 (0.1248)\tLoss 0.6136 (0.6159)\tAccu 0.6602 (0.6654)\t\n",
            "Epoch: [83][4/10]\tTime 0.156 (0.155)\tData 0.1252 (0.1249)\tLoss 0.5407 (0.5971)\tAccu 0.7383 (0.6836)\t\n",
            "Epoch: [83][5/10]\tTime 0.166 (0.157)\tData 0.1351 (0.1269)\tLoss 0.5674 (0.5911)\tAccu 0.7031 (0.6875)\t\n",
            "Epoch: [83][6/10]\tTime 0.159 (0.158)\tData 0.1284 (0.1272)\tLoss 0.5775 (0.5889)\tAccu 0.6797 (0.6862)\t\n",
            "Epoch: [83][7/10]\tTime 0.158 (0.158)\tData 0.1255 (0.1269)\tLoss 0.6053 (0.5912)\tAccu 0.6641 (0.6830)\t\n",
            "Epoch: [83][8/10]\tTime 0.154 (0.157)\tData 0.1239 (0.1266)\tLoss 0.6179 (0.5945)\tAccu 0.6523 (0.6792)\t\n",
            "Epoch: [83][9/10]\tTime 0.156 (0.157)\tData 0.1260 (0.1265)\tLoss 0.6070 (0.5959)\tAccu 0.7031 (0.6819)\t\n",
            "Epoch: [83][10/10]\tTime 0.126 (0.154)\tData 0.0974 (0.1236)\tLoss 0.6206 (0.5979)\tAccu 0.6531 (0.6796)\t\n",
            "Time 0.044\tAccu 0.6760\tLoss 0.6162\t\n",
            "Epoch: [84][1/10]\tTime 0.153 (0.153)\tData 0.1229 (0.1229)\tLoss 0.6026 (0.6026)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [84][2/10]\tTime 0.158 (0.155)\tData 0.1277 (0.1253)\tLoss 0.5890 (0.5958)\tAccu 0.6758 (0.6797)\t\n",
            "Epoch: [84][3/10]\tTime 0.156 (0.156)\tData 0.1257 (0.1255)\tLoss 0.5946 (0.5954)\tAccu 0.6641 (0.6745)\t\n",
            "Epoch: [84][4/10]\tTime 0.156 (0.156)\tData 0.1256 (0.1255)\tLoss 0.6141 (0.6001)\tAccu 0.6641 (0.6719)\t\n",
            "Epoch: [84][5/10]\tTime 0.155 (0.156)\tData 0.1234 (0.1251)\tLoss 0.6252 (0.6051)\tAccu 0.6367 (0.6648)\t\n",
            "Epoch: [84][6/10]\tTime 0.158 (0.156)\tData 0.1278 (0.1255)\tLoss 0.6050 (0.6051)\tAccu 0.6680 (0.6654)\t\n",
            "Epoch: [84][7/10]\tTime 0.156 (0.156)\tData 0.1255 (0.1255)\tLoss 0.6091 (0.6056)\tAccu 0.7070 (0.6713)\t\n",
            "Epoch: [84][8/10]\tTime 0.158 (0.156)\tData 0.1262 (0.1256)\tLoss 0.5803 (0.6025)\tAccu 0.7266 (0.6782)\t\n",
            "Epoch: [84][9/10]\tTime 0.153 (0.156)\tData 0.1222 (0.1252)\tLoss 0.5370 (0.5952)\tAccu 0.7227 (0.6832)\t\n",
            "Epoch: [84][10/10]\tTime 0.128 (0.153)\tData 0.0975 (0.1225)\tLoss 0.6861 (0.6023)\tAccu 0.6378 (0.6796)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6186\t\n",
            "Epoch: [85][1/10]\tTime 0.157 (0.157)\tData 0.1264 (0.1264)\tLoss 0.6051 (0.6051)\tAccu 0.6484 (0.6484)\t\n",
            "Epoch: [85][2/10]\tTime 0.154 (0.155)\tData 0.1229 (0.1246)\tLoss 0.5703 (0.5877)\tAccu 0.7031 (0.6758)\t\n",
            "Epoch: [85][3/10]\tTime 0.155 (0.155)\tData 0.1250 (0.1248)\tLoss 0.5975 (0.5910)\tAccu 0.7109 (0.6875)\t\n",
            "Epoch: [85][4/10]\tTime 0.158 (0.156)\tData 0.1281 (0.1256)\tLoss 0.5822 (0.5888)\tAccu 0.7109 (0.6934)\t\n",
            "Epoch: [85][5/10]\tTime 0.156 (0.156)\tData 0.1256 (0.1256)\tLoss 0.6009 (0.5912)\tAccu 0.6875 (0.6922)\t\n",
            "Epoch: [85][6/10]\tTime 0.154 (0.156)\tData 0.1238 (0.1253)\tLoss 0.5912 (0.5912)\tAccu 0.7109 (0.6953)\t\n",
            "Epoch: [85][7/10]\tTime 0.155 (0.155)\tData 0.1243 (0.1252)\tLoss 0.6704 (0.6025)\tAccu 0.6484 (0.6886)\t\n",
            "Epoch: [85][8/10]\tTime 0.158 (0.156)\tData 0.1272 (0.1254)\tLoss 0.6339 (0.6064)\tAccu 0.6523 (0.6841)\t\n",
            "Epoch: [85][9/10]\tTime 0.161 (0.156)\tData 0.1305 (0.1260)\tLoss 0.6100 (0.6068)\tAccu 0.6719 (0.6827)\t\n",
            "Epoch: [85][10/10]\tTime 0.126 (0.153)\tData 0.0969 (0.1231)\tLoss 0.6309 (0.6087)\tAccu 0.7041 (0.6844)\t\n",
            "Time 0.044\tAccu 0.6700\tLoss 0.6325\t\n",
            "Epoch: [86][1/10]\tTime 0.156 (0.156)\tData 0.1254 (0.1254)\tLoss 0.6179 (0.6179)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [86][2/10]\tTime 0.154 (0.155)\tData 0.1237 (0.1246)\tLoss 0.5730 (0.5954)\tAccu 0.7188 (0.7051)\t\n",
            "Epoch: [86][3/10]\tTime 0.156 (0.155)\tData 0.1256 (0.1249)\tLoss 0.6207 (0.6039)\tAccu 0.6680 (0.6927)\t\n",
            "Epoch: [86][4/10]\tTime 0.153 (0.155)\tData 0.1218 (0.1241)\tLoss 0.5868 (0.5996)\tAccu 0.7070 (0.6963)\t\n",
            "Epoch: [86][5/10]\tTime 0.153 (0.155)\tData 0.1234 (0.1240)\tLoss 0.6237 (0.6044)\tAccu 0.6641 (0.6898)\t\n",
            "Epoch: [86][6/10]\tTime 0.166 (0.156)\tData 0.1350 (0.1258)\tLoss 0.5854 (0.6012)\tAccu 0.6953 (0.6908)\t\n",
            "Epoch: [86][7/10]\tTime 0.156 (0.156)\tData 0.1250 (0.1257)\tLoss 0.5954 (0.6004)\tAccu 0.6680 (0.6875)\t\n",
            "Epoch: [86][8/10]\tTime 0.158 (0.157)\tData 0.1273 (0.1259)\tLoss 0.6175 (0.6025)\tAccu 0.6680 (0.6851)\t\n",
            "Epoch: [86][9/10]\tTime 0.155 (0.156)\tData 0.1244 (0.1257)\tLoss 0.6533 (0.6082)\tAccu 0.6133 (0.6771)\t\n",
            "Epoch: [86][10/10]\tTime 0.126 (0.153)\tData 0.0955 (0.1227)\tLoss 0.6074 (0.6081)\tAccu 0.6888 (0.6780)\t\n",
            "Time 0.043\tAccu 0.6780\tLoss 0.6148\t\n",
            "Epoch: [87][1/10]\tTime 0.153 (0.153)\tData 0.1220 (0.1220)\tLoss 0.5784 (0.5784)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [87][2/10]\tTime 0.155 (0.154)\tData 0.1239 (0.1230)\tLoss 0.5975 (0.5880)\tAccu 0.6875 (0.6973)\t\n",
            "Epoch: [87][3/10]\tTime 0.156 (0.155)\tData 0.1258 (0.1239)\tLoss 0.6284 (0.6014)\tAccu 0.6758 (0.6901)\t\n",
            "Epoch: [87][4/10]\tTime 0.154 (0.154)\tData 0.1238 (0.1239)\tLoss 0.5992 (0.6009)\tAccu 0.6680 (0.6846)\t\n",
            "Epoch: [87][5/10]\tTime 0.155 (0.155)\tData 0.1254 (0.1242)\tLoss 0.5410 (0.5889)\tAccu 0.7305 (0.6937)\t\n",
            "Epoch: [87][6/10]\tTime 0.154 (0.154)\tData 0.1242 (0.1242)\tLoss 0.6240 (0.5947)\tAccu 0.6445 (0.6855)\t\n",
            "Epoch: [87][7/10]\tTime 0.161 (0.155)\tData 0.1306 (0.1251)\tLoss 0.5832 (0.5931)\tAccu 0.6953 (0.6869)\t\n",
            "Epoch: [87][8/10]\tTime 0.159 (0.156)\tData 0.1281 (0.1255)\tLoss 0.5904 (0.5928)\tAccu 0.7031 (0.6890)\t\n",
            "Epoch: [87][9/10]\tTime 0.157 (0.156)\tData 0.1275 (0.1257)\tLoss 0.6562 (0.5998)\tAccu 0.6094 (0.6801)\t\n",
            "Epoch: [87][10/10]\tTime 0.124 (0.153)\tData 0.0934 (0.1225)\tLoss 0.6270 (0.6019)\tAccu 0.6429 (0.6772)\t\n",
            "Time 0.043\tAccu 0.6740\tLoss 0.6171\t\n",
            "Epoch: [88][1/10]\tTime 0.154 (0.154)\tData 0.1233 (0.1233)\tLoss 0.5990 (0.5990)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [88][2/10]\tTime 0.159 (0.156)\tData 0.1281 (0.1257)\tLoss 0.6162 (0.6076)\tAccu 0.6641 (0.6777)\t\n",
            "Epoch: [88][3/10]\tTime 0.153 (0.155)\tData 0.1231 (0.1248)\tLoss 0.6187 (0.6113)\tAccu 0.6523 (0.6693)\t\n",
            "Epoch: [88][4/10]\tTime 0.157 (0.156)\tData 0.1275 (0.1255)\tLoss 0.5729 (0.6017)\tAccu 0.6875 (0.6738)\t\n",
            "Epoch: [88][5/10]\tTime 0.158 (0.156)\tData 0.1282 (0.1260)\tLoss 0.5904 (0.5994)\tAccu 0.6758 (0.6742)\t\n",
            "Epoch: [88][6/10]\tTime 0.158 (0.157)\tData 0.1282 (0.1264)\tLoss 0.5789 (0.5960)\tAccu 0.6875 (0.6764)\t\n",
            "Epoch: [88][7/10]\tTime 0.157 (0.157)\tData 0.1265 (0.1264)\tLoss 0.6319 (0.6011)\tAccu 0.6484 (0.6724)\t\n",
            "Epoch: [88][8/10]\tTime 0.160 (0.157)\tData 0.1297 (0.1268)\tLoss 0.6206 (0.6036)\tAccu 0.6680 (0.6719)\t\n",
            "Epoch: [88][9/10]\tTime 0.156 (0.157)\tData 0.1258 (0.1267)\tLoss 0.5884 (0.6019)\tAccu 0.7305 (0.6784)\t\n",
            "Epoch: [88][10/10]\tTime 0.127 (0.154)\tData 0.0971 (0.1237)\tLoss 0.5699 (0.5994)\tAccu 0.7296 (0.6824)\t\n",
            "Time 0.043\tAccu 0.6780\tLoss 0.6132\t\n",
            "Epoch: [89][1/10]\tTime 0.152 (0.152)\tData 0.1219 (0.1219)\tLoss 0.6786 (0.6786)\tAccu 0.6016 (0.6016)\t\n",
            "Epoch: [89][2/10]\tTime 0.155 (0.154)\tData 0.1258 (0.1238)\tLoss 0.5783 (0.6284)\tAccu 0.6875 (0.6445)\t\n",
            "Epoch: [89][3/10]\tTime 0.155 (0.154)\tData 0.1255 (0.1244)\tLoss 0.5841 (0.6137)\tAccu 0.6953 (0.6615)\t\n",
            "Epoch: [89][4/10]\tTime 0.153 (0.154)\tData 0.1230 (0.1240)\tLoss 0.6015 (0.6106)\tAccu 0.6797 (0.6660)\t\n",
            "Epoch: [89][5/10]\tTime 0.151 (0.153)\tData 0.1213 (0.1235)\tLoss 0.5985 (0.6082)\tAccu 0.6992 (0.6727)\t\n",
            "Epoch: [89][6/10]\tTime 0.151 (0.153)\tData 0.1199 (0.1229)\tLoss 0.5652 (0.6010)\tAccu 0.6797 (0.6738)\t\n",
            "Epoch: [89][7/10]\tTime 0.154 (0.153)\tData 0.1235 (0.1230)\tLoss 0.5874 (0.5991)\tAccu 0.6797 (0.6747)\t\n",
            "Epoch: [89][8/10]\tTime 0.166 (0.155)\tData 0.1350 (0.1245)\tLoss 0.5576 (0.5939)\tAccu 0.7188 (0.6802)\t\n",
            "Epoch: [89][9/10]\tTime 0.156 (0.155)\tData 0.1254 (0.1246)\tLoss 0.6189 (0.5967)\tAccu 0.6602 (0.6780)\t\n",
            "Epoch: [89][10/10]\tTime 0.123 (0.152)\tData 0.0936 (0.1215)\tLoss 0.6135 (0.5980)\tAccu 0.6378 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6720\tLoss 0.6217\t\n",
            "Epoch: [90][1/10]\tTime 0.154 (0.154)\tData 0.1244 (0.1244)\tLoss 0.5872 (0.5872)\tAccu 0.7383 (0.7383)\t\n",
            "Epoch: [90][2/10]\tTime 0.155 (0.155)\tData 0.1234 (0.1239)\tLoss 0.5949 (0.5911)\tAccu 0.6875 (0.7129)\t\n",
            "Epoch: [90][3/10]\tTime 0.151 (0.154)\tData 0.1215 (0.1231)\tLoss 0.6071 (0.5964)\tAccu 0.6641 (0.6966)\t\n",
            "Epoch: [90][4/10]\tTime 0.151 (0.153)\tData 0.1201 (0.1223)\tLoss 0.6155 (0.6012)\tAccu 0.6719 (0.6904)\t\n",
            "Epoch: [90][5/10]\tTime 0.152 (0.153)\tData 0.1225 (0.1224)\tLoss 0.6017 (0.6013)\tAccu 0.6758 (0.6875)\t\n",
            "Epoch: [90][6/10]\tTime 0.149 (0.152)\tData 0.1194 (0.1219)\tLoss 0.5757 (0.5970)\tAccu 0.6914 (0.6882)\t\n",
            "Epoch: [90][7/10]\tTime 0.153 (0.152)\tData 0.1227 (0.1220)\tLoss 0.5880 (0.5957)\tAccu 0.7070 (0.6908)\t\n",
            "Epoch: [90][8/10]\tTime 0.149 (0.152)\tData 0.1182 (0.1215)\tLoss 0.5834 (0.5942)\tAccu 0.7188 (0.6943)\t\n",
            "Epoch: [90][9/10]\tTime 0.165 (0.153)\tData 0.1344 (0.1230)\tLoss 0.6232 (0.5974)\tAccu 0.6328 (0.6875)\t\n",
            "Epoch: [90][10/10]\tTime 0.124 (0.151)\tData 0.0949 (0.1202)\tLoss 0.5893 (0.5968)\tAccu 0.6888 (0.6876)\t\n",
            "Time 0.043\tAccu 0.6780\tLoss 0.6193\t\n",
            "Epoch: [91][1/10]\tTime 0.154 (0.154)\tData 0.1241 (0.1241)\tLoss 0.6202 (0.6202)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [91][2/10]\tTime 0.156 (0.155)\tData 0.1261 (0.1251)\tLoss 0.5997 (0.6099)\tAccu 0.7344 (0.7070)\t\n",
            "Epoch: [91][3/10]\tTime 0.155 (0.155)\tData 0.1246 (0.1249)\tLoss 0.5920 (0.6040)\tAccu 0.6758 (0.6966)\t\n",
            "Epoch: [91][4/10]\tTime 0.159 (0.156)\tData 0.1267 (0.1254)\tLoss 0.5297 (0.5854)\tAccu 0.7227 (0.7031)\t\n",
            "Epoch: [91][5/10]\tTime 0.152 (0.155)\tData 0.1221 (0.1247)\tLoss 0.5721 (0.5827)\tAccu 0.6992 (0.7023)\t\n",
            "Epoch: [91][6/10]\tTime 0.155 (0.155)\tData 0.1246 (0.1247)\tLoss 0.6429 (0.5928)\tAccu 0.6719 (0.6973)\t\n",
            "Epoch: [91][7/10]\tTime 0.155 (0.155)\tData 0.1252 (0.1248)\tLoss 0.6296 (0.5980)\tAccu 0.6445 (0.6897)\t\n",
            "Epoch: [91][8/10]\tTime 0.156 (0.155)\tData 0.1250 (0.1248)\tLoss 0.6315 (0.6022)\tAccu 0.6562 (0.6855)\t\n",
            "Epoch: [91][9/10]\tTime 0.157 (0.156)\tData 0.1270 (0.1250)\tLoss 0.6374 (0.6061)\tAccu 0.6641 (0.6832)\t\n",
            "Epoch: [91][10/10]\tTime 0.125 (0.153)\tData 0.0954 (0.1221)\tLoss 0.6184 (0.6071)\tAccu 0.7143 (0.6856)\t\n",
            "Time 0.044\tAccu 0.6780\tLoss 0.6129\t\n",
            "Epoch: [92][1/10]\tTime 0.156 (0.156)\tData 0.1250 (0.1250)\tLoss 0.6080 (0.6080)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [92][2/10]\tTime 0.157 (0.157)\tData 0.1268 (0.1259)\tLoss 0.6074 (0.6077)\tAccu 0.6719 (0.6660)\t\n",
            "Epoch: [92][3/10]\tTime 0.159 (0.157)\tData 0.1286 (0.1268)\tLoss 0.6085 (0.6080)\tAccu 0.6680 (0.6667)\t\n",
            "Epoch: [92][4/10]\tTime 0.159 (0.158)\tData 0.1283 (0.1272)\tLoss 0.6398 (0.6159)\tAccu 0.6445 (0.6611)\t\n",
            "Epoch: [92][5/10]\tTime 0.155 (0.157)\tData 0.1248 (0.1267)\tLoss 0.5957 (0.6119)\tAccu 0.6719 (0.6633)\t\n",
            "Epoch: [92][6/10]\tTime 0.158 (0.157)\tData 0.1267 (0.1267)\tLoss 0.5964 (0.6093)\tAccu 0.6914 (0.6680)\t\n",
            "Epoch: [92][7/10]\tTime 0.153 (0.157)\tData 0.1237 (0.1263)\tLoss 0.6052 (0.6087)\tAccu 0.6836 (0.6702)\t\n",
            "Epoch: [92][8/10]\tTime 0.157 (0.157)\tData 0.1267 (0.1263)\tLoss 0.5857 (0.6058)\tAccu 0.7031 (0.6743)\t\n",
            "Epoch: [92][9/10]\tTime 0.153 (0.156)\tData 0.1237 (0.1260)\tLoss 0.6129 (0.6066)\tAccu 0.6680 (0.6736)\t\n",
            "Epoch: [92][10/10]\tTime 0.127 (0.153)\tData 0.0977 (0.1232)\tLoss 0.5903 (0.6053)\tAccu 0.6888 (0.6748)\t\n",
            "Time 0.043\tAccu 0.6800\tLoss 0.6235\t\n",
            "Epoch: [93][1/10]\tTime 0.158 (0.158)\tData 0.1274 (0.1274)\tLoss 0.5830 (0.5830)\tAccu 0.7031 (0.7031)\t\n",
            "Epoch: [93][2/10]\tTime 0.161 (0.160)\tData 0.1311 (0.1293)\tLoss 0.6015 (0.5922)\tAccu 0.6719 (0.6875)\t\n",
            "Epoch: [93][3/10]\tTime 0.163 (0.161)\tData 0.1324 (0.1303)\tLoss 0.5964 (0.5936)\tAccu 0.6875 (0.6875)\t\n",
            "Epoch: [93][4/10]\tTime 0.155 (0.159)\tData 0.1238 (0.1287)\tLoss 0.6136 (0.5986)\tAccu 0.6250 (0.6719)\t\n",
            "Epoch: [93][5/10]\tTime 0.159 (0.159)\tData 0.1271 (0.1284)\tLoss 0.5810 (0.5951)\tAccu 0.7500 (0.6875)\t\n",
            "Epoch: [93][6/10]\tTime 0.160 (0.159)\tData 0.1299 (0.1286)\tLoss 0.5997 (0.5959)\tAccu 0.6992 (0.6895)\t\n",
            "Epoch: [93][7/10]\tTime 0.160 (0.160)\tData 0.1299 (0.1288)\tLoss 0.5696 (0.5921)\tAccu 0.7188 (0.6936)\t\n",
            "Epoch: [93][8/10]\tTime 0.156 (0.159)\tData 0.1252 (0.1284)\tLoss 0.6328 (0.5972)\tAccu 0.6484 (0.6880)\t\n",
            "Epoch: [93][9/10]\tTime 0.161 (0.159)\tData 0.1312 (0.1287)\tLoss 0.6434 (0.6023)\tAccu 0.6562 (0.6845)\t\n",
            "Epoch: [93][10/10]\tTime 0.128 (0.156)\tData 0.0982 (0.1256)\tLoss 0.6491 (0.6060)\tAccu 0.6224 (0.6796)\t\n",
            "Time 0.043\tAccu 0.6780\tLoss 0.6215\t\n",
            "Epoch: [94][1/10]\tTime 0.153 (0.153)\tData 0.1230 (0.1230)\tLoss 0.6103 (0.6103)\tAccu 0.6875 (0.6875)\t\n",
            "Epoch: [94][2/10]\tTime 0.160 (0.156)\tData 0.1295 (0.1263)\tLoss 0.6435 (0.6269)\tAccu 0.6406 (0.6641)\t\n",
            "Epoch: [94][3/10]\tTime 0.157 (0.157)\tData 0.1265 (0.1264)\tLoss 0.6175 (0.6238)\tAccu 0.7031 (0.6771)\t\n",
            "Epoch: [94][4/10]\tTime 0.164 (0.159)\tData 0.1336 (0.1282)\tLoss 0.6075 (0.6197)\tAccu 0.6680 (0.6748)\t\n",
            "Epoch: [94][5/10]\tTime 0.156 (0.158)\tData 0.1255 (0.1276)\tLoss 0.6157 (0.6189)\tAccu 0.6445 (0.6687)\t\n",
            "Epoch: [94][6/10]\tTime 0.161 (0.159)\tData 0.1314 (0.1283)\tLoss 0.6229 (0.6196)\tAccu 0.6680 (0.6686)\t\n",
            "Epoch: [94][7/10]\tTime 0.155 (0.158)\tData 0.1242 (0.1277)\tLoss 0.5495 (0.6095)\tAccu 0.6953 (0.6724)\t\n",
            "Epoch: [94][8/10]\tTime 0.161 (0.158)\tData 0.1303 (0.1280)\tLoss 0.6205 (0.6109)\tAccu 0.6680 (0.6719)\t\n",
            "Epoch: [94][9/10]\tTime 0.157 (0.158)\tData 0.1260 (0.1278)\tLoss 0.5601 (0.6053)\tAccu 0.7227 (0.6775)\t\n",
            "Epoch: [94][10/10]\tTime 0.131 (0.155)\tData 0.1006 (0.1251)\tLoss 0.5885 (0.6040)\tAccu 0.7092 (0.6800)\t\n",
            "Time 0.045\tAccu 0.6860\tLoss 0.6118\t\n",
            "Epoch: [95][1/10]\tTime 0.153 (0.153)\tData 0.1230 (0.1230)\tLoss 0.6088 (0.6088)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [95][2/10]\tTime 0.157 (0.155)\tData 0.1252 (0.1241)\tLoss 0.6405 (0.6247)\tAccu 0.5938 (0.6426)\t\n",
            "Epoch: [95][3/10]\tTime 0.161 (0.157)\tData 0.1313 (0.1265)\tLoss 0.5932 (0.6142)\tAccu 0.6992 (0.6615)\t\n",
            "Epoch: [95][4/10]\tTime 0.153 (0.156)\tData 0.1230 (0.1256)\tLoss 0.6147 (0.6143)\tAccu 0.6484 (0.6582)\t\n",
            "Epoch: [95][5/10]\tTime 0.160 (0.157)\tData 0.1301 (0.1265)\tLoss 0.5832 (0.6081)\tAccu 0.7031 (0.6672)\t\n",
            "Epoch: [95][6/10]\tTime 0.151 (0.156)\tData 0.1211 (0.1256)\tLoss 0.6048 (0.6075)\tAccu 0.6523 (0.6647)\t\n",
            "Epoch: [95][7/10]\tTime 0.155 (0.156)\tData 0.1246 (0.1255)\tLoss 0.5546 (0.6000)\tAccu 0.7305 (0.6741)\t\n",
            "Epoch: [95][8/10]\tTime 0.154 (0.156)\tData 0.1216 (0.1250)\tLoss 0.5913 (0.5989)\tAccu 0.6680 (0.6733)\t\n",
            "Epoch: [95][9/10]\tTime 0.166 (0.157)\tData 0.1353 (0.1262)\tLoss 0.5909 (0.5980)\tAccu 0.6875 (0.6749)\t\n",
            "Epoch: [95][10/10]\tTime 0.123 (0.153)\tData 0.0943 (0.1230)\tLoss 0.5437 (0.5937)\tAccu 0.7194 (0.6784)\t\n",
            "Time 0.042\tAccu 0.6820\tLoss 0.6115\t\n",
            "Epoch: [96][1/10]\tTime 0.160 (0.160)\tData 0.1302 (0.1302)\tLoss 0.6029 (0.6029)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [96][2/10]\tTime 0.153 (0.157)\tData 0.1225 (0.1263)\tLoss 0.5864 (0.5946)\tAccu 0.6836 (0.6719)\t\n",
            "Epoch: [96][3/10]\tTime 0.159 (0.157)\tData 0.1287 (0.1271)\tLoss 0.6060 (0.5984)\tAccu 0.6680 (0.6706)\t\n",
            "Epoch: [96][4/10]\tTime 0.156 (0.157)\tData 0.1257 (0.1268)\tLoss 0.6040 (0.5998)\tAccu 0.7031 (0.6787)\t\n",
            "Epoch: [96][5/10]\tTime 0.159 (0.158)\tData 0.1288 (0.1272)\tLoss 0.5790 (0.5956)\tAccu 0.7188 (0.6867)\t\n",
            "Epoch: [96][6/10]\tTime 0.157 (0.158)\tData 0.1260 (0.1270)\tLoss 0.6213 (0.5999)\tAccu 0.6641 (0.6829)\t\n",
            "Epoch: [96][7/10]\tTime 0.156 (0.157)\tData 0.1264 (0.1269)\tLoss 0.6197 (0.6027)\tAccu 0.6719 (0.6814)\t\n",
            "Epoch: [96][8/10]\tTime 0.157 (0.157)\tData 0.1265 (0.1268)\tLoss 0.5761 (0.5994)\tAccu 0.7031 (0.6841)\t\n",
            "Epoch: [96][9/10]\tTime 0.158 (0.157)\tData 0.1272 (0.1269)\tLoss 0.5931 (0.5987)\tAccu 0.6875 (0.6845)\t\n",
            "Epoch: [96][10/10]\tTime 0.128 (0.154)\tData 0.0997 (0.1242)\tLoss 0.5644 (0.5960)\tAccu 0.7092 (0.6864)\t\n",
            "Time 0.044\tAccu 0.6840\tLoss 0.6120\t\n",
            "Epoch: [97][1/10]\tTime 0.154 (0.154)\tData 0.1225 (0.1225)\tLoss 0.5795 (0.5795)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [97][2/10]\tTime 0.163 (0.158)\tData 0.1328 (0.1276)\tLoss 0.5782 (0.5788)\tAccu 0.6758 (0.6934)\t\n",
            "Epoch: [97][3/10]\tTime 0.154 (0.157)\tData 0.1231 (0.1261)\tLoss 0.5774 (0.5784)\tAccu 0.6953 (0.6940)\t\n",
            "Epoch: [97][4/10]\tTime 0.157 (0.157)\tData 0.1265 (0.1262)\tLoss 0.6254 (0.5901)\tAccu 0.6328 (0.6787)\t\n",
            "Epoch: [97][5/10]\tTime 0.156 (0.157)\tData 0.1233 (0.1256)\tLoss 0.5860 (0.5893)\tAccu 0.7188 (0.6867)\t\n",
            "Epoch: [97][6/10]\tTime 0.159 (0.157)\tData 0.1282 (0.1261)\tLoss 0.6199 (0.5944)\tAccu 0.6719 (0.6842)\t\n",
            "Epoch: [97][7/10]\tTime 0.153 (0.157)\tData 0.1227 (0.1256)\tLoss 0.5878 (0.5935)\tAccu 0.6875 (0.6847)\t\n",
            "Epoch: [97][8/10]\tTime 0.160 (0.157)\tData 0.1292 (0.1260)\tLoss 0.6224 (0.5971)\tAccu 0.6680 (0.6826)\t\n",
            "Epoch: [97][9/10]\tTime 0.152 (0.156)\tData 0.1215 (0.1255)\tLoss 0.6534 (0.6033)\tAccu 0.6250 (0.6762)\t\n",
            "Epoch: [97][10/10]\tTime 0.125 (0.153)\tData 0.0957 (0.1225)\tLoss 0.5755 (0.6011)\tAccu 0.6888 (0.6772)\t\n",
            "Time 0.044\tAccu 0.6660\tLoss 0.6162\t\n",
            "Epoch: [98][1/10]\tTime 0.162 (0.162)\tData 0.1305 (0.1305)\tLoss 0.6183 (0.6183)\tAccu 0.6328 (0.6328)\t\n",
            "Epoch: [98][2/10]\tTime 0.155 (0.159)\tData 0.1241 (0.1273)\tLoss 0.6151 (0.6167)\tAccu 0.6602 (0.6465)\t\n",
            "Epoch: [98][3/10]\tTime 0.158 (0.158)\tData 0.1275 (0.1274)\tLoss 0.5868 (0.6067)\tAccu 0.7109 (0.6680)\t\n",
            "Epoch: [98][4/10]\tTime 0.158 (0.158)\tData 0.1274 (0.1274)\tLoss 0.5792 (0.5998)\tAccu 0.7031 (0.6768)\t\n",
            "Epoch: [98][5/10]\tTime 0.156 (0.158)\tData 0.1259 (0.1271)\tLoss 0.5929 (0.5985)\tAccu 0.6953 (0.6805)\t\n",
            "Epoch: [98][6/10]\tTime 0.152 (0.157)\tData 0.1218 (0.1262)\tLoss 0.6093 (0.6003)\tAccu 0.6562 (0.6764)\t\n",
            "Epoch: [98][7/10]\tTime 0.156 (0.157)\tData 0.1246 (0.1260)\tLoss 0.6230 (0.6035)\tAccu 0.6367 (0.6708)\t\n",
            "Epoch: [98][8/10]\tTime 0.155 (0.156)\tData 0.1245 (0.1258)\tLoss 0.5868 (0.6014)\tAccu 0.7109 (0.6758)\t\n",
            "Epoch: [98][9/10]\tTime 0.156 (0.156)\tData 0.1262 (0.1258)\tLoss 0.6182 (0.6033)\tAccu 0.7070 (0.6793)\t\n",
            "Epoch: [98][10/10]\tTime 0.122 (0.153)\tData 0.0932 (0.1226)\tLoss 0.5762 (0.6012)\tAccu 0.7296 (0.6832)\t\n",
            "Time 0.044\tAccu 0.6840\tLoss 0.6109\t\n",
            "Epoch: [99][1/10]\tTime 0.164 (0.164)\tData 0.1336 (0.1336)\tLoss 0.5711 (0.5711)\tAccu 0.7344 (0.7344)\t\n",
            "Epoch: [99][2/10]\tTime 0.156 (0.160)\tData 0.1258 (0.1297)\tLoss 0.6177 (0.5944)\tAccu 0.6602 (0.6973)\t\n",
            "Epoch: [99][3/10]\tTime 0.157 (0.159)\tData 0.1270 (0.1288)\tLoss 0.6322 (0.6070)\tAccu 0.6602 (0.6849)\t\n",
            "Epoch: [99][4/10]\tTime 0.162 (0.160)\tData 0.1309 (0.1293)\tLoss 0.5547 (0.5939)\tAccu 0.7188 (0.6934)\t\n",
            "Epoch: [99][5/10]\tTime 0.158 (0.159)\tData 0.1267 (0.1288)\tLoss 0.6074 (0.5966)\tAccu 0.6641 (0.6875)\t\n",
            "Epoch: [99][6/10]\tTime 0.156 (0.159)\tData 0.1259 (0.1283)\tLoss 0.5732 (0.5927)\tAccu 0.7578 (0.6992)\t\n",
            "Epoch: [99][7/10]\tTime 0.157 (0.159)\tData 0.1249 (0.1278)\tLoss 0.5804 (0.5910)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [99][8/10]\tTime 0.161 (0.159)\tData 0.1305 (0.1282)\tLoss 0.6216 (0.5948)\tAccu 0.6328 (0.6909)\t\n",
            "Epoch: [99][9/10]\tTime 0.156 (0.159)\tData 0.1255 (0.1279)\tLoss 0.6121 (0.5967)\tAccu 0.6562 (0.6871)\t\n",
            "Epoch: [99][10/10]\tTime 0.120 (0.155)\tData 0.0915 (0.1242)\tLoss 0.6310 (0.5994)\tAccu 0.6633 (0.6852)\t\n",
            "Time 0.043\tAccu 0.6780\tLoss 0.6187\t\n",
            "Epoch: [100][1/10]\tTime 0.151 (0.151)\tData 0.1209 (0.1209)\tLoss 0.6074 (0.6074)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [100][2/10]\tTime 0.161 (0.156)\tData 0.1302 (0.1255)\tLoss 0.6255 (0.6165)\tAccu 0.6523 (0.6660)\t\n",
            "Epoch: [100][3/10]\tTime 0.159 (0.157)\tData 0.1292 (0.1268)\tLoss 0.6068 (0.6133)\tAccu 0.6758 (0.6693)\t\n",
            "Epoch: [100][4/10]\tTime 0.154 (0.156)\tData 0.1239 (0.1260)\tLoss 0.5630 (0.6007)\tAccu 0.7188 (0.6816)\t\n",
            "Epoch: [100][5/10]\tTime 0.148 (0.155)\tData 0.1186 (0.1246)\tLoss 0.6024 (0.6011)\tAccu 0.6836 (0.6820)\t\n",
            "Epoch: [100][6/10]\tTime 0.160 (0.155)\tData 0.1283 (0.1252)\tLoss 0.5736 (0.5965)\tAccu 0.7070 (0.6862)\t\n",
            "Epoch: [100][7/10]\tTime 0.149 (0.155)\tData 0.1188 (0.1243)\tLoss 0.6181 (0.5996)\tAccu 0.6484 (0.6808)\t\n",
            "Epoch: [100][8/10]\tTime 0.155 (0.155)\tData 0.1251 (0.1244)\tLoss 0.5720 (0.5961)\tAccu 0.7031 (0.6836)\t\n",
            "Epoch: [100][9/10]\tTime 0.148 (0.154)\tData 0.1183 (0.1237)\tLoss 0.5945 (0.5959)\tAccu 0.7070 (0.6862)\t\n",
            "Epoch: [100][10/10]\tTime 0.126 (0.151)\tData 0.0972 (0.1210)\tLoss 0.6064 (0.5968)\tAccu 0.6837 (0.6860)\t\n",
            "Time 0.042\tAccu 0.6720\tLoss 0.6145\t\n",
            "0.6108505697971714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWJZLiIiMmkj",
        "colab_type": "text"
      },
      "source": [
        "#### Plotting Your Results [4 pts]\n",
        "\n",
        "You need to provide two distinct plots, one demonstrating training and validation losses in y axis and iteration in the x axis and the other demonstrating training and validation accuracies in the y axis and iteration  in the x axis. <br><br>\n",
        "Please note that we need these plots to see if your model behaves as expected. Therefore, you may lose additional points if you do not provide these plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDfGUr10Mmkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "a17cf4f2-3a54-4035-c701-d9dd5d49dcd3"
      },
      "source": [
        "# write your code in this cell to plot your results\n",
        "\n",
        "# plt.plot(range(len(train_loss)-10), train_loss[10:], label='Training Loss')\n",
        "# plt.plot(range(len(val_loss)-10), val_loss[10:], label='Validation Loss')\n",
        "\n",
        "plt.plot(range(len(train_loss)), train_loss, label='Training Loss')\n",
        "plt.plot(range(len(val_loss)), val_loss, label='Validation Loss')\n",
        "\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(range(len(train_acc)), train_acc, label='Train Accuracy')\n",
        "plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy')\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7v8am0hMmkm",
        "colab_type": "text"
      },
      "source": [
        "#### Testing [5 pts]\n",
        "\n",
        "Test your final, i.e. best, model on your test set. Calculate confusion matrix, F1 score, precision and recall values and report these findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq6YqUadMmkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "8f38f6f8-dcc0-41d5-99f8-63328f75fb0e"
      },
      "source": [
        "# write your code in this cell to test your best model with the test dataset\n",
        "model = FNet()\n",
        "model.load_state_dict(torch.load('FNet.pth'))\n",
        "model.to(device)\n",
        "\n",
        "batch_time = AverageMeter()\n",
        "accuracies = AverageMeter()\n",
        "losses = AverageMeter()\n",
        "test_loader = DataLoader(test_set, test_batch)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "\n",
        "TN = 0\n",
        "TP = 0\n",
        "FN = 0\n",
        "FP = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    end = time.time()\n",
        "    for batch_idx, (data, labels) in enumerate(test_loader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        output = model(data.float())\n",
        "        # print(output[:10])\n",
        "        # output = torch.round(output)\n",
        "        # output = torch.flatten(output)\n",
        "        # print(output[:10])\n",
        "\n",
        "        loss = criterion(output, labels.unsqueeze(1)) \n",
        "        output = torch.round(torch.sigmoid(output))\n",
        "        output = torch.flatten(output)  \n",
        "        prec = accuracy(output, labels)\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            if output[i] == 0 and labels[i] == 0:\n",
        "                TN += 1\n",
        "            if output[i] == 0 and labels[i] == 1:\n",
        "                FN += 1\n",
        "            if output[i] == 1 and labels[i] == 0:\n",
        "                FP += 1\n",
        "            if output[i] == 1 and labels[i] == 1:\n",
        "                TP += 1\n",
        "\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        accuracies.update(prec, data.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        \n",
        "    print('Time {batch_time.avg:.3f}\\t'\n",
        "            'Accu {acc.avg:.4f}\\t'\n",
        "            'Loss {loss.avg:.4f}\\t'.format(\n",
        "            batch_time=batch_time, \n",
        "            acc=accuracies,\n",
        "            loss=losses))\n",
        "    \n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(\"TP = \" + str(TP) + \"   FP = \" + str(FP))\n",
        "    print(\"FN = \" + str(FN) + \"  TN = \" + str(TN))\n",
        "\n",
        "    precision = TP/(TP + FP)\n",
        "    recall = TP/(TP + FN)\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)    \n",
        "    \n",
        "    print(\"Precision = \" + str(precision))\n",
        "    print(\"Recall = \" + str(recall))\n",
        "    print(\"F1-score = \" + str(F1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time 0.090\tAccu 0.6680\tLoss 0.6303\t\n",
            "Confusion Matrix:\n",
            "TP = 6   FP = 5\n",
            "FN = 161  TN = 328\n",
            "Precision = 0.5454545454545454\n",
            "Recall = 0.03592814371257485\n",
            "F1-score = 0.06741573033707866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-J6f1QPMmk3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Convolutional Neural Network (CNN) [30 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssXrFEtjMmk4",
        "colab_type": "text"
      },
      "source": [
        "#### Data Loader [5 pts]\n",
        "\n",
        "In this part, you will train a CNN for the same problem. Again, the pixel values need to be normalized to [0,1] range. Please do **not** change images to grayscale this time. First, implement the data loader (OcularDataset). You have to divide the files into three sets which are <b>train (5/7)</b>, <b>validation (1/7)</b> and **test (1/7)**.  These non-overlapping splits, which are subsets of OcularDataset, should be retrieved using the \"get_dataset\" function.<br> You may use your data loader from the previous sections with propoer modifications. Note that this time you do **not** need to flatten the image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLpZ2tCnMmk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OcularDatasetCNN(Dataset):\n",
        "    # TODO:\n",
        "    # Define constructor for SVHNDataset class\n",
        "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "\n",
        "    '''This function should return sample count in the dataset'''\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
        "    def __getitem__(self, index):\n",
        "        _x = self.data[index, 0]\n",
        "        _y = float(self.data[index, 1])\n",
        "        return _x, _y\n",
        "\n",
        "def normalize(column):\n",
        "        return np.divide((column - np.min(column)),(np.max(column - np.min(column))))\n",
        "\n",
        "def get_dataset_cnn(root):\n",
        "    # TODO: \n",
        "    # Read dataset files\n",
        "    # Construct training, validation and test sets\n",
        "    # Normalize & flatten datasets\n",
        "\n",
        "    labels_path = os.path.join(root, 'labels.xlsx') \n",
        "    labels = pd.read_excel(labels_path)\n",
        "    y = np.array(labels[1])\n",
        "    y = y.reshape(3500, -1)\n",
        "\n",
        "    images = []\n",
        "    # image_path = (os.path.join(root, 'data/images/*.jpg'))\n",
        "    image_path = '/temp/data/images/*.jpg'\n",
        "    \n",
        "    regex = re.compile(r'\\d+')\n",
        "    for im in glob.iglob(image_path):\n",
        "        num = int(regex.search(im).group(0))\n",
        "        images.append([(np.asarray(Image.open(im)))/255.0, num])\n",
        "    \n",
        "    images = np.array(images)\n",
        "    images = images[images[:,-1].argsort()][:,:-1]\n",
        "    #np.squeeze(images)\n",
        "    # plt.imshow(images[4][0])\n",
        "\n",
        "\n",
        "    #images = np.apply_along_axis(normalize, 0, images)\n",
        "    data = []\n",
        "    i = 0\n",
        "    for image in images:\n",
        "         data.append([image[0], y[i]])\n",
        "         i += 1\n",
        "\n",
        "    data = np.array(data)\n",
        "\n",
        "    # np.random.seed(42)\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    train_set, val_set, test_set = np.array_split(data, [int(data.shape[0]*(5/7)), int(data.shape[0]*(5/7)+data.shape[0]*(1/7))])\n",
        "\n",
        "\n",
        "\n",
        "    train_set = OcularDatasetCNN(train_set)\n",
        "    val_set = OcularDatasetCNN(val_set)\n",
        "    test_set = OcularDatasetCNN(test_set)\n",
        "\n",
        "    #return train_dataset, val_dataset, test_dataset\n",
        "    return train_set, val_set, test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15V10iWT2HrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set_cnn, val_set_cnn, test_set_cnn = get_dataset_cnn(root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-RsQD3EMmk8",
        "colab_type": "text"
      },
      "source": [
        "#### Convolutional Neural Network [8 pts]\n",
        "\n",
        "Now implement your CNN. ConvNet class will represent your convolutional neural network. Implement 3 layers of convolution: \n",
        "1. <i>> 8 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br> \n",
        "    <i>> ReLU </i><br>\n",
        "2. <i>> 16 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br>\n",
        "    <i>> ReLU </i><br>\n",
        "    <i>> MaxPool 2 x 2 </i><br>   \n",
        "3. <i>> 32 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br>\n",
        "    <i>> ReLU </i><br>\n",
        "    <i>> MaxPool 2 x 2 </i><br>\n",
        "\n",
        "As a classification layer, you need to add only one more fully-connected layer at the end of the network. You need to choose the appropriate input and output neuron sizes and the activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y_TUgUNJh5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    '''Define your neural network'''\n",
        "    def __init__(self, **kwargs): # you can add any additional parameters you want \n",
        "        super(ConvNet, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=0),   #128x256x3 -> 126x254x8\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0),  #126x254x8 -> 124x252x16\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2),                                                    #124x252x16 -> 62x126x16\n",
        "                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0), #62x126x16 -> 60x124x32\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2),                                                    #60x124x32 -> 30x62x32 \n",
        "                nn.Flatten(),\n",
        "                nn.Linear(30*62*32, 1))\n",
        "\n",
        "     \n",
        "    def forward(self, X): # you can add any additional parameters you want\n",
        "    # TODO:\n",
        "    # Forward propagation implementation should be here\n",
        "        return self.network(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioU02PmPMmlA",
        "colab_type": "text"
      },
      "source": [
        "#### Training and Testing [17 pts]\n",
        "\n",
        "Now, train your network. You need to select the appropriate loss function and your hyper-parameters.<br>\n",
        "Make sure to shuffle the samples in the training split.<br>\n",
        " Plot the training and validation loss for each iteration. Also plot the training  and validation accuracy as another figure.<br>\n",
        "  Your model is going to run upto the \"max_epoch\" parameter. Pick the best model as your final model and save this model as a \".pth\" file. Note that the best accuracy does not always imply the best model. Try to track losses instead of accuracies. <br>\n",
        "  Report the test performance change (In terms of accuracy, F1 score, precision and recall) between MLP and CNN and explain the reason for this change explicitly, if there is any."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx7wC65h5oWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HINT: note that your training time should not take many days.\n",
        "\n",
        "#TODO:\n",
        "#Pick your hyper parameters\n",
        "train_batch = 256\n",
        "val_batch = 128\n",
        "test_batch = 128\n",
        "max_epoch = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "#use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_acc = []\n",
        "train_loss = []\n",
        "val_acc = []\n",
        "val_loss = []\n",
        "best_path = 'CNN.pth'\n",
        "\n",
        "def main(): # you are free to change parameters\n",
        "\n",
        "    # Create train dataset loader\n",
        "    train_loader = DataLoader(train_set_cnn, batch_size=train_batch, shuffle=True)\n",
        "    # Create validation dataset loader\n",
        "    val_loader = DataLoader(val_set_cnn, batch_size=val_batch)\n",
        "    # Create test dataset loader\n",
        "    test_loader = DataLoader(test_set_cnn, test_batch)\n",
        "    # initialize your GENet neural network\n",
        "    model = ConvNet()\n",
        "    model.to(device)\n",
        "\n",
        "    # define your loss function\n",
        "    loss = nn.BCEWithLogitsLoss().to(device)\n",
        "    \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
        "    # start training\n",
        "    # for each epoch calculate validation performance\n",
        "    # save best model according to validation performance\n",
        "    best_loss = 10\n",
        "\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "       train_a, train_l = train(epoch, model, loss, optimizer, train_loader)\n",
        "       train_acc.append(train_a)\n",
        "       train_loss.append(train_l)\n",
        "       val_a, val_l = test(model, val_loader, loss)\n",
        "       val_acc.append(val_a)\n",
        "       val_loss.append(val_l)\n",
        "       if val_l < best_loss: #minimize loss\n",
        "          torch.save(model.state_dict(), best_path)\n",
        "          best_loss = val_l\n",
        "    print(best_loss)\n",
        "    \n",
        "''' Train your network for a one epoch '''\n",
        "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    end = time.time()\n",
        "    for batch_idx, (data, labels) in enumerate(loader):\n",
        "        # TODO:\n",
        "        # Implement training code for a one iteration\n",
        "        data_time.update(time.time() - end)\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        data = data.permute(0,3,1,2)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data.float())\n",
        "        # output = torch.round(output)\n",
        "        # output = torch.flatten(output)\n",
        "\n",
        "        loss = criterion(output, labels.unsqueeze(1))\n",
        "        \n",
        "        output = torch.round(torch.sigmoid(output))\n",
        "        output = torch.flatten(output)\n",
        "        prec = accuracy(output, labels)\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        accuracies.update(prec, data.size(0))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "            'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
        "            epoch + 1, batch_idx + 1, len(loader), \n",
        "            batch_time=batch_time,\n",
        "            data_time=data_time, \n",
        "            loss=losses,\n",
        "            acc=accuracies))\n",
        "        \n",
        "    return accuracies.avg, losses.avg\n",
        "        \n",
        "\n",
        "''' Test&Validate your network '''\n",
        "def test(model, loader, criterion): # you are free to change parameters\n",
        "    batch_time = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        end = time.time()\n",
        "        for batch_idx, (data, labels) in enumerate(loader):\n",
        "            # TODO:\n",
        "            # Implement test code\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            data = data.permute(0,3,1,2)\n",
        "            output = model(data.float())\n",
        "            # print(output[:10])\n",
        "            # output = torch.round(output)\n",
        "            # output = torch.flatten(output)\n",
        "            # print(output[:10])\n",
        "\n",
        "            loss = criterion(output, labels.unsqueeze(1)) \n",
        "            output = torch.round(torch.sigmoid(output))\n",
        "            output = torch.flatten(output)  \n",
        "            prec = accuracy(output, labels)\n",
        "            losses.update(loss.item(), data.size(0))\n",
        "            accuracies.update(prec, data.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            \n",
        "        print('Time {batch_time.avg:.3f}\\t'\n",
        "              'Accu {acc.avg:.4f}\\t'\n",
        "              'Loss {loss.avg:.4f}\\t'.format(\n",
        "               batch_time=batch_time, \n",
        "               acc=accuracies,\n",
        "               loss=losses))\n",
        "        \n",
        "    return accuracies.avg, losses.avg\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2O0_fab-kMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a776534-4735-4f95-96f1-a8d997d7c3da"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1][1/10]\tTime 0.208 (0.208)\tData 0.0502 (0.0502)\tLoss 0.6970 (0.6970)\tAccu 0.2891 (0.2891)\t\n",
            "Epoch: [1][2/10]\tTime 0.302 (0.255)\tData 0.0487 (0.0495)\tLoss 0.6530 (0.6750)\tAccu 0.6406 (0.4648)\t\n",
            "Epoch: [1][3/10]\tTime 0.248 (0.252)\tData 0.0496 (0.0495)\tLoss 0.6337 (0.6612)\tAccu 0.6797 (0.5365)\t\n",
            "Epoch: [1][4/10]\tTime 0.249 (0.252)\tData 0.0486 (0.0493)\tLoss 0.6795 (0.6658)\tAccu 0.6680 (0.5693)\t\n",
            "Epoch: [1][5/10]\tTime 0.248 (0.251)\tData 0.0483 (0.0491)\tLoss 0.6325 (0.6591)\tAccu 0.6953 (0.5945)\t\n",
            "Epoch: [1][6/10]\tTime 0.249 (0.250)\tData 0.0491 (0.0491)\tLoss 0.6039 (0.6499)\tAccu 0.7070 (0.6133)\t\n",
            "Epoch: [1][7/10]\tTime 0.248 (0.250)\tData 0.0513 (0.0494)\tLoss 0.6342 (0.6477)\tAccu 0.6680 (0.6211)\t\n",
            "Epoch: [1][8/10]\tTime 0.249 (0.250)\tData 0.0494 (0.0494)\tLoss 0.6470 (0.6476)\tAccu 0.6523 (0.6250)\t\n",
            "Epoch: [1][9/10]\tTime 0.250 (0.250)\tData 0.0490 (0.0494)\tLoss 0.6526 (0.6481)\tAccu 0.6523 (0.6280)\t\n",
            "Epoch: [1][10/10]\tTime 0.224 (0.247)\tData 0.0389 (0.0483)\tLoss 0.6307 (0.6468)\tAccu 0.7194 (0.6352)\t\n",
            "Time 0.094\tAccu 0.6520\tLoss 0.6494\t\n",
            "Epoch: [2][1/10]\tTime 0.150 (0.150)\tData 0.0466 (0.0466)\tLoss 0.6594 (0.6594)\tAccu 0.6289 (0.6289)\t\n",
            "Epoch: [2][2/10]\tTime 0.249 (0.200)\tData 0.0484 (0.0475)\tLoss 0.6532 (0.6563)\tAccu 0.6367 (0.6328)\t\n",
            "Epoch: [2][3/10]\tTime 0.250 (0.216)\tData 0.0480 (0.0477)\tLoss 0.6177 (0.6434)\tAccu 0.6953 (0.6536)\t\n",
            "Epoch: [2][4/10]\tTime 0.249 (0.224)\tData 0.0497 (0.0482)\tLoss 0.5918 (0.6305)\tAccu 0.7227 (0.6709)\t\n",
            "Epoch: [2][5/10]\tTime 0.248 (0.229)\tData 0.0489 (0.0483)\tLoss 0.6284 (0.6301)\tAccu 0.6797 (0.6727)\t\n",
            "Epoch: [2][6/10]\tTime 0.249 (0.232)\tData 0.0497 (0.0485)\tLoss 0.6528 (0.6339)\tAccu 0.6602 (0.6706)\t\n",
            "Epoch: [2][7/10]\tTime 0.251 (0.235)\tData 0.0491 (0.0486)\tLoss 0.5963 (0.6285)\tAccu 0.7148 (0.6769)\t\n",
            "Epoch: [2][8/10]\tTime 0.250 (0.237)\tData 0.0503 (0.0488)\tLoss 0.6336 (0.6291)\tAccu 0.6797 (0.6772)\t\n",
            "Epoch: [2][9/10]\tTime 0.249 (0.238)\tData 0.0486 (0.0488)\tLoss 0.6162 (0.6277)\tAccu 0.6914 (0.6788)\t\n",
            "Epoch: [2][10/10]\tTime 0.225 (0.237)\tData 0.0381 (0.0477)\tLoss 0.6224 (0.6273)\tAccu 0.6837 (0.6792)\t\n",
            "Time 0.095\tAccu 0.6520\tLoss 0.6440\t\n",
            "Epoch: [3][1/10]\tTime 0.154 (0.154)\tData 0.0499 (0.0499)\tLoss 0.6059 (0.6059)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [3][2/10]\tTime 0.250 (0.202)\tData 0.0488 (0.0494)\tLoss 0.6520 (0.6289)\tAccu 0.6406 (0.6758)\t\n",
            "Epoch: [3][3/10]\tTime 0.251 (0.218)\tData 0.0485 (0.0491)\tLoss 0.6311 (0.6297)\tAccu 0.6719 (0.6745)\t\n",
            "Epoch: [3][4/10]\tTime 0.254 (0.227)\tData 0.0484 (0.0489)\tLoss 0.6158 (0.6262)\tAccu 0.6992 (0.6807)\t\n",
            "Epoch: [3][5/10]\tTime 0.251 (0.232)\tData 0.0485 (0.0488)\tLoss 0.6281 (0.6266)\tAccu 0.6758 (0.6797)\t\n",
            "Epoch: [3][6/10]\tTime 0.251 (0.235)\tData 0.0486 (0.0488)\tLoss 0.6034 (0.6227)\tAccu 0.7148 (0.6855)\t\n",
            "Epoch: [3][7/10]\tTime 0.250 (0.237)\tData 0.0486 (0.0488)\tLoss 0.6122 (0.6212)\tAccu 0.6914 (0.6864)\t\n",
            "Epoch: [3][8/10]\tTime 0.253 (0.239)\tData 0.0492 (0.0488)\tLoss 0.6163 (0.6206)\tAccu 0.6875 (0.6865)\t\n",
            "Epoch: [3][9/10]\tTime 0.252 (0.241)\tData 0.0488 (0.0488)\tLoss 0.6630 (0.6253)\tAccu 0.6406 (0.6814)\t\n",
            "Epoch: [3][10/10]\tTime 0.226 (0.239)\tData 0.0373 (0.0477)\tLoss 0.6467 (0.6270)\tAccu 0.6531 (0.6792)\t\n",
            "Time 0.095\tAccu 0.6520\tLoss 0.6456\t\n",
            "Epoch: [4][1/10]\tTime 0.154 (0.154)\tData 0.0478 (0.0478)\tLoss 0.6265 (0.6265)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [4][2/10]\tTime 0.257 (0.205)\tData 0.0486 (0.0482)\tLoss 0.6485 (0.6375)\tAccu 0.6367 (0.6582)\t\n",
            "Epoch: [4][3/10]\tTime 0.253 (0.221)\tData 0.0487 (0.0483)\tLoss 0.6416 (0.6389)\tAccu 0.6484 (0.6549)\t\n",
            "Epoch: [4][4/10]\tTime 0.250 (0.228)\tData 0.0486 (0.0484)\tLoss 0.6160 (0.6332)\tAccu 0.7070 (0.6680)\t\n",
            "Epoch: [4][5/10]\tTime 0.253 (0.233)\tData 0.0510 (0.0489)\tLoss 0.6212 (0.6308)\tAccu 0.6953 (0.6734)\t\n",
            "Epoch: [4][6/10]\tTime 0.255 (0.237)\tData 0.0499 (0.0491)\tLoss 0.6060 (0.6266)\tAccu 0.7227 (0.6816)\t\n",
            "Epoch: [4][7/10]\tTime 0.251 (0.239)\tData 0.0488 (0.0490)\tLoss 0.6153 (0.6250)\tAccu 0.6914 (0.6830)\t\n",
            "Epoch: [4][8/10]\tTime 0.251 (0.240)\tData 0.0485 (0.0490)\tLoss 0.5861 (0.6202)\tAccu 0.7188 (0.6875)\t\n",
            "Epoch: [4][9/10]\tTime 0.251 (0.242)\tData 0.0487 (0.0489)\tLoss 0.6753 (0.6263)\tAccu 0.6406 (0.6823)\t\n",
            "Epoch: [4][10/10]\tTime 0.227 (0.240)\tData 0.0379 (0.0478)\tLoss 0.6691 (0.6296)\tAccu 0.6429 (0.6792)\t\n",
            "Time 0.095\tAccu 0.6520\tLoss 0.6474\t\n",
            "Epoch: [5][1/10]\tTime 0.154 (0.154)\tData 0.0483 (0.0483)\tLoss 0.6177 (0.6177)\tAccu 0.6875 (0.6875)\t\n",
            "Epoch: [5][2/10]\tTime 0.252 (0.203)\tData 0.0500 (0.0492)\tLoss 0.6076 (0.6127)\tAccu 0.7031 (0.6953)\t\n",
            "Epoch: [5][3/10]\tTime 0.252 (0.219)\tData 0.0490 (0.0491)\tLoss 0.6019 (0.6091)\tAccu 0.7070 (0.6992)\t\n",
            "Epoch: [5][4/10]\tTime 0.251 (0.227)\tData 0.0486 (0.0490)\tLoss 0.6213 (0.6121)\tAccu 0.6797 (0.6943)\t\n",
            "Epoch: [5][5/10]\tTime 0.251 (0.232)\tData 0.0483 (0.0488)\tLoss 0.6115 (0.6120)\tAccu 0.6992 (0.6953)\t\n",
            "Epoch: [5][6/10]\tTime 0.251 (0.235)\tData 0.0486 (0.0488)\tLoss 0.6305 (0.6151)\tAccu 0.6641 (0.6901)\t\n",
            "Epoch: [5][7/10]\tTime 0.251 (0.237)\tData 0.0484 (0.0487)\tLoss 0.6014 (0.6131)\tAccu 0.7031 (0.6920)\t\n",
            "Epoch: [5][8/10]\tTime 0.252 (0.239)\tData 0.0489 (0.0488)\tLoss 0.6508 (0.6178)\tAccu 0.6523 (0.6870)\t\n",
            "Epoch: [5][9/10]\tTime 0.252 (0.241)\tData 0.0484 (0.0487)\tLoss 0.6806 (0.6248)\tAccu 0.6055 (0.6780)\t\n",
            "Epoch: [5][10/10]\tTime 0.226 (0.239)\tData 0.0383 (0.0477)\tLoss 0.6108 (0.6237)\tAccu 0.6939 (0.6792)\t\n",
            "Time 0.096\tAccu 0.6520\tLoss 0.6378\t\n",
            "Epoch: [6][1/10]\tTime 0.152 (0.152)\tData 0.0479 (0.0479)\tLoss 0.6070 (0.6070)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [6][2/10]\tTime 0.251 (0.202)\tData 0.0491 (0.0485)\tLoss 0.6425 (0.6248)\tAccu 0.6562 (0.6758)\t\n",
            "Epoch: [6][3/10]\tTime 0.252 (0.219)\tData 0.0497 (0.0489)\tLoss 0.6288 (0.6261)\tAccu 0.6719 (0.6745)\t\n",
            "Epoch: [6][4/10]\tTime 0.252 (0.227)\tData 0.0482 (0.0487)\tLoss 0.6517 (0.6325)\tAccu 0.6484 (0.6680)\t\n",
            "Epoch: [6][5/10]\tTime 0.250 (0.232)\tData 0.0500 (0.0490)\tLoss 0.6240 (0.6308)\tAccu 0.6719 (0.6687)\t\n",
            "Epoch: [6][6/10]\tTime 0.250 (0.235)\tData 0.0480 (0.0488)\tLoss 0.6455 (0.6333)\tAccu 0.6445 (0.6647)\t\n",
            "Epoch: [6][7/10]\tTime 0.252 (0.237)\tData 0.0481 (0.0487)\tLoss 0.5985 (0.6283)\tAccu 0.7070 (0.6708)\t\n",
            "Epoch: [6][8/10]\tTime 0.253 (0.239)\tData 0.0491 (0.0488)\tLoss 0.5989 (0.6246)\tAccu 0.7109 (0.6758)\t\n",
            "Epoch: [6][9/10]\tTime 0.255 (0.241)\tData 0.0504 (0.0489)\tLoss 0.6198 (0.6241)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [6][10/10]\tTime 0.230 (0.240)\tData 0.0369 (0.0477)\tLoss 0.5912 (0.6215)\tAccu 0.7194 (0.6792)\t\n",
            "Time 0.095\tAccu 0.6520\tLoss 0.6481\t\n",
            "Epoch: [7][1/10]\tTime 0.151 (0.151)\tData 0.0471 (0.0471)\tLoss 0.6017 (0.6017)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [7][2/10]\tTime 0.253 (0.202)\tData 0.0545 (0.0508)\tLoss 0.6556 (0.6287)\tAccu 0.6719 (0.6855)\t\n",
            "Epoch: [7][3/10]\tTime 0.253 (0.219)\tData 0.0488 (0.0501)\tLoss 0.6281 (0.6285)\tAccu 0.6719 (0.6810)\t\n",
            "Epoch: [7][4/10]\tTime 0.253 (0.227)\tData 0.0483 (0.0497)\tLoss 0.5779 (0.6158)\tAccu 0.7383 (0.6953)\t\n",
            "Epoch: [7][5/10]\tTime 0.252 (0.232)\tData 0.0494 (0.0496)\tLoss 0.6076 (0.6142)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [7][6/10]\tTime 0.253 (0.236)\tData 0.0518 (0.0500)\tLoss 0.6389 (0.6183)\tAccu 0.6562 (0.6888)\t\n",
            "Epoch: [7][7/10]\tTime 0.253 (0.238)\tData 0.0485 (0.0498)\tLoss 0.6218 (0.6188)\tAccu 0.6602 (0.6847)\t\n",
            "Epoch: [7][8/10]\tTime 0.253 (0.240)\tData 0.0485 (0.0496)\tLoss 0.6174 (0.6186)\tAccu 0.6797 (0.6841)\t\n",
            "Epoch: [7][9/10]\tTime 0.253 (0.241)\tData 0.0497 (0.0496)\tLoss 0.6568 (0.6229)\tAccu 0.6328 (0.6784)\t\n",
            "Epoch: [7][10/10]\tTime 0.229 (0.240)\tData 0.0376 (0.0484)\tLoss 0.6143 (0.6222)\tAccu 0.6888 (0.6792)\t\n",
            "Time 0.095\tAccu 0.6520\tLoss 0.6357\t\n",
            "Epoch: [8][1/10]\tTime 0.153 (0.153)\tData 0.0467 (0.0467)\tLoss 0.5969 (0.5969)\tAccu 0.7227 (0.7227)\t\n",
            "Epoch: [8][2/10]\tTime 0.253 (0.203)\tData 0.0486 (0.0477)\tLoss 0.6333 (0.6151)\tAccu 0.6523 (0.6875)\t\n",
            "Epoch: [8][3/10]\tTime 0.254 (0.220)\tData 0.0484 (0.0479)\tLoss 0.6766 (0.6356)\tAccu 0.6016 (0.6589)\t\n",
            "Epoch: [8][4/10]\tTime 0.251 (0.228)\tData 0.0487 (0.0481)\tLoss 0.6034 (0.6275)\tAccu 0.7070 (0.6709)\t\n",
            "Epoch: [8][5/10]\tTime 0.255 (0.233)\tData 0.0490 (0.0483)\tLoss 0.6152 (0.6251)\tAccu 0.6836 (0.6734)\t\n",
            "Epoch: [8][6/10]\tTime 0.253 (0.236)\tData 0.0488 (0.0484)\tLoss 0.6387 (0.6274)\tAccu 0.6641 (0.6719)\t\n",
            "Epoch: [8][7/10]\tTime 0.255 (0.239)\tData 0.0487 (0.0484)\tLoss 0.6263 (0.6272)\tAccu 0.6641 (0.6708)\t\n",
            "Epoch: [8][8/10]\tTime 0.254 (0.241)\tData 0.0488 (0.0485)\tLoss 0.6131 (0.6254)\tAccu 0.6875 (0.6729)\t\n",
            "Epoch: [8][9/10]\tTime 0.254 (0.242)\tData 0.0491 (0.0485)\tLoss 0.6019 (0.6228)\tAccu 0.7070 (0.6766)\t\n",
            "Epoch: [8][10/10]\tTime 0.228 (0.241)\tData 0.0377 (0.0475)\tLoss 0.5983 (0.6209)\tAccu 0.7092 (0.6792)\t\n",
            "Time 0.096\tAccu 0.6520\tLoss 0.6379\t\n",
            "Epoch: [9][1/10]\tTime 0.153 (0.153)\tData 0.0475 (0.0475)\tLoss 0.6375 (0.6375)\tAccu 0.6484 (0.6484)\t\n",
            "Epoch: [9][2/10]\tTime 0.255 (0.204)\tData 0.0514 (0.0494)\tLoss 0.6375 (0.6375)\tAccu 0.6719 (0.6602)\t\n",
            "Epoch: [9][3/10]\tTime 0.254 (0.220)\tData 0.0487 (0.0492)\tLoss 0.6453 (0.6401)\tAccu 0.6445 (0.6549)\t\n",
            "Epoch: [9][4/10]\tTime 0.253 (0.229)\tData 0.0495 (0.0493)\tLoss 0.6175 (0.6345)\tAccu 0.6875 (0.6631)\t\n",
            "Epoch: [9][5/10]\tTime 0.255 (0.234)\tData 0.0488 (0.0492)\tLoss 0.6264 (0.6328)\tAccu 0.6641 (0.6633)\t\n",
            "Epoch: [9][6/10]\tTime 0.255 (0.238)\tData 0.0488 (0.0491)\tLoss 0.6174 (0.6303)\tAccu 0.6836 (0.6667)\t\n",
            "Epoch: [9][7/10]\tTime 0.255 (0.240)\tData 0.0495 (0.0492)\tLoss 0.6197 (0.6288)\tAccu 0.6836 (0.6691)\t\n",
            "Epoch: [9][8/10]\tTime 0.256 (0.242)\tData 0.0487 (0.0491)\tLoss 0.5927 (0.6243)\tAccu 0.7227 (0.6758)\t\n",
            "Epoch: [9][9/10]\tTime 0.254 (0.243)\tData 0.0491 (0.0491)\tLoss 0.6139 (0.6231)\tAccu 0.6836 (0.6766)\t\n",
            "Epoch: [9][10/10]\tTime 0.232 (0.242)\tData 0.0377 (0.0480)\tLoss 0.5874 (0.6203)\tAccu 0.7092 (0.6792)\t\n",
            "Time 0.096\tAccu 0.6520\tLoss 0.6468\t\n",
            "Epoch: [10][1/10]\tTime 0.153 (0.153)\tData 0.0478 (0.0478)\tLoss 0.6947 (0.6947)\tAccu 0.6055 (0.6055)\t\n",
            "Epoch: [10][2/10]\tTime 0.256 (0.204)\tData 0.0488 (0.0483)\tLoss 0.6506 (0.6726)\tAccu 0.6562 (0.6309)\t\n",
            "Epoch: [10][3/10]\tTime 0.257 (0.222)\tData 0.0488 (0.0484)\tLoss 0.6234 (0.6562)\tAccu 0.6680 (0.6432)\t\n",
            "Epoch: [10][4/10]\tTime 0.256 (0.230)\tData 0.0483 (0.0484)\tLoss 0.6153 (0.6460)\tAccu 0.6875 (0.6543)\t\n",
            "Epoch: [10][5/10]\tTime 0.255 (0.235)\tData 0.0501 (0.0488)\tLoss 0.6168 (0.6401)\tAccu 0.6797 (0.6594)\t\n",
            "Epoch: [10][6/10]\tTime 0.255 (0.239)\tData 0.0490 (0.0488)\tLoss 0.5861 (0.6311)\tAccu 0.7461 (0.6738)\t\n",
            "Epoch: [10][7/10]\tTime 0.256 (0.241)\tData 0.0480 (0.0487)\tLoss 0.6114 (0.6283)\tAccu 0.6992 (0.6775)\t\n",
            "Epoch: [10][8/10]\tTime 0.255 (0.243)\tData 0.0479 (0.0486)\tLoss 0.6167 (0.6269)\tAccu 0.6875 (0.6787)\t\n",
            "Epoch: [10][9/10]\tTime 0.254 (0.244)\tData 0.0489 (0.0486)\tLoss 0.6109 (0.6251)\tAccu 0.6914 (0.6801)\t\n",
            "Epoch: [10][10/10]\tTime 0.230 (0.243)\tData 0.0375 (0.0475)\tLoss 0.6263 (0.6252)\tAccu 0.6684 (0.6792)\t\n",
            "Time 0.096\tAccu 0.6520\tLoss 0.6492\t\n",
            "Epoch: [11][1/10]\tTime 0.151 (0.151)\tData 0.0465 (0.0465)\tLoss 0.6145 (0.6145)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [11][2/10]\tTime 0.254 (0.203)\tData 0.0480 (0.0473)\tLoss 0.5952 (0.6048)\tAccu 0.7070 (0.6992)\t\n",
            "Epoch: [11][3/10]\tTime 0.255 (0.220)\tData 0.0500 (0.0482)\tLoss 0.6337 (0.6145)\tAccu 0.6602 (0.6862)\t\n",
            "Epoch: [11][4/10]\tTime 0.256 (0.229)\tData 0.0485 (0.0483)\tLoss 0.6191 (0.6156)\tAccu 0.6797 (0.6846)\t\n",
            "Epoch: [11][5/10]\tTime 0.254 (0.234)\tData 0.0487 (0.0483)\tLoss 0.6322 (0.6189)\tAccu 0.6641 (0.6805)\t\n",
            "Epoch: [11][6/10]\tTime 0.258 (0.238)\tData 0.0479 (0.0483)\tLoss 0.5932 (0.6146)\tAccu 0.7188 (0.6868)\t\n",
            "Epoch: [11][7/10]\tTime 0.256 (0.241)\tData 0.0498 (0.0485)\tLoss 0.5949 (0.6118)\tAccu 0.7266 (0.6925)\t\n",
            "Epoch: [11][8/10]\tTime 0.257 (0.243)\tData 0.0496 (0.0486)\tLoss 0.6389 (0.6152)\tAccu 0.6562 (0.6880)\t\n",
            "Epoch: [11][9/10]\tTime 0.258 (0.244)\tData 0.0496 (0.0487)\tLoss 0.6380 (0.6177)\tAccu 0.6445 (0.6832)\t\n",
            "Epoch: [11][10/10]\tTime 0.232 (0.243)\tData 0.0385 (0.0477)\tLoss 0.6492 (0.6202)\tAccu 0.6327 (0.6792)\t\n",
            "Time 0.096\tAccu 0.6520\tLoss 0.6335\t\n",
            "Epoch: [12][1/10]\tTime 0.154 (0.154)\tData 0.0486 (0.0486)\tLoss 0.6383 (0.6383)\tAccu 0.6523 (0.6523)\t\n",
            "Epoch: [12][2/10]\tTime 0.257 (0.206)\tData 0.0490 (0.0488)\tLoss 0.6174 (0.6278)\tAccu 0.6797 (0.6660)\t\n",
            "Epoch: [12][3/10]\tTime 0.255 (0.222)\tData 0.0481 (0.0486)\tLoss 0.6440 (0.6332)\tAccu 0.6250 (0.6523)\t\n",
            "Epoch: [12][4/10]\tTime 0.257 (0.231)\tData 0.0487 (0.0486)\tLoss 0.6050 (0.6262)\tAccu 0.7031 (0.6650)\t\n",
            "Epoch: [12][5/10]\tTime 0.258 (0.236)\tData 0.0483 (0.0485)\tLoss 0.6249 (0.6259)\tAccu 0.6680 (0.6656)\t\n",
            "Epoch: [12][6/10]\tTime 0.255 (0.239)\tData 0.0482 (0.0485)\tLoss 0.6105 (0.6234)\tAccu 0.6875 (0.6693)\t\n",
            "Epoch: [12][7/10]\tTime 0.257 (0.242)\tData 0.0486 (0.0485)\tLoss 0.5858 (0.6180)\tAccu 0.7305 (0.6780)\t\n",
            "Epoch: [12][8/10]\tTime 0.257 (0.244)\tData 0.0490 (0.0486)\tLoss 0.6029 (0.6161)\tAccu 0.7148 (0.6826)\t\n",
            "Epoch: [12][9/10]\tTime 0.255 (0.245)\tData 0.0489 (0.0486)\tLoss 0.6112 (0.6156)\tAccu 0.6797 (0.6823)\t\n",
            "Epoch: [12][10/10]\tTime 0.232 (0.244)\tData 0.0379 (0.0475)\tLoss 0.6551 (0.6187)\tAccu 0.6429 (0.6792)\t\n",
            "Time 0.097\tAccu 0.6520\tLoss 0.6422\t\n",
            "Epoch: [13][1/10]\tTime 0.153 (0.153)\tData 0.0479 (0.0479)\tLoss 0.5496 (0.5496)\tAccu 0.7461 (0.7461)\t\n",
            "Epoch: [13][2/10]\tTime 0.257 (0.205)\tData 0.0486 (0.0482)\tLoss 0.6594 (0.6045)\tAccu 0.6328 (0.6895)\t\n",
            "Epoch: [13][3/10]\tTime 0.257 (0.222)\tData 0.0489 (0.0484)\tLoss 0.5952 (0.6014)\tAccu 0.7109 (0.6966)\t\n",
            "Epoch: [13][4/10]\tTime 0.257 (0.231)\tData 0.0486 (0.0485)\tLoss 0.6301 (0.6086)\tAccu 0.6523 (0.6855)\t\n",
            "Epoch: [13][5/10]\tTime 0.256 (0.236)\tData 0.0490 (0.0486)\tLoss 0.6254 (0.6119)\tAccu 0.6562 (0.6797)\t\n",
            "Epoch: [13][6/10]\tTime 0.258 (0.240)\tData 0.0494 (0.0487)\tLoss 0.5859 (0.6076)\tAccu 0.7148 (0.6855)\t\n",
            "Epoch: [13][7/10]\tTime 0.257 (0.242)\tData 0.0486 (0.0487)\tLoss 0.6251 (0.6101)\tAccu 0.6797 (0.6847)\t\n",
            "Epoch: [13][8/10]\tTime 0.256 (0.244)\tData 0.0484 (0.0487)\tLoss 0.6408 (0.6139)\tAccu 0.6562 (0.6812)\t\n",
            "Epoch: [13][9/10]\tTime 0.258 (0.245)\tData 0.0484 (0.0486)\tLoss 0.6377 (0.6166)\tAccu 0.6719 (0.6801)\t\n",
            "Epoch: [13][10/10]\tTime 0.234 (0.244)\tData 0.0376 (0.0475)\tLoss 0.6278 (0.6175)\tAccu 0.6684 (0.6792)\t\n",
            "Time 0.097\tAccu 0.6520\tLoss 0.6320\t\n",
            "Epoch: [14][1/10]\tTime 0.153 (0.153)\tData 0.0475 (0.0475)\tLoss 0.6079 (0.6079)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [14][2/10]\tTime 0.259 (0.206)\tData 0.0489 (0.0482)\tLoss 0.6437 (0.6258)\tAccu 0.6641 (0.6777)\t\n",
            "Epoch: [14][3/10]\tTime 0.261 (0.224)\tData 0.0494 (0.0486)\tLoss 0.5992 (0.6170)\tAccu 0.7031 (0.6862)\t\n",
            "Epoch: [14][4/10]\tTime 0.258 (0.233)\tData 0.0485 (0.0486)\tLoss 0.6394 (0.6226)\tAccu 0.6406 (0.6748)\t\n",
            "Epoch: [14][5/10]\tTime 0.259 (0.238)\tData 0.0502 (0.0489)\tLoss 0.5994 (0.6179)\tAccu 0.6953 (0.6789)\t\n",
            "Epoch: [14][6/10]\tTime 0.257 (0.241)\tData 0.0495 (0.0490)\tLoss 0.6281 (0.6196)\tAccu 0.6484 (0.6738)\t\n",
            "Epoch: [14][7/10]\tTime 0.259 (0.244)\tData 0.0492 (0.0490)\tLoss 0.6260 (0.6205)\tAccu 0.6758 (0.6741)\t\n",
            "Epoch: [14][8/10]\tTime 0.258 (0.245)\tData 0.0517 (0.0494)\tLoss 0.6100 (0.6192)\tAccu 0.6797 (0.6748)\t\n",
            "Epoch: [14][9/10]\tTime 0.259 (0.247)\tData 0.0529 (0.0498)\tLoss 0.5988 (0.6169)\tAccu 0.7109 (0.6788)\t\n",
            "Epoch: [14][10/10]\tTime 0.233 (0.246)\tData 0.0378 (0.0486)\tLoss 0.6086 (0.6163)\tAccu 0.6837 (0.6792)\t\n",
            "Time 0.096\tAccu 0.6520\tLoss 0.6335\t\n",
            "Epoch: [15][1/10]\tTime 0.154 (0.154)\tData 0.0475 (0.0475)\tLoss 0.6002 (0.6002)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [15][2/10]\tTime 0.260 (0.207)\tData 0.0523 (0.0499)\tLoss 0.6408 (0.6205)\tAccu 0.6445 (0.6719)\t\n",
            "Epoch: [15][3/10]\tTime 0.256 (0.223)\tData 0.0487 (0.0495)\tLoss 0.6380 (0.6263)\tAccu 0.6641 (0.6693)\t\n",
            "Epoch: [15][4/10]\tTime 0.260 (0.233)\tData 0.0490 (0.0494)\tLoss 0.6193 (0.6246)\tAccu 0.6836 (0.6729)\t\n",
            "Epoch: [15][5/10]\tTime 0.259 (0.238)\tData 0.0489 (0.0493)\tLoss 0.6002 (0.6197)\tAccu 0.7109 (0.6805)\t\n",
            "Epoch: [15][6/10]\tTime 0.259 (0.241)\tData 0.0531 (0.0499)\tLoss 0.6032 (0.6170)\tAccu 0.6875 (0.6816)\t\n",
            "Epoch: [15][7/10]\tTime 0.259 (0.244)\tData 0.0493 (0.0498)\tLoss 0.6107 (0.6161)\tAccu 0.6797 (0.6814)\t\n",
            "Epoch: [15][8/10]\tTime 0.258 (0.246)\tData 0.0488 (0.0497)\tLoss 0.6294 (0.6177)\tAccu 0.6523 (0.6777)\t\n",
            "Epoch: [15][9/10]\tTime 0.261 (0.247)\tData 0.0485 (0.0496)\tLoss 0.6112 (0.6170)\tAccu 0.6836 (0.6784)\t\n",
            "Epoch: [15][10/10]\tTime 0.234 (0.246)\tData 0.0424 (0.0489)\tLoss 0.6057 (0.6161)\tAccu 0.6888 (0.6792)\t\n",
            "Time 0.097\tAccu 0.6520\tLoss 0.6344\t\n",
            "Epoch: [16][1/10]\tTime 0.153 (0.153)\tData 0.0475 (0.0475)\tLoss 0.6302 (0.6302)\tAccu 0.6523 (0.6523)\t\n",
            "Epoch: [16][2/10]\tTime 0.259 (0.206)\tData 0.0500 (0.0487)\tLoss 0.5708 (0.6005)\tAccu 0.7383 (0.6953)\t\n",
            "Epoch: [16][3/10]\tTime 0.261 (0.224)\tData 0.0515 (0.0496)\tLoss 0.6137 (0.6049)\tAccu 0.6875 (0.6927)\t\n",
            "Epoch: [16][4/10]\tTime 0.261 (0.233)\tData 0.0486 (0.0494)\tLoss 0.6020 (0.6042)\tAccu 0.6719 (0.6875)\t\n",
            "Epoch: [16][5/10]\tTime 0.260 (0.239)\tData 0.0493 (0.0494)\tLoss 0.6052 (0.6044)\tAccu 0.6758 (0.6852)\t\n",
            "Epoch: [16][6/10]\tTime 0.261 (0.242)\tData 0.0487 (0.0492)\tLoss 0.6610 (0.6138)\tAccu 0.6211 (0.6745)\t\n",
            "Epoch: [16][7/10]\tTime 0.259 (0.245)\tData 0.0507 (0.0494)\tLoss 0.6235 (0.6152)\tAccu 0.6953 (0.6775)\t\n",
            "Epoch: [16][8/10]\tTime 0.258 (0.246)\tData 0.0490 (0.0494)\tLoss 0.6283 (0.6168)\tAccu 0.6758 (0.6772)\t\n",
            "Epoch: [16][9/10]\tTime 0.261 (0.248)\tData 0.0489 (0.0493)\tLoss 0.6155 (0.6167)\tAccu 0.6992 (0.6797)\t\n",
            "Epoch: [16][10/10]\tTime 0.236 (0.247)\tData 0.0376 (0.0482)\tLoss 0.6127 (0.6164)\tAccu 0.6786 (0.6796)\t\n",
            "Time 0.097\tAccu 0.6500\tLoss 0.6300\t\n",
            "Epoch: [17][1/10]\tTime 0.153 (0.153)\tData 0.0473 (0.0473)\tLoss 0.6485 (0.6485)\tAccu 0.6328 (0.6328)\t\n",
            "Epoch: [17][2/10]\tTime 0.261 (0.207)\tData 0.0482 (0.0477)\tLoss 0.6317 (0.6401)\tAccu 0.6484 (0.6406)\t\n",
            "Epoch: [17][3/10]\tTime 0.259 (0.224)\tData 0.0495 (0.0483)\tLoss 0.6128 (0.6310)\tAccu 0.6914 (0.6576)\t\n",
            "Epoch: [17][4/10]\tTime 0.260 (0.233)\tData 0.0493 (0.0486)\tLoss 0.6100 (0.6258)\tAccu 0.6914 (0.6660)\t\n",
            "Epoch: [17][5/10]\tTime 0.262 (0.239)\tData 0.0497 (0.0488)\tLoss 0.6480 (0.6302)\tAccu 0.6484 (0.6625)\t\n",
            "Epoch: [17][6/10]\tTime 0.261 (0.243)\tData 0.0484 (0.0487)\tLoss 0.6123 (0.6272)\tAccu 0.6758 (0.6647)\t\n",
            "Epoch: [17][7/10]\tTime 0.261 (0.245)\tData 0.0493 (0.0488)\tLoss 0.5740 (0.6196)\tAccu 0.7344 (0.6747)\t\n",
            "Epoch: [17][8/10]\tTime 0.260 (0.247)\tData 0.0493 (0.0489)\tLoss 0.5947 (0.6165)\tAccu 0.7109 (0.6792)\t\n",
            "Epoch: [17][9/10]\tTime 0.259 (0.248)\tData 0.0507 (0.0491)\tLoss 0.6008 (0.6148)\tAccu 0.6797 (0.6793)\t\n",
            "Epoch: [17][10/10]\tTime 0.235 (0.247)\tData 0.0374 (0.0479)\tLoss 0.6086 (0.6143)\tAccu 0.6786 (0.6792)\t\n",
            "Time 0.097\tAccu 0.6520\tLoss 0.6413\t\n",
            "Epoch: [18][1/10]\tTime 0.154 (0.154)\tData 0.0472 (0.0472)\tLoss 0.6387 (0.6387)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [18][2/10]\tTime 0.261 (0.208)\tData 0.0494 (0.0483)\tLoss 0.5801 (0.6094)\tAccu 0.7031 (0.6855)\t\n",
            "Epoch: [18][3/10]\tTime 0.263 (0.226)\tData 0.0488 (0.0485)\tLoss 0.6069 (0.6085)\tAccu 0.6875 (0.6862)\t\n",
            "Epoch: [18][4/10]\tTime 0.261 (0.235)\tData 0.0488 (0.0486)\tLoss 0.6129 (0.6096)\tAccu 0.6875 (0.6865)\t\n",
            "Epoch: [18][5/10]\tTime 0.260 (0.240)\tData 0.0489 (0.0486)\tLoss 0.6181 (0.6113)\tAccu 0.6641 (0.6820)\t\n",
            "Epoch: [18][6/10]\tTime 0.261 (0.243)\tData 0.0483 (0.0486)\tLoss 0.6123 (0.6115)\tAccu 0.6797 (0.6816)\t\n",
            "Epoch: [18][7/10]\tTime 0.262 (0.246)\tData 0.0484 (0.0486)\tLoss 0.6354 (0.6149)\tAccu 0.6602 (0.6786)\t\n",
            "Epoch: [18][8/10]\tTime 0.266 (0.248)\tData 0.0544 (0.0493)\tLoss 0.6061 (0.6138)\tAccu 0.6914 (0.6802)\t\n",
            "Epoch: [18][9/10]\tTime 0.262 (0.250)\tData 0.0488 (0.0492)\tLoss 0.6004 (0.6123)\tAccu 0.6953 (0.6819)\t\n",
            "Epoch: [18][10/10]\tTime 0.237 (0.249)\tData 0.0381 (0.0481)\tLoss 0.6293 (0.6136)\tAccu 0.6480 (0.6792)\t\n",
            "Time 0.097\tAccu 0.6500\tLoss 0.6339\t\n",
            "Epoch: [19][1/10]\tTime 0.158 (0.158)\tData 0.0510 (0.0510)\tLoss 0.6149 (0.6149)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [19][2/10]\tTime 0.265 (0.212)\tData 0.0494 (0.0502)\tLoss 0.6065 (0.6107)\tAccu 0.6914 (0.6836)\t\n",
            "Epoch: [19][3/10]\tTime 0.263 (0.229)\tData 0.0484 (0.0496)\tLoss 0.5880 (0.6031)\tAccu 0.7109 (0.6927)\t\n",
            "Epoch: [19][4/10]\tTime 0.260 (0.236)\tData 0.0490 (0.0495)\tLoss 0.6114 (0.6052)\tAccu 0.6758 (0.6885)\t\n",
            "Epoch: [19][5/10]\tTime 0.262 (0.242)\tData 0.0486 (0.0493)\tLoss 0.6138 (0.6069)\tAccu 0.6680 (0.6844)\t\n",
            "Epoch: [19][6/10]\tTime 0.264 (0.245)\tData 0.0487 (0.0492)\tLoss 0.6109 (0.6076)\tAccu 0.6641 (0.6810)\t\n",
            "Epoch: [19][7/10]\tTime 0.264 (0.248)\tData 0.0493 (0.0492)\tLoss 0.6310 (0.6109)\tAccu 0.6406 (0.6752)\t\n",
            "Epoch: [19][8/10]\tTime 0.262 (0.250)\tData 0.0486 (0.0491)\tLoss 0.6276 (0.6130)\tAccu 0.6953 (0.6777)\t\n",
            "Epoch: [19][9/10]\tTime 0.262 (0.251)\tData 0.0488 (0.0491)\tLoss 0.6093 (0.6126)\tAccu 0.6836 (0.6784)\t\n",
            "Epoch: [19][10/10]\tTime 0.236 (0.250)\tData 0.0376 (0.0479)\tLoss 0.6189 (0.6131)\tAccu 0.6735 (0.6780)\t\n",
            "Time 0.097\tAccu 0.6500\tLoss 0.6286\t\n",
            "Epoch: [20][1/10]\tTime 0.154 (0.154)\tData 0.0481 (0.0481)\tLoss 0.6328 (0.6328)\tAccu 0.6523 (0.6523)\t\n",
            "Epoch: [20][2/10]\tTime 0.264 (0.209)\tData 0.0485 (0.0483)\tLoss 0.6154 (0.6241)\tAccu 0.6602 (0.6562)\t\n",
            "Epoch: [20][3/10]\tTime 0.263 (0.227)\tData 0.0495 (0.0487)\tLoss 0.5964 (0.6149)\tAccu 0.7031 (0.6719)\t\n",
            "Epoch: [20][4/10]\tTime 0.265 (0.236)\tData 0.0488 (0.0487)\tLoss 0.5985 (0.6108)\tAccu 0.7031 (0.6797)\t\n",
            "Epoch: [20][5/10]\tTime 0.263 (0.242)\tData 0.0492 (0.0488)\tLoss 0.6317 (0.6150)\tAccu 0.6758 (0.6789)\t\n",
            "Epoch: [20][6/10]\tTime 0.264 (0.245)\tData 0.0501 (0.0490)\tLoss 0.6226 (0.6162)\tAccu 0.6836 (0.6797)\t\n",
            "Epoch: [20][7/10]\tTime 0.264 (0.248)\tData 0.0509 (0.0493)\tLoss 0.6134 (0.6158)\tAccu 0.6602 (0.6769)\t\n",
            "Epoch: [20][8/10]\tTime 0.264 (0.250)\tData 0.0488 (0.0492)\tLoss 0.5945 (0.6132)\tAccu 0.6836 (0.6777)\t\n",
            "Epoch: [20][9/10]\tTime 0.263 (0.251)\tData 0.0496 (0.0493)\tLoss 0.6099 (0.6128)\tAccu 0.6875 (0.6788)\t\n",
            "Epoch: [20][10/10]\tTime 0.238 (0.250)\tData 0.0401 (0.0483)\tLoss 0.6031 (0.6120)\tAccu 0.6837 (0.6792)\t\n",
            "Time 0.098\tAccu 0.6500\tLoss 0.6300\t\n",
            "Epoch: [21][1/10]\tTime 0.156 (0.156)\tData 0.0485 (0.0485)\tLoss 0.6183 (0.6183)\tAccu 0.6562 (0.6562)\t\n",
            "Epoch: [21][2/10]\tTime 0.266 (0.211)\tData 0.0489 (0.0487)\tLoss 0.5803 (0.5993)\tAccu 0.7188 (0.6875)\t\n",
            "Epoch: [21][3/10]\tTime 0.265 (0.229)\tData 0.0498 (0.0491)\tLoss 0.5859 (0.5948)\tAccu 0.7109 (0.6953)\t\n",
            "Epoch: [21][4/10]\tTime 0.265 (0.238)\tData 0.0486 (0.0490)\tLoss 0.6898 (0.6186)\tAccu 0.6094 (0.6738)\t\n",
            "Epoch: [21][5/10]\tTime 0.264 (0.243)\tData 0.0483 (0.0488)\tLoss 0.6542 (0.6257)\tAccu 0.6484 (0.6687)\t\n",
            "Epoch: [21][6/10]\tTime 0.264 (0.247)\tData 0.0486 (0.0488)\tLoss 0.5961 (0.6208)\tAccu 0.7109 (0.6758)\t\n",
            "Epoch: [21][7/10]\tTime 0.263 (0.249)\tData 0.0491 (0.0488)\tLoss 0.6076 (0.6189)\tAccu 0.6953 (0.6786)\t\n",
            "Epoch: [21][8/10]\tTime 0.263 (0.251)\tData 0.0485 (0.0488)\tLoss 0.6245 (0.6196)\tAccu 0.6758 (0.6782)\t\n",
            "Epoch: [21][9/10]\tTime 0.264 (0.252)\tData 0.0486 (0.0488)\tLoss 0.6011 (0.6175)\tAccu 0.7031 (0.6810)\t\n",
            "Epoch: [21][10/10]\tTime 0.238 (0.251)\tData 0.0383 (0.0477)\tLoss 0.6066 (0.6167)\tAccu 0.6684 (0.6800)\t\n",
            "Time 0.098\tAccu 0.6520\tLoss 0.6403\t\n",
            "Epoch: [22][1/10]\tTime 0.154 (0.154)\tData 0.0472 (0.0472)\tLoss 0.6010 (0.6010)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [22][2/10]\tTime 0.265 (0.210)\tData 0.0490 (0.0481)\tLoss 0.6435 (0.6222)\tAccu 0.6758 (0.6836)\t\n",
            "Epoch: [22][3/10]\tTime 0.271 (0.230)\tData 0.0486 (0.0483)\tLoss 0.5936 (0.6127)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [22][4/10]\tTime 0.266 (0.239)\tData 0.0494 (0.0486)\tLoss 0.5899 (0.6070)\tAccu 0.6953 (0.6865)\t\n",
            "Epoch: [22][5/10]\tTime 0.265 (0.244)\tData 0.0486 (0.0486)\tLoss 0.5870 (0.6030)\tAccu 0.7148 (0.6922)\t\n",
            "Epoch: [22][6/10]\tTime 0.264 (0.248)\tData 0.0491 (0.0487)\tLoss 0.6085 (0.6039)\tAccu 0.6836 (0.6908)\t\n",
            "Epoch: [22][7/10]\tTime 0.265 (0.250)\tData 0.0485 (0.0486)\tLoss 0.6536 (0.6110)\tAccu 0.6367 (0.6830)\t\n",
            "Epoch: [22][8/10]\tTime 0.264 (0.252)\tData 0.0518 (0.0490)\tLoss 0.6175 (0.6118)\tAccu 0.6719 (0.6816)\t\n",
            "Epoch: [22][9/10]\tTime 0.264 (0.253)\tData 0.0489 (0.0490)\tLoss 0.6342 (0.6143)\tAccu 0.6484 (0.6780)\t\n",
            "Epoch: [22][10/10]\tTime 0.240 (0.252)\tData 0.0378 (0.0479)\tLoss 0.5850 (0.6120)\tAccu 0.7092 (0.6804)\t\n",
            "Time 0.099\tAccu 0.6500\tLoss 0.6287\t\n",
            "Epoch: [23][1/10]\tTime 0.156 (0.156)\tData 0.0480 (0.0480)\tLoss 0.6282 (0.6282)\tAccu 0.6328 (0.6328)\t\n",
            "Epoch: [23][2/10]\tTime 0.266 (0.211)\tData 0.0493 (0.0486)\tLoss 0.6028 (0.6155)\tAccu 0.6875 (0.6602)\t\n",
            "Epoch: [23][3/10]\tTime 0.265 (0.229)\tData 0.0490 (0.0488)\tLoss 0.5890 (0.6067)\tAccu 0.7070 (0.6758)\t\n",
            "Epoch: [23][4/10]\tTime 0.266 (0.238)\tData 0.0483 (0.0487)\tLoss 0.6050 (0.6062)\tAccu 0.6836 (0.6777)\t\n",
            "Epoch: [23][5/10]\tTime 0.265 (0.244)\tData 0.0486 (0.0486)\tLoss 0.6272 (0.6104)\tAccu 0.6953 (0.6813)\t\n",
            "Epoch: [23][6/10]\tTime 0.265 (0.247)\tData 0.0490 (0.0487)\tLoss 0.6404 (0.6154)\tAccu 0.6484 (0.6758)\t\n",
            "Epoch: [23][7/10]\tTime 0.265 (0.250)\tData 0.0486 (0.0487)\tLoss 0.5894 (0.6117)\tAccu 0.7070 (0.6802)\t\n",
            "Epoch: [23][8/10]\tTime 0.266 (0.252)\tData 0.0536 (0.0493)\tLoss 0.5947 (0.6096)\tAccu 0.6836 (0.6807)\t\n",
            "Epoch: [23][9/10]\tTime 0.266 (0.253)\tData 0.0485 (0.0492)\tLoss 0.5975 (0.6082)\tAccu 0.6836 (0.6810)\t\n",
            "Epoch: [23][10/10]\tTime 0.242 (0.252)\tData 0.0375 (0.0480)\tLoss 0.6363 (0.6104)\tAccu 0.6582 (0.6792)\t\n",
            "Time 0.099\tAccu 0.6480\tLoss 0.6288\t\n",
            "Epoch: [24][1/10]\tTime 0.155 (0.155)\tData 0.0479 (0.0479)\tLoss 0.6040 (0.6040)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [24][2/10]\tTime 0.267 (0.211)\tData 0.0496 (0.0488)\tLoss 0.6102 (0.6071)\tAccu 0.6758 (0.6836)\t\n",
            "Epoch: [24][3/10]\tTime 0.266 (0.229)\tData 0.0495 (0.0490)\tLoss 0.5976 (0.6040)\tAccu 0.6953 (0.6875)\t\n",
            "Epoch: [24][4/10]\tTime 0.267 (0.239)\tData 0.0498 (0.0492)\tLoss 0.5915 (0.6008)\tAccu 0.6992 (0.6904)\t\n",
            "Epoch: [24][5/10]\tTime 0.268 (0.244)\tData 0.0500 (0.0494)\tLoss 0.6704 (0.6147)\tAccu 0.6172 (0.6758)\t\n",
            "Epoch: [24][6/10]\tTime 0.265 (0.248)\tData 0.0510 (0.0496)\tLoss 0.5793 (0.6088)\tAccu 0.7109 (0.6816)\t\n",
            "Epoch: [24][7/10]\tTime 0.265 (0.250)\tData 0.0489 (0.0495)\tLoss 0.6320 (0.6121)\tAccu 0.6562 (0.6780)\t\n",
            "Epoch: [24][8/10]\tTime 0.272 (0.253)\tData 0.0490 (0.0495)\tLoss 0.6178 (0.6129)\tAccu 0.6641 (0.6763)\t\n",
            "Epoch: [24][9/10]\tTime 0.268 (0.255)\tData 0.0505 (0.0496)\tLoss 0.6070 (0.6122)\tAccu 0.6992 (0.6788)\t\n",
            "Epoch: [24][10/10]\tTime 0.242 (0.254)\tData 0.0381 (0.0484)\tLoss 0.5960 (0.6109)\tAccu 0.7041 (0.6808)\t\n",
            "Time 0.099\tAccu 0.6480\tLoss 0.6314\t\n",
            "Epoch: [25][1/10]\tTime 0.161 (0.161)\tData 0.0518 (0.0518)\tLoss 0.6142 (0.6142)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [25][2/10]\tTime 0.268 (0.215)\tData 0.0511 (0.0514)\tLoss 0.6127 (0.6134)\tAccu 0.6641 (0.6621)\t\n",
            "Epoch: [25][3/10]\tTime 0.267 (0.232)\tData 0.0493 (0.0507)\tLoss 0.6288 (0.6186)\tAccu 0.6680 (0.6641)\t\n",
            "Epoch: [25][4/10]\tTime 0.269 (0.241)\tData 0.0492 (0.0503)\tLoss 0.6050 (0.6152)\tAccu 0.6992 (0.6729)\t\n",
            "Epoch: [25][5/10]\tTime 0.268 (0.247)\tData 0.0489 (0.0500)\tLoss 0.6183 (0.6158)\tAccu 0.6562 (0.6695)\t\n",
            "Epoch: [25][6/10]\tTime 0.270 (0.251)\tData 0.0492 (0.0499)\tLoss 0.6051 (0.6140)\tAccu 0.6797 (0.6712)\t\n",
            "Epoch: [25][7/10]\tTime 0.271 (0.253)\tData 0.0489 (0.0498)\tLoss 0.5916 (0.6108)\tAccu 0.7070 (0.6763)\t\n",
            "Epoch: [25][8/10]\tTime 0.266 (0.255)\tData 0.0486 (0.0496)\tLoss 0.6247 (0.6125)\tAccu 0.6797 (0.6768)\t\n",
            "Epoch: [25][9/10]\tTime 0.271 (0.257)\tData 0.0535 (0.0500)\tLoss 0.6393 (0.6155)\tAccu 0.6445 (0.6732)\t\n",
            "Epoch: [25][10/10]\tTime 0.240 (0.255)\tData 0.0383 (0.0489)\tLoss 0.5448 (0.6100)\tAccu 0.7704 (0.6808)\t\n",
            "Time 0.100\tAccu 0.6480\tLoss 0.6355\t\n",
            "Epoch: [26][1/10]\tTime 0.154 (0.154)\tData 0.0480 (0.0480)\tLoss 0.6203 (0.6203)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [26][2/10]\tTime 0.271 (0.212)\tData 0.0558 (0.0519)\tLoss 0.5867 (0.6035)\tAccu 0.7070 (0.6836)\t\n",
            "Epoch: [26][3/10]\tTime 0.268 (0.231)\tData 0.0504 (0.0514)\tLoss 0.6088 (0.6053)\tAccu 0.6914 (0.6862)\t\n",
            "Epoch: [26][4/10]\tTime 0.268 (0.240)\tData 0.0489 (0.0508)\tLoss 0.6027 (0.6046)\tAccu 0.6914 (0.6875)\t\n",
            "Epoch: [26][5/10]\tTime 0.268 (0.245)\tData 0.0492 (0.0505)\tLoss 0.6156 (0.6068)\tAccu 0.6953 (0.6891)\t\n",
            "Epoch: [26][6/10]\tTime 0.268 (0.249)\tData 0.0496 (0.0503)\tLoss 0.5975 (0.6053)\tAccu 0.6914 (0.6895)\t\n",
            "Epoch: [26][7/10]\tTime 0.268 (0.252)\tData 0.0503 (0.0503)\tLoss 0.6261 (0.6083)\tAccu 0.6602 (0.6853)\t\n",
            "Epoch: [26][8/10]\tTime 0.268 (0.254)\tData 0.0501 (0.0503)\tLoss 0.6013 (0.6074)\tAccu 0.6836 (0.6851)\t\n",
            "Epoch: [26][9/10]\tTime 0.272 (0.256)\tData 0.0489 (0.0501)\tLoss 0.6179 (0.6086)\tAccu 0.6562 (0.6819)\t\n",
            "Epoch: [26][10/10]\tTime 0.245 (0.255)\tData 0.0382 (0.0489)\tLoss 0.6128 (0.6089)\tAccu 0.6633 (0.6804)\t\n",
            "Time 0.099\tAccu 0.6500\tLoss 0.6314\t\n",
            "Epoch: [27][1/10]\tTime 0.155 (0.155)\tData 0.0473 (0.0473)\tLoss 0.6026 (0.6026)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [27][2/10]\tTime 0.271 (0.213)\tData 0.0499 (0.0486)\tLoss 0.6137 (0.6081)\tAccu 0.6484 (0.6719)\t\n",
            "Epoch: [27][3/10]\tTime 0.267 (0.231)\tData 0.0500 (0.0491)\tLoss 0.5928 (0.6030)\tAccu 0.7109 (0.6849)\t\n",
            "Epoch: [27][4/10]\tTime 0.269 (0.241)\tData 0.0498 (0.0492)\tLoss 0.6122 (0.6053)\tAccu 0.6875 (0.6855)\t\n",
            "Epoch: [27][5/10]\tTime 0.268 (0.246)\tData 0.0480 (0.0490)\tLoss 0.5948 (0.6032)\tAccu 0.6836 (0.6852)\t\n",
            "Epoch: [27][6/10]\tTime 0.269 (0.250)\tData 0.0496 (0.0491)\tLoss 0.6342 (0.6084)\tAccu 0.6523 (0.6797)\t\n",
            "Epoch: [27][7/10]\tTime 0.271 (0.253)\tData 0.0496 (0.0492)\tLoss 0.6323 (0.6118)\tAccu 0.6328 (0.6730)\t\n",
            "Epoch: [27][8/10]\tTime 0.269 (0.255)\tData 0.0494 (0.0492)\tLoss 0.5661 (0.6061)\tAccu 0.7617 (0.6841)\t\n",
            "Epoch: [27][9/10]\tTime 0.270 (0.257)\tData 0.0493 (0.0492)\tLoss 0.5803 (0.6032)\tAccu 0.7109 (0.6871)\t\n",
            "Epoch: [27][10/10]\tTime 0.245 (0.256)\tData 0.0395 (0.0482)\tLoss 0.6508 (0.6069)\tAccu 0.6276 (0.6824)\t\n",
            "Time 0.099\tAccu 0.6500\tLoss 0.6297\t\n",
            "Epoch: [28][1/10]\tTime 0.156 (0.156)\tData 0.0474 (0.0474)\tLoss 0.5884 (0.5884)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [28][2/10]\tTime 0.269 (0.212)\tData 0.0510 (0.0492)\tLoss 0.6293 (0.6088)\tAccu 0.6289 (0.6621)\t\n",
            "Epoch: [28][3/10]\tTime 0.270 (0.232)\tData 0.0507 (0.0497)\tLoss 0.6326 (0.6168)\tAccu 0.6562 (0.6602)\t\n",
            "Epoch: [28][4/10]\tTime 0.271 (0.241)\tData 0.0487 (0.0495)\tLoss 0.5883 (0.6097)\tAccu 0.7227 (0.6758)\t\n",
            "Epoch: [28][5/10]\tTime 0.269 (0.247)\tData 0.0486 (0.0493)\tLoss 0.5667 (0.6011)\tAccu 0.7422 (0.6891)\t\n",
            "Epoch: [28][6/10]\tTime 0.271 (0.251)\tData 0.0494 (0.0493)\tLoss 0.6062 (0.6019)\tAccu 0.6602 (0.6842)\t\n",
            "Epoch: [28][7/10]\tTime 0.270 (0.254)\tData 0.0488 (0.0492)\tLoss 0.6451 (0.6081)\tAccu 0.6641 (0.6814)\t\n",
            "Epoch: [28][8/10]\tTime 0.269 (0.256)\tData 0.0489 (0.0492)\tLoss 0.6636 (0.6150)\tAccu 0.6445 (0.6768)\t\n",
            "Epoch: [28][9/10]\tTime 0.270 (0.257)\tData 0.0482 (0.0491)\tLoss 0.6091 (0.6144)\tAccu 0.6641 (0.6753)\t\n",
            "Epoch: [28][10/10]\tTime 0.248 (0.256)\tData 0.0438 (0.0486)\tLoss 0.5980 (0.6131)\tAccu 0.7296 (0.6796)\t\n",
            "Time 0.099\tAccu 0.6400\tLoss 0.6281\t\n",
            "Epoch: [29][1/10]\tTime 0.156 (0.156)\tData 0.0474 (0.0474)\tLoss 0.6132 (0.6132)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [29][2/10]\tTime 0.271 (0.213)\tData 0.0507 (0.0490)\tLoss 0.6281 (0.6207)\tAccu 0.6641 (0.6738)\t\n",
            "Epoch: [29][3/10]\tTime 0.271 (0.233)\tData 0.0498 (0.0493)\tLoss 0.6035 (0.6150)\tAccu 0.6836 (0.6771)\t\n",
            "Epoch: [29][4/10]\tTime 0.270 (0.242)\tData 0.0485 (0.0491)\tLoss 0.6031 (0.6120)\tAccu 0.6797 (0.6777)\t\n",
            "Epoch: [29][5/10]\tTime 0.270 (0.248)\tData 0.0482 (0.0489)\tLoss 0.6166 (0.6129)\tAccu 0.6680 (0.6758)\t\n",
            "Epoch: [29][6/10]\tTime 0.271 (0.251)\tData 0.0482 (0.0488)\tLoss 0.6118 (0.6127)\tAccu 0.6680 (0.6745)\t\n",
            "Epoch: [29][7/10]\tTime 0.271 (0.254)\tData 0.0494 (0.0489)\tLoss 0.6128 (0.6127)\tAccu 0.6719 (0.6741)\t\n",
            "Epoch: [29][8/10]\tTime 0.272 (0.256)\tData 0.0485 (0.0488)\tLoss 0.6273 (0.6146)\tAccu 0.6484 (0.6709)\t\n",
            "Epoch: [29][9/10]\tTime 0.270 (0.258)\tData 0.0497 (0.0489)\tLoss 0.5856 (0.6113)\tAccu 0.7344 (0.6780)\t\n",
            "Epoch: [29][10/10]\tTime 0.249 (0.257)\tData 0.0380 (0.0478)\tLoss 0.5771 (0.6087)\tAccu 0.7041 (0.6800)\t\n",
            "Time 0.100\tAccu 0.6540\tLoss 0.6331\t\n",
            "Epoch: [30][1/10]\tTime 0.154 (0.154)\tData 0.0472 (0.0472)\tLoss 0.6013 (0.6013)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [30][2/10]\tTime 0.271 (0.213)\tData 0.0484 (0.0478)\tLoss 0.6108 (0.6060)\tAccu 0.6680 (0.6836)\t\n",
            "Epoch: [30][3/10]\tTime 0.271 (0.232)\tData 0.0484 (0.0480)\tLoss 0.5514 (0.5878)\tAccu 0.7305 (0.6992)\t\n",
            "Epoch: [30][4/10]\tTime 0.272 (0.242)\tData 0.0489 (0.0482)\tLoss 0.6034 (0.5917)\tAccu 0.6875 (0.6963)\t\n",
            "Epoch: [30][5/10]\tTime 0.272 (0.248)\tData 0.0503 (0.0486)\tLoss 0.6169 (0.5968)\tAccu 0.6680 (0.6906)\t\n",
            "Epoch: [30][6/10]\tTime 0.273 (0.252)\tData 0.0487 (0.0487)\tLoss 0.5670 (0.5918)\tAccu 0.7188 (0.6953)\t\n",
            "Epoch: [30][7/10]\tTime 0.277 (0.256)\tData 0.0494 (0.0488)\tLoss 0.6262 (0.5967)\tAccu 0.6523 (0.6892)\t\n",
            "Epoch: [30][8/10]\tTime 0.272 (0.258)\tData 0.0487 (0.0487)\tLoss 0.6362 (0.6017)\tAccu 0.6406 (0.6831)\t\n",
            "Epoch: [30][9/10]\tTime 0.270 (0.259)\tData 0.0488 (0.0487)\tLoss 0.6261 (0.6044)\tAccu 0.6602 (0.6806)\t\n",
            "Epoch: [30][10/10]\tTime 0.246 (0.258)\tData 0.0379 (0.0477)\tLoss 0.6260 (0.6061)\tAccu 0.6786 (0.6804)\t\n",
            "Time 0.100\tAccu 0.6460\tLoss 0.6327\t\n",
            "Epoch: [31][1/10]\tTime 0.158 (0.158)\tData 0.0484 (0.0484)\tLoss 0.6216 (0.6216)\tAccu 0.6680 (0.6680)\t\n",
            "Epoch: [31][2/10]\tTime 0.271 (0.215)\tData 0.0489 (0.0487)\tLoss 0.6144 (0.6180)\tAccu 0.6719 (0.6699)\t\n",
            "Epoch: [31][3/10]\tTime 0.272 (0.234)\tData 0.0504 (0.0492)\tLoss 0.5953 (0.6104)\tAccu 0.7109 (0.6836)\t\n",
            "Epoch: [31][4/10]\tTime 0.271 (0.243)\tData 0.0484 (0.0490)\tLoss 0.6013 (0.6081)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [31][5/10]\tTime 0.273 (0.249)\tData 0.0489 (0.0490)\tLoss 0.5982 (0.6061)\tAccu 0.7031 (0.6875)\t\n",
            "Epoch: [31][6/10]\tTime 0.271 (0.253)\tData 0.0489 (0.0490)\tLoss 0.6460 (0.6128)\tAccu 0.6562 (0.6823)\t\n",
            "Epoch: [31][7/10]\tTime 0.273 (0.256)\tData 0.0484 (0.0489)\tLoss 0.5394 (0.6023)\tAccu 0.7422 (0.6908)\t\n",
            "Epoch: [31][8/10]\tTime 0.272 (0.258)\tData 0.0498 (0.0490)\tLoss 0.6560 (0.6090)\tAccu 0.6367 (0.6841)\t\n",
            "Epoch: [31][9/10]\tTime 0.271 (0.259)\tData 0.0483 (0.0489)\tLoss 0.6247 (0.6108)\tAccu 0.6562 (0.6810)\t\n",
            "Epoch: [31][10/10]\tTime 0.247 (0.258)\tData 0.0388 (0.0479)\tLoss 0.5978 (0.6097)\tAccu 0.6786 (0.6808)\t\n",
            "Time 0.100\tAccu 0.6440\tLoss 0.6291\t\n",
            "Epoch: [32][1/10]\tTime 0.156 (0.156)\tData 0.0476 (0.0476)\tLoss 0.6115 (0.6115)\tAccu 0.6641 (0.6641)\t\n",
            "Epoch: [32][2/10]\tTime 0.271 (0.213)\tData 0.0496 (0.0486)\tLoss 0.5864 (0.5989)\tAccu 0.7109 (0.6875)\t\n",
            "Epoch: [32][3/10]\tTime 0.274 (0.233)\tData 0.0486 (0.0486)\tLoss 0.6270 (0.6083)\tAccu 0.6641 (0.6797)\t\n",
            "Epoch: [32][4/10]\tTime 0.272 (0.243)\tData 0.0490 (0.0487)\tLoss 0.5964 (0.6053)\tAccu 0.6758 (0.6787)\t\n",
            "Epoch: [32][5/10]\tTime 0.273 (0.249)\tData 0.0491 (0.0488)\tLoss 0.6131 (0.6069)\tAccu 0.6797 (0.6789)\t\n",
            "Epoch: [32][6/10]\tTime 0.271 (0.253)\tData 0.0482 (0.0487)\tLoss 0.5868 (0.6035)\tAccu 0.7031 (0.6829)\t\n",
            "Epoch: [32][7/10]\tTime 0.271 (0.255)\tData 0.0488 (0.0487)\tLoss 0.6275 (0.6069)\tAccu 0.6562 (0.6791)\t\n",
            "Epoch: [32][8/10]\tTime 0.273 (0.257)\tData 0.0497 (0.0488)\tLoss 0.6334 (0.6103)\tAccu 0.6680 (0.6777)\t\n",
            "Epoch: [32][9/10]\tTime 0.270 (0.259)\tData 0.0488 (0.0488)\tLoss 0.6088 (0.6101)\tAccu 0.6953 (0.6797)\t\n",
            "Epoch: [32][10/10]\tTime 0.246 (0.258)\tData 0.0375 (0.0477)\tLoss 0.5910 (0.6086)\tAccu 0.6786 (0.6796)\t\n",
            "Time 0.100\tAccu 0.6560\tLoss 0.6299\t\n",
            "Epoch: [33][1/10]\tTime 0.155 (0.155)\tData 0.0474 (0.0474)\tLoss 0.6052 (0.6052)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [33][2/10]\tTime 0.272 (0.214)\tData 0.0497 (0.0485)\tLoss 0.6028 (0.6040)\tAccu 0.6758 (0.6797)\t\n",
            "Epoch: [33][3/10]\tTime 0.272 (0.233)\tData 0.0509 (0.0493)\tLoss 0.5929 (0.6003)\tAccu 0.6914 (0.6836)\t\n",
            "Epoch: [33][4/10]\tTime 0.270 (0.242)\tData 0.0493 (0.0493)\tLoss 0.6159 (0.6042)\tAccu 0.6641 (0.6787)\t\n",
            "Epoch: [33][5/10]\tTime 0.271 (0.248)\tData 0.0485 (0.0492)\tLoss 0.6049 (0.6043)\tAccu 0.6797 (0.6789)\t\n",
            "Epoch: [33][6/10]\tTime 0.271 (0.252)\tData 0.0496 (0.0492)\tLoss 0.5979 (0.6033)\tAccu 0.6875 (0.6803)\t\n",
            "Epoch: [33][7/10]\tTime 0.272 (0.255)\tData 0.0485 (0.0491)\tLoss 0.5651 (0.5978)\tAccu 0.7461 (0.6897)\t\n",
            "Epoch: [33][8/10]\tTime 0.273 (0.257)\tData 0.0484 (0.0490)\tLoss 0.6667 (0.6064)\tAccu 0.5977 (0.6782)\t\n",
            "Epoch: [33][9/10]\tTime 0.270 (0.258)\tData 0.0492 (0.0491)\tLoss 0.6025 (0.6060)\tAccu 0.6992 (0.6806)\t\n",
            "Epoch: [33][10/10]\tTime 0.243 (0.257)\tData 0.0371 (0.0479)\tLoss 0.5962 (0.6052)\tAccu 0.6939 (0.6816)\t\n",
            "Time 0.099\tAccu 0.6560\tLoss 0.6303\t\n",
            "Epoch: [34][1/10]\tTime 0.158 (0.158)\tData 0.0520 (0.0520)\tLoss 0.5664 (0.5664)\tAccu 0.7188 (0.7188)\t\n",
            "Epoch: [34][2/10]\tTime 0.272 (0.215)\tData 0.0493 (0.0507)\tLoss 0.5910 (0.5787)\tAccu 0.6914 (0.7051)\t\n",
            "Epoch: [34][3/10]\tTime 0.269 (0.233)\tData 0.0488 (0.0500)\tLoss 0.6526 (0.6033)\tAccu 0.6094 (0.6732)\t\n",
            "Epoch: [34][4/10]\tTime 0.270 (0.242)\tData 0.0483 (0.0496)\tLoss 0.6459 (0.6140)\tAccu 0.6367 (0.6641)\t\n",
            "Epoch: [34][5/10]\tTime 0.270 (0.248)\tData 0.0482 (0.0493)\tLoss 0.5993 (0.6111)\tAccu 0.6836 (0.6680)\t\n",
            "Epoch: [34][6/10]\tTime 0.270 (0.251)\tData 0.0489 (0.0493)\tLoss 0.5893 (0.6074)\tAccu 0.7148 (0.6758)\t\n",
            "Epoch: [34][7/10]\tTime 0.270 (0.254)\tData 0.0500 (0.0494)\tLoss 0.5976 (0.6060)\tAccu 0.7188 (0.6819)\t\n",
            "Epoch: [34][8/10]\tTime 0.270 (0.256)\tData 0.0486 (0.0493)\tLoss 0.5998 (0.6052)\tAccu 0.6836 (0.6821)\t\n",
            "Epoch: [34][9/10]\tTime 0.269 (0.258)\tData 0.0510 (0.0495)\tLoss 0.5701 (0.6013)\tAccu 0.7070 (0.6849)\t\n",
            "Epoch: [34][10/10]\tTime 0.243 (0.256)\tData 0.0383 (0.0483)\tLoss 0.6725 (0.6069)\tAccu 0.6276 (0.6804)\t\n",
            "Time 0.100\tAccu 0.6500\tLoss 0.6440\t\n",
            "Epoch: [35][1/10]\tTime 0.155 (0.155)\tData 0.0476 (0.0476)\tLoss 0.6042 (0.6042)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [35][2/10]\tTime 0.270 (0.212)\tData 0.0507 (0.0492)\tLoss 0.5988 (0.6015)\tAccu 0.6758 (0.6836)\t\n",
            "Epoch: [35][3/10]\tTime 0.269 (0.231)\tData 0.0487 (0.0490)\tLoss 0.5661 (0.5897)\tAccu 0.7148 (0.6940)\t\n",
            "Epoch: [35][4/10]\tTime 0.268 (0.240)\tData 0.0490 (0.0490)\tLoss 0.6243 (0.5983)\tAccu 0.6406 (0.6807)\t\n",
            "Epoch: [35][5/10]\tTime 0.269 (0.246)\tData 0.0486 (0.0489)\tLoss 0.6371 (0.6061)\tAccu 0.6250 (0.6695)\t\n",
            "Epoch: [35][6/10]\tTime 0.268 (0.250)\tData 0.0485 (0.0488)\tLoss 0.6000 (0.6051)\tAccu 0.6953 (0.6738)\t\n",
            "Epoch: [35][7/10]\tTime 0.268 (0.252)\tData 0.0488 (0.0488)\tLoss 0.6016 (0.6046)\tAccu 0.6914 (0.6763)\t\n",
            "Epoch: [35][8/10]\tTime 0.268 (0.254)\tData 0.0491 (0.0489)\tLoss 0.5774 (0.6012)\tAccu 0.7109 (0.6807)\t\n",
            "Epoch: [35][9/10]\tTime 0.276 (0.257)\tData 0.0491 (0.0489)\tLoss 0.6255 (0.6039)\tAccu 0.6562 (0.6780)\t\n",
            "Epoch: [35][10/10]\tTime 0.243 (0.255)\tData 0.0378 (0.0478)\tLoss 0.6257 (0.6056)\tAccu 0.6633 (0.6768)\t\n",
            "Time 0.099\tAccu 0.6520\tLoss 0.6389\t\n",
            "Epoch: [36][1/10]\tTime 0.154 (0.154)\tData 0.0471 (0.0471)\tLoss 0.5537 (0.5537)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [36][2/10]\tTime 0.269 (0.212)\tData 0.0483 (0.0477)\tLoss 0.6190 (0.5863)\tAccu 0.6562 (0.6816)\t\n",
            "Epoch: [36][3/10]\tTime 0.270 (0.231)\tData 0.0485 (0.0480)\tLoss 0.5913 (0.5880)\tAccu 0.6875 (0.6836)\t\n",
            "Epoch: [36][4/10]\tTime 0.268 (0.240)\tData 0.0499 (0.0485)\tLoss 0.6126 (0.5941)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [36][5/10]\tTime 0.268 (0.246)\tData 0.0488 (0.0485)\tLoss 0.5748 (0.5903)\tAccu 0.7148 (0.6898)\t\n",
            "Epoch: [36][6/10]\tTime 0.269 (0.250)\tData 0.0490 (0.0486)\tLoss 0.5879 (0.5899)\tAccu 0.6875 (0.6895)\t\n",
            "Epoch: [36][7/10]\tTime 0.268 (0.252)\tData 0.0482 (0.0485)\tLoss 0.6220 (0.5945)\tAccu 0.6562 (0.6847)\t\n",
            "Epoch: [36][8/10]\tTime 0.269 (0.254)\tData 0.0484 (0.0485)\tLoss 0.6312 (0.5991)\tAccu 0.6562 (0.6812)\t\n",
            "Epoch: [36][9/10]\tTime 0.267 (0.256)\tData 0.0486 (0.0485)\tLoss 0.5858 (0.5976)\tAccu 0.6758 (0.6806)\t\n",
            "Epoch: [36][10/10]\tTime 0.241 (0.254)\tData 0.0400 (0.0477)\tLoss 0.6482 (0.6016)\tAccu 0.6429 (0.6776)\t\n",
            "Time 0.099\tAccu 0.6460\tLoss 0.6272\t\n",
            "Epoch: [37][1/10]\tTime 0.154 (0.154)\tData 0.0480 (0.0480)\tLoss 0.5663 (0.5663)\tAccu 0.7266 (0.7266)\t\n",
            "Epoch: [37][2/10]\tTime 0.269 (0.211)\tData 0.0477 (0.0479)\tLoss 0.5455 (0.5559)\tAccu 0.7539 (0.7402)\t\n",
            "Epoch: [37][3/10]\tTime 0.269 (0.231)\tData 0.0493 (0.0483)\tLoss 0.6098 (0.5738)\tAccu 0.6641 (0.7148)\t\n",
            "Epoch: [37][4/10]\tTime 0.268 (0.240)\tData 0.0487 (0.0484)\tLoss 0.6125 (0.5835)\tAccu 0.6758 (0.7051)\t\n",
            "Epoch: [37][5/10]\tTime 0.269 (0.246)\tData 0.0494 (0.0486)\tLoss 0.5781 (0.5824)\tAccu 0.7109 (0.7063)\t\n",
            "Epoch: [37][6/10]\tTime 0.266 (0.249)\tData 0.0490 (0.0487)\tLoss 0.5931 (0.5842)\tAccu 0.6758 (0.7012)\t\n",
            "Epoch: [37][7/10]\tTime 0.269 (0.252)\tData 0.0492 (0.0487)\tLoss 0.6146 (0.5886)\tAccu 0.6406 (0.6925)\t\n",
            "Epoch: [37][8/10]\tTime 0.268 (0.254)\tData 0.0492 (0.0488)\tLoss 0.6268 (0.5933)\tAccu 0.6562 (0.6880)\t\n",
            "Epoch: [37][9/10]\tTime 0.267 (0.256)\tData 0.0501 (0.0490)\tLoss 0.6222 (0.5965)\tAccu 0.6445 (0.6832)\t\n",
            "Epoch: [37][10/10]\tTime 0.245 (0.254)\tData 0.0387 (0.0479)\tLoss 0.6241 (0.5987)\tAccu 0.6378 (0.6796)\t\n",
            "Time 0.099\tAccu 0.6400\tLoss 0.6304\t\n",
            "Epoch: [38][1/10]\tTime 0.152 (0.152)\tData 0.0469 (0.0469)\tLoss 0.5846 (0.5846)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [38][2/10]\tTime 0.270 (0.211)\tData 0.0490 (0.0480)\tLoss 0.5961 (0.5904)\tAccu 0.6836 (0.6953)\t\n",
            "Epoch: [38][3/10]\tTime 0.268 (0.230)\tData 0.0489 (0.0483)\tLoss 0.5819 (0.5875)\tAccu 0.6797 (0.6901)\t\n",
            "Epoch: [38][4/10]\tTime 0.266 (0.239)\tData 0.0488 (0.0484)\tLoss 0.6109 (0.5934)\tAccu 0.6680 (0.6846)\t\n",
            "Epoch: [38][5/10]\tTime 0.267 (0.245)\tData 0.0485 (0.0484)\tLoss 0.6281 (0.6003)\tAccu 0.6680 (0.6813)\t\n",
            "Epoch: [38][6/10]\tTime 0.268 (0.249)\tData 0.0482 (0.0484)\tLoss 0.5967 (0.5997)\tAccu 0.6602 (0.6777)\t\n",
            "Epoch: [38][7/10]\tTime 0.268 (0.251)\tData 0.0483 (0.0484)\tLoss 0.6037 (0.6003)\tAccu 0.6992 (0.6808)\t\n",
            "Epoch: [38][8/10]\tTime 0.273 (0.254)\tData 0.0484 (0.0484)\tLoss 0.6249 (0.6033)\tAccu 0.6562 (0.6777)\t\n",
            "Epoch: [38][9/10]\tTime 0.267 (0.255)\tData 0.0484 (0.0484)\tLoss 0.6090 (0.6040)\tAccu 0.6836 (0.6784)\t\n",
            "Epoch: [38][10/10]\tTime 0.241 (0.254)\tData 0.0373 (0.0473)\tLoss 0.5781 (0.6019)\tAccu 0.7194 (0.6816)\t\n",
            "Time 0.098\tAccu 0.6560\tLoss 0.6297\t\n",
            "Epoch: [39][1/10]\tTime 0.154 (0.154)\tData 0.0480 (0.0480)\tLoss 0.5731 (0.5731)\tAccu 0.7031 (0.7031)\t\n",
            "Epoch: [39][2/10]\tTime 0.269 (0.211)\tData 0.0508 (0.0494)\tLoss 0.6281 (0.6006)\tAccu 0.6758 (0.6895)\t\n",
            "Epoch: [39][3/10]\tTime 0.267 (0.230)\tData 0.0496 (0.0494)\tLoss 0.5902 (0.5971)\tAccu 0.6953 (0.6914)\t\n",
            "Epoch: [39][4/10]\tTime 0.267 (0.239)\tData 0.0521 (0.0501)\tLoss 0.6176 (0.6022)\tAccu 0.6641 (0.6846)\t\n",
            "Epoch: [39][5/10]\tTime 0.267 (0.245)\tData 0.0488 (0.0498)\tLoss 0.5925 (0.6003)\tAccu 0.6602 (0.6797)\t\n",
            "Epoch: [39][6/10]\tTime 0.268 (0.249)\tData 0.0490 (0.0497)\tLoss 0.6240 (0.6042)\tAccu 0.6602 (0.6764)\t\n",
            "Epoch: [39][7/10]\tTime 0.267 (0.251)\tData 0.0480 (0.0495)\tLoss 0.6015 (0.6039)\tAccu 0.6953 (0.6791)\t\n",
            "Epoch: [39][8/10]\tTime 0.267 (0.253)\tData 0.0484 (0.0493)\tLoss 0.6010 (0.6035)\tAccu 0.7031 (0.6821)\t\n",
            "Epoch: [39][9/10]\tTime 0.268 (0.255)\tData 0.0491 (0.0493)\tLoss 0.5900 (0.6020)\tAccu 0.6992 (0.6840)\t\n",
            "Epoch: [39][10/10]\tTime 0.242 (0.254)\tData 0.0370 (0.0481)\tLoss 0.6067 (0.6024)\tAccu 0.6684 (0.6828)\t\n",
            "Time 0.100\tAccu 0.6520\tLoss 0.6413\t\n",
            "Epoch: [40][1/10]\tTime 0.153 (0.153)\tData 0.0477 (0.0477)\tLoss 0.5763 (0.5763)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [40][2/10]\tTime 0.268 (0.211)\tData 0.0491 (0.0484)\tLoss 0.6071 (0.5917)\tAccu 0.6719 (0.6855)\t\n",
            "Epoch: [40][3/10]\tTime 0.267 (0.229)\tData 0.0493 (0.0487)\tLoss 0.5712 (0.5849)\tAccu 0.7031 (0.6914)\t\n",
            "Epoch: [40][4/10]\tTime 0.267 (0.239)\tData 0.0486 (0.0487)\tLoss 0.6062 (0.5902)\tAccu 0.6641 (0.6846)\t\n",
            "Epoch: [40][5/10]\tTime 0.266 (0.244)\tData 0.0482 (0.0486)\tLoss 0.5970 (0.5916)\tAccu 0.6875 (0.6852)\t\n",
            "Epoch: [40][6/10]\tTime 0.267 (0.248)\tData 0.0476 (0.0484)\tLoss 0.6219 (0.5966)\tAccu 0.6680 (0.6823)\t\n",
            "Epoch: [40][7/10]\tTime 0.267 (0.251)\tData 0.0495 (0.0486)\tLoss 0.6058 (0.5979)\tAccu 0.6875 (0.6830)\t\n",
            "Epoch: [40][8/10]\tTime 0.266 (0.253)\tData 0.0482 (0.0485)\tLoss 0.5977 (0.5979)\tAccu 0.6914 (0.6841)\t\n",
            "Epoch: [40][9/10]\tTime 0.265 (0.254)\tData 0.0483 (0.0485)\tLoss 0.6114 (0.5994)\tAccu 0.6953 (0.6853)\t\n",
            "Epoch: [40][10/10]\tTime 0.243 (0.253)\tData 0.0380 (0.0474)\tLoss 0.6269 (0.6016)\tAccu 0.6480 (0.6824)\t\n",
            "Time 0.099\tAccu 0.6480\tLoss 0.6267\t\n",
            "Epoch: [41][1/10]\tTime 0.157 (0.157)\tData 0.0488 (0.0488)\tLoss 0.5910 (0.5910)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [41][2/10]\tTime 0.268 (0.213)\tData 0.0514 (0.0501)\tLoss 0.6141 (0.6025)\tAccu 0.6758 (0.6738)\t\n",
            "Epoch: [41][3/10]\tTime 0.266 (0.230)\tData 0.0501 (0.0501)\tLoss 0.6046 (0.6032)\tAccu 0.6875 (0.6784)\t\n",
            "Epoch: [41][4/10]\tTime 0.267 (0.240)\tData 0.0487 (0.0497)\tLoss 0.6261 (0.6089)\tAccu 0.6562 (0.6729)\t\n",
            "Epoch: [41][5/10]\tTime 0.267 (0.245)\tData 0.0552 (0.0508)\tLoss 0.5875 (0.6047)\tAccu 0.6953 (0.6773)\t\n",
            "Epoch: [41][6/10]\tTime 0.268 (0.249)\tData 0.0484 (0.0504)\tLoss 0.6445 (0.6113)\tAccu 0.6055 (0.6654)\t\n",
            "Epoch: [41][7/10]\tTime 0.266 (0.251)\tData 0.0491 (0.0502)\tLoss 0.5970 (0.6092)\tAccu 0.6680 (0.6657)\t\n",
            "Epoch: [41][8/10]\tTime 0.267 (0.253)\tData 0.0492 (0.0501)\tLoss 0.5712 (0.6045)\tAccu 0.7422 (0.6753)\t\n",
            "Epoch: [41][9/10]\tTime 0.265 (0.255)\tData 0.0493 (0.0500)\tLoss 0.5647 (0.6001)\tAccu 0.7188 (0.6801)\t\n",
            "Epoch: [41][10/10]\tTime 0.242 (0.253)\tData 0.0378 (0.0488)\tLoss 0.5904 (0.5993)\tAccu 0.6786 (0.6800)\t\n",
            "Time 0.098\tAccu 0.6540\tLoss 0.6493\t\n",
            "Epoch: [42][1/10]\tTime 0.153 (0.153)\tData 0.0474 (0.0474)\tLoss 0.6622 (0.6622)\tAccu 0.6328 (0.6328)\t\n",
            "Epoch: [42][2/10]\tTime 0.269 (0.211)\tData 0.0484 (0.0479)\tLoss 0.6297 (0.6460)\tAccu 0.6797 (0.6562)\t\n",
            "Epoch: [42][3/10]\tTime 0.266 (0.229)\tData 0.0491 (0.0483)\tLoss 0.6013 (0.6311)\tAccu 0.6680 (0.6602)\t\n",
            "Epoch: [42][4/10]\tTime 0.267 (0.239)\tData 0.0492 (0.0485)\tLoss 0.6006 (0.6235)\tAccu 0.6641 (0.6611)\t\n",
            "Epoch: [42][5/10]\tTime 0.270 (0.245)\tData 0.0490 (0.0486)\tLoss 0.6137 (0.6215)\tAccu 0.6484 (0.6586)\t\n",
            "Epoch: [42][6/10]\tTime 0.266 (0.248)\tData 0.0492 (0.0487)\tLoss 0.5797 (0.6146)\tAccu 0.7227 (0.6693)\t\n",
            "Epoch: [42][7/10]\tTime 0.266 (0.251)\tData 0.0502 (0.0489)\tLoss 0.5723 (0.6085)\tAccu 0.7266 (0.6775)\t\n",
            "Epoch: [42][8/10]\tTime 0.265 (0.253)\tData 0.0489 (0.0489)\tLoss 0.5935 (0.6066)\tAccu 0.6836 (0.6782)\t\n",
            "Epoch: [42][9/10]\tTime 0.266 (0.254)\tData 0.0489 (0.0489)\tLoss 0.5387 (0.5991)\tAccu 0.7461 (0.6858)\t\n",
            "Epoch: [42][10/10]\tTime 0.242 (0.253)\tData 0.0375 (0.0478)\tLoss 0.6656 (0.6043)\tAccu 0.6276 (0.6812)\t\n",
            "Time 0.098\tAccu 0.6540\tLoss 0.6494\t\n",
            "Epoch: [43][1/10]\tTime 0.156 (0.156)\tData 0.0477 (0.0477)\tLoss 0.5937 (0.5937)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [43][2/10]\tTime 0.265 (0.211)\tData 0.0500 (0.0488)\tLoss 0.5918 (0.5927)\tAccu 0.6953 (0.6973)\t\n",
            "Epoch: [43][3/10]\tTime 0.265 (0.229)\tData 0.0490 (0.0489)\tLoss 0.6344 (0.6066)\tAccu 0.6445 (0.6797)\t\n",
            "Epoch: [43][4/10]\tTime 0.266 (0.238)\tData 0.0492 (0.0490)\tLoss 0.5875 (0.6018)\tAccu 0.7031 (0.6855)\t\n",
            "Epoch: [43][5/10]\tTime 0.267 (0.244)\tData 0.0486 (0.0489)\tLoss 0.5984 (0.6012)\tAccu 0.6523 (0.6789)\t\n",
            "Epoch: [43][6/10]\tTime 0.265 (0.247)\tData 0.0496 (0.0490)\tLoss 0.5912 (0.5995)\tAccu 0.6992 (0.6823)\t\n",
            "Epoch: [43][7/10]\tTime 0.269 (0.250)\tData 0.0491 (0.0490)\tLoss 0.6040 (0.6001)\tAccu 0.6953 (0.6842)\t\n",
            "Epoch: [43][8/10]\tTime 0.267 (0.252)\tData 0.0490 (0.0490)\tLoss 0.5910 (0.5990)\tAccu 0.6953 (0.6855)\t\n",
            "Epoch: [43][9/10]\tTime 0.265 (0.254)\tData 0.0500 (0.0491)\tLoss 0.6338 (0.6029)\tAccu 0.6484 (0.6814)\t\n",
            "Epoch: [43][10/10]\tTime 0.239 (0.252)\tData 0.0402 (0.0482)\tLoss 0.5665 (0.6000)\tAccu 0.6990 (0.6828)\t\n",
            "Time 0.099\tAccu 0.6500\tLoss 0.6343\t\n",
            "Epoch: [44][1/10]\tTime 0.155 (0.155)\tData 0.0477 (0.0477)\tLoss 0.5583 (0.5583)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [44][2/10]\tTime 0.267 (0.211)\tData 0.0499 (0.0488)\tLoss 0.5861 (0.5722)\tAccu 0.7266 (0.7207)\t\n",
            "Epoch: [44][3/10]\tTime 0.265 (0.229)\tData 0.0493 (0.0490)\tLoss 0.5989 (0.5811)\tAccu 0.6875 (0.7096)\t\n",
            "Epoch: [44][4/10]\tTime 0.267 (0.239)\tData 0.0489 (0.0490)\tLoss 0.5629 (0.5765)\tAccu 0.7070 (0.7090)\t\n",
            "Epoch: [44][5/10]\tTime 0.265 (0.244)\tData 0.0489 (0.0490)\tLoss 0.6137 (0.5840)\tAccu 0.6523 (0.6977)\t\n",
            "Epoch: [44][6/10]\tTime 0.265 (0.247)\tData 0.0498 (0.0491)\tLoss 0.5778 (0.5829)\tAccu 0.7109 (0.6999)\t\n",
            "Epoch: [44][7/10]\tTime 0.266 (0.250)\tData 0.0486 (0.0490)\tLoss 0.6185 (0.5880)\tAccu 0.6328 (0.6903)\t\n",
            "Epoch: [44][8/10]\tTime 0.266 (0.252)\tData 0.0497 (0.0491)\tLoss 0.6204 (0.5921)\tAccu 0.6445 (0.6846)\t\n",
            "Epoch: [44][9/10]\tTime 0.265 (0.253)\tData 0.0487 (0.0491)\tLoss 0.6076 (0.5938)\tAccu 0.6953 (0.6858)\t\n",
            "Epoch: [44][10/10]\tTime 0.244 (0.252)\tData 0.0376 (0.0479)\tLoss 0.6244 (0.5962)\tAccu 0.6378 (0.6820)\t\n",
            "Time 0.098\tAccu 0.6500\tLoss 0.6277\t\n",
            "Epoch: [45][1/10]\tTime 0.155 (0.155)\tData 0.0477 (0.0477)\tLoss 0.5965 (0.5965)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [45][2/10]\tTime 0.266 (0.211)\tData 0.0490 (0.0484)\tLoss 0.5996 (0.5980)\tAccu 0.6797 (0.6816)\t\n",
            "Epoch: [45][3/10]\tTime 0.264 (0.228)\tData 0.0485 (0.0484)\tLoss 0.5678 (0.5879)\tAccu 0.7266 (0.6966)\t\n",
            "Epoch: [45][4/10]\tTime 0.265 (0.238)\tData 0.0483 (0.0484)\tLoss 0.6435 (0.6018)\tAccu 0.6250 (0.6787)\t\n",
            "Epoch: [45][5/10]\tTime 0.265 (0.243)\tData 0.0488 (0.0485)\tLoss 0.5418 (0.5898)\tAccu 0.7773 (0.6984)\t\n",
            "Epoch: [45][6/10]\tTime 0.264 (0.247)\tData 0.0491 (0.0486)\tLoss 0.6046 (0.5923)\tAccu 0.6562 (0.6914)\t\n",
            "Epoch: [45][7/10]\tTime 0.269 (0.250)\tData 0.0486 (0.0486)\tLoss 0.5767 (0.5900)\tAccu 0.7109 (0.6942)\t\n",
            "Epoch: [45][8/10]\tTime 0.265 (0.252)\tData 0.0491 (0.0486)\tLoss 0.5891 (0.5899)\tAccu 0.6797 (0.6924)\t\n",
            "Epoch: [45][9/10]\tTime 0.265 (0.253)\tData 0.0489 (0.0487)\tLoss 0.6246 (0.5938)\tAccu 0.6406 (0.6866)\t\n",
            "Epoch: [45][10/10]\tTime 0.240 (0.252)\tData 0.0377 (0.0476)\tLoss 0.5843 (0.5930)\tAccu 0.7092 (0.6884)\t\n",
            "Time 0.099\tAccu 0.6380\tLoss 0.6294\t\n",
            "Epoch: [46][1/10]\tTime 0.156 (0.156)\tData 0.0487 (0.0487)\tLoss 0.5817 (0.5817)\tAccu 0.7188 (0.7188)\t\n",
            "Epoch: [46][2/10]\tTime 0.266 (0.211)\tData 0.0498 (0.0492)\tLoss 0.5901 (0.5859)\tAccu 0.6836 (0.7012)\t\n",
            "Epoch: [46][3/10]\tTime 0.266 (0.229)\tData 0.0483 (0.0489)\tLoss 0.6254 (0.5991)\tAccu 0.6562 (0.6862)\t\n",
            "Epoch: [46][4/10]\tTime 0.267 (0.239)\tData 0.0518 (0.0496)\tLoss 0.6362 (0.6084)\tAccu 0.6523 (0.6777)\t\n",
            "Epoch: [46][5/10]\tTime 0.266 (0.244)\tData 0.0494 (0.0496)\tLoss 0.5503 (0.5968)\tAccu 0.7070 (0.6836)\t\n",
            "Epoch: [46][6/10]\tTime 0.265 (0.248)\tData 0.0501 (0.0497)\tLoss 0.6144 (0.5997)\tAccu 0.6680 (0.6810)\t\n",
            "Epoch: [46][7/10]\tTime 0.266 (0.250)\tData 0.0489 (0.0496)\tLoss 0.5793 (0.5968)\tAccu 0.7266 (0.6875)\t\n",
            "Epoch: [46][8/10]\tTime 0.263 (0.252)\tData 0.0499 (0.0496)\tLoss 0.5985 (0.5970)\tAccu 0.6875 (0.6875)\t\n",
            "Epoch: [46][9/10]\tTime 0.269 (0.254)\tData 0.0504 (0.0497)\tLoss 0.6253 (0.6001)\tAccu 0.6367 (0.6819)\t\n",
            "Epoch: [46][10/10]\tTime 0.241 (0.252)\tData 0.0410 (0.0488)\tLoss 0.5451 (0.5958)\tAccu 0.7347 (0.6860)\t\n",
            "Time 0.099\tAccu 0.6540\tLoss 0.6311\t\n",
            "Epoch: [47][1/10]\tTime 0.154 (0.154)\tData 0.0476 (0.0476)\tLoss 0.6079 (0.6079)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [47][2/10]\tTime 0.268 (0.211)\tData 0.0500 (0.0488)\tLoss 0.5782 (0.5930)\tAccu 0.6992 (0.6895)\t\n",
            "Epoch: [47][3/10]\tTime 0.265 (0.229)\tData 0.0501 (0.0492)\tLoss 0.6437 (0.6099)\tAccu 0.6328 (0.6706)\t\n",
            "Epoch: [47][4/10]\tTime 0.269 (0.239)\tData 0.0491 (0.0492)\tLoss 0.6136 (0.6108)\tAccu 0.6680 (0.6699)\t\n",
            "Epoch: [47][5/10]\tTime 0.267 (0.245)\tData 0.0501 (0.0494)\tLoss 0.5955 (0.6078)\tAccu 0.7188 (0.6797)\t\n",
            "Epoch: [47][6/10]\tTime 0.266 (0.248)\tData 0.0493 (0.0494)\tLoss 0.5925 (0.6052)\tAccu 0.6992 (0.6829)\t\n",
            "Epoch: [47][7/10]\tTime 0.266 (0.251)\tData 0.0494 (0.0494)\tLoss 0.5948 (0.6037)\tAccu 0.6797 (0.6825)\t\n",
            "Epoch: [47][8/10]\tTime 0.269 (0.253)\tData 0.0486 (0.0493)\tLoss 0.5838 (0.6012)\tAccu 0.6797 (0.6821)\t\n",
            "Epoch: [47][9/10]\tTime 0.266 (0.254)\tData 0.0499 (0.0493)\tLoss 0.5734 (0.5981)\tAccu 0.7031 (0.6845)\t\n",
            "Epoch: [47][10/10]\tTime 0.242 (0.253)\tData 0.0381 (0.0482)\tLoss 0.5882 (0.5974)\tAccu 0.6939 (0.6852)\t\n",
            "Time 0.098\tAccu 0.6540\tLoss 0.6493\t\n",
            "Epoch: [48][1/10]\tTime 0.160 (0.160)\tData 0.0529 (0.0529)\tLoss 0.6224 (0.6224)\tAccu 0.6562 (0.6562)\t\n",
            "Epoch: [48][2/10]\tTime 0.268 (0.214)\tData 0.0508 (0.0519)\tLoss 0.5915 (0.6070)\tAccu 0.6641 (0.6602)\t\n",
            "Epoch: [48][3/10]\tTime 0.266 (0.231)\tData 0.0492 (0.0510)\tLoss 0.5847 (0.5996)\tAccu 0.6914 (0.6706)\t\n",
            "Epoch: [48][4/10]\tTime 0.267 (0.240)\tData 0.0496 (0.0506)\tLoss 0.5671 (0.5914)\tAccu 0.7227 (0.6836)\t\n",
            "Epoch: [48][5/10]\tTime 0.265 (0.245)\tData 0.0495 (0.0504)\tLoss 0.6206 (0.5973)\tAccu 0.6562 (0.6781)\t\n",
            "Epoch: [48][6/10]\tTime 0.265 (0.248)\tData 0.0508 (0.0505)\tLoss 0.5733 (0.5933)\tAccu 0.7227 (0.6855)\t\n",
            "Epoch: [48][7/10]\tTime 0.266 (0.251)\tData 0.0507 (0.0505)\tLoss 0.5779 (0.5911)\tAccu 0.6914 (0.6864)\t\n",
            "Epoch: [48][8/10]\tTime 0.266 (0.253)\tData 0.0525 (0.0508)\tLoss 0.6219 (0.5949)\tAccu 0.6797 (0.6855)\t\n",
            "Epoch: [48][9/10]\tTime 0.265 (0.254)\tData 0.0525 (0.0510)\tLoss 0.6080 (0.5964)\tAccu 0.6953 (0.6866)\t\n",
            "Epoch: [48][10/10]\tTime 0.240 (0.253)\tData 0.0383 (0.0497)\tLoss 0.5953 (0.5963)\tAccu 0.6888 (0.6868)\t\n",
            "Time 0.099\tAccu 0.6540\tLoss 0.6303\t\n",
            "Epoch: [49][1/10]\tTime 0.156 (0.156)\tData 0.0477 (0.0477)\tLoss 0.5578 (0.5578)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [49][2/10]\tTime 0.268 (0.212)\tData 0.0564 (0.0521)\tLoss 0.6253 (0.5916)\tAccu 0.6445 (0.6777)\t\n",
            "Epoch: [49][3/10]\tTime 0.265 (0.230)\tData 0.0494 (0.0512)\tLoss 0.6099 (0.5977)\tAccu 0.6680 (0.6745)\t\n",
            "Epoch: [49][4/10]\tTime 0.265 (0.238)\tData 0.0492 (0.0507)\tLoss 0.5762 (0.5923)\tAccu 0.7109 (0.6836)\t\n",
            "Epoch: [49][5/10]\tTime 0.266 (0.244)\tData 0.0495 (0.0505)\tLoss 0.6176 (0.5973)\tAccu 0.6680 (0.6805)\t\n",
            "Epoch: [49][6/10]\tTime 0.266 (0.248)\tData 0.0499 (0.0504)\tLoss 0.6022 (0.5981)\tAccu 0.6836 (0.6810)\t\n",
            "Epoch: [49][7/10]\tTime 0.266 (0.250)\tData 0.0501 (0.0503)\tLoss 0.5805 (0.5956)\tAccu 0.7109 (0.6853)\t\n",
            "Epoch: [49][8/10]\tTime 0.266 (0.252)\tData 0.0495 (0.0502)\tLoss 0.5697 (0.5924)\tAccu 0.7148 (0.6890)\t\n",
            "Epoch: [49][9/10]\tTime 0.267 (0.254)\tData 0.0497 (0.0502)\tLoss 0.5774 (0.5907)\tAccu 0.6719 (0.6871)\t\n",
            "Epoch: [49][10/10]\tTime 0.241 (0.253)\tData 0.0382 (0.0490)\tLoss 0.5743 (0.5894)\tAccu 0.6990 (0.6880)\t\n",
            "Time 0.098\tAccu 0.6500\tLoss 0.6346\t\n",
            "Epoch: [50][1/10]\tTime 0.155 (0.155)\tData 0.0479 (0.0479)\tLoss 0.5802 (0.5802)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [50][2/10]\tTime 0.270 (0.213)\tData 0.0497 (0.0488)\tLoss 0.6205 (0.6004)\tAccu 0.6953 (0.6836)\t\n",
            "Epoch: [50][3/10]\tTime 0.266 (0.230)\tData 0.0509 (0.0495)\tLoss 0.5769 (0.5925)\tAccu 0.6992 (0.6888)\t\n",
            "Epoch: [50][4/10]\tTime 0.265 (0.239)\tData 0.0493 (0.0494)\tLoss 0.5584 (0.5840)\tAccu 0.7227 (0.6973)\t\n",
            "Epoch: [50][5/10]\tTime 0.266 (0.244)\tData 0.0504 (0.0496)\tLoss 0.6034 (0.5879)\tAccu 0.6562 (0.6891)\t\n",
            "Epoch: [50][6/10]\tTime 0.267 (0.248)\tData 0.0492 (0.0496)\tLoss 0.5612 (0.5834)\tAccu 0.6914 (0.6895)\t\n",
            "Epoch: [50][7/10]\tTime 0.264 (0.251)\tData 0.0498 (0.0496)\tLoss 0.6217 (0.5889)\tAccu 0.6836 (0.6886)\t\n",
            "Epoch: [50][8/10]\tTime 0.265 (0.252)\tData 0.0501 (0.0497)\tLoss 0.5822 (0.5881)\tAccu 0.6797 (0.6875)\t\n",
            "Epoch: [50][9/10]\tTime 0.267 (0.254)\tData 0.0502 (0.0497)\tLoss 0.5796 (0.5871)\tAccu 0.6953 (0.6884)\t\n",
            "Epoch: [50][10/10]\tTime 0.241 (0.253)\tData 0.0389 (0.0486)\tLoss 0.5948 (0.5877)\tAccu 0.6990 (0.6892)\t\n",
            "Time 0.098\tAccu 0.6400\tLoss 0.6296\t\n",
            "Epoch: [51][1/10]\tTime 0.154 (0.154)\tData 0.0479 (0.0479)\tLoss 0.5828 (0.5828)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [51][2/10]\tTime 0.267 (0.210)\tData 0.0496 (0.0487)\tLoss 0.5917 (0.5873)\tAccu 0.6836 (0.6895)\t\n",
            "Epoch: [51][3/10]\tTime 0.267 (0.229)\tData 0.0488 (0.0488)\tLoss 0.6122 (0.5956)\tAccu 0.6602 (0.6797)\t\n",
            "Epoch: [51][4/10]\tTime 0.267 (0.239)\tData 0.0503 (0.0491)\tLoss 0.5754 (0.5905)\tAccu 0.6953 (0.6836)\t\n",
            "Epoch: [51][5/10]\tTime 0.265 (0.244)\tData 0.0492 (0.0492)\tLoss 0.5818 (0.5888)\tAccu 0.7109 (0.6891)\t\n",
            "Epoch: [51][6/10]\tTime 0.266 (0.248)\tData 0.0491 (0.0491)\tLoss 0.6067 (0.5918)\tAccu 0.6836 (0.6882)\t\n",
            "Epoch: [51][7/10]\tTime 0.266 (0.250)\tData 0.0516 (0.0495)\tLoss 0.6012 (0.5931)\tAccu 0.6758 (0.6864)\t\n",
            "Epoch: [51][8/10]\tTime 0.267 (0.252)\tData 0.0489 (0.0494)\tLoss 0.5860 (0.5922)\tAccu 0.6914 (0.6870)\t\n",
            "Epoch: [51][9/10]\tTime 0.266 (0.254)\tData 0.0509 (0.0496)\tLoss 0.5573 (0.5883)\tAccu 0.7031 (0.6888)\t\n",
            "Epoch: [51][10/10]\tTime 0.242 (0.253)\tData 0.0384 (0.0485)\tLoss 0.5759 (0.5874)\tAccu 0.6837 (0.6884)\t\n",
            "Time 0.099\tAccu 0.6500\tLoss 0.6437\t\n",
            "Epoch: [52][1/10]\tTime 0.155 (0.155)\tData 0.0481 (0.0481)\tLoss 0.5401 (0.5401)\tAccu 0.7344 (0.7344)\t\n",
            "Epoch: [52][2/10]\tTime 0.268 (0.211)\tData 0.0516 (0.0499)\tLoss 0.5952 (0.5677)\tAccu 0.6836 (0.7090)\t\n",
            "Epoch: [52][3/10]\tTime 0.266 (0.230)\tData 0.0493 (0.0497)\tLoss 0.5939 (0.5764)\tAccu 0.6953 (0.7044)\t\n",
            "Epoch: [52][4/10]\tTime 0.266 (0.239)\tData 0.0493 (0.0496)\tLoss 0.5863 (0.5789)\tAccu 0.6914 (0.7012)\t\n",
            "Epoch: [52][5/10]\tTime 0.267 (0.244)\tData 0.0490 (0.0495)\tLoss 0.5675 (0.5766)\tAccu 0.7188 (0.7047)\t\n",
            "Epoch: [52][6/10]\tTime 0.266 (0.248)\tData 0.0488 (0.0494)\tLoss 0.5487 (0.5720)\tAccu 0.7188 (0.7070)\t\n",
            "Epoch: [52][7/10]\tTime 0.273 (0.252)\tData 0.0494 (0.0494)\tLoss 0.6284 (0.5800)\tAccu 0.6797 (0.7031)\t\n",
            "Epoch: [52][8/10]\tTime 0.268 (0.254)\tData 0.0495 (0.0494)\tLoss 0.5987 (0.5824)\tAccu 0.6758 (0.6997)\t\n",
            "Epoch: [52][9/10]\tTime 0.267 (0.255)\tData 0.0510 (0.0496)\tLoss 0.6023 (0.5846)\tAccu 0.6602 (0.6953)\t\n",
            "Epoch: [52][10/10]\tTime 0.241 (0.254)\tData 0.0382 (0.0484)\tLoss 0.6394 (0.5889)\tAccu 0.6224 (0.6896)\t\n",
            "Time 0.099\tAccu 0.6320\tLoss 0.6368\t\n",
            "Epoch: [53][1/10]\tTime 0.154 (0.154)\tData 0.0473 (0.0473)\tLoss 0.5845 (0.5845)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [53][2/10]\tTime 0.266 (0.210)\tData 0.0504 (0.0488)\tLoss 0.5844 (0.5845)\tAccu 0.6602 (0.6797)\t\n",
            "Epoch: [53][3/10]\tTime 0.266 (0.229)\tData 0.0490 (0.0489)\tLoss 0.5880 (0.5856)\tAccu 0.6836 (0.6810)\t\n",
            "Epoch: [53][4/10]\tTime 0.268 (0.238)\tData 0.0497 (0.0491)\tLoss 0.5877 (0.5862)\tAccu 0.6836 (0.6816)\t\n",
            "Epoch: [53][5/10]\tTime 0.266 (0.244)\tData 0.0493 (0.0491)\tLoss 0.6155 (0.5920)\tAccu 0.6484 (0.6750)\t\n",
            "Epoch: [53][6/10]\tTime 0.267 (0.248)\tData 0.0493 (0.0492)\tLoss 0.5941 (0.5924)\tAccu 0.6992 (0.6790)\t\n",
            "Epoch: [53][7/10]\tTime 0.268 (0.251)\tData 0.0492 (0.0492)\tLoss 0.5850 (0.5913)\tAccu 0.7148 (0.6842)\t\n",
            "Epoch: [53][8/10]\tTime 0.266 (0.253)\tData 0.0505 (0.0493)\tLoss 0.5831 (0.5903)\tAccu 0.7227 (0.6890)\t\n",
            "Epoch: [53][9/10]\tTime 0.267 (0.254)\tData 0.0497 (0.0494)\tLoss 0.6097 (0.5925)\tAccu 0.6602 (0.6858)\t\n",
            "Epoch: [53][10/10]\tTime 0.242 (0.253)\tData 0.0379 (0.0482)\tLoss 0.5479 (0.5890)\tAccu 0.7296 (0.6892)\t\n",
            "Time 0.099\tAccu 0.6560\tLoss 0.6433\t\n",
            "Epoch: [54][1/10]\tTime 0.156 (0.156)\tData 0.0478 (0.0478)\tLoss 0.5566 (0.5566)\tAccu 0.7305 (0.7305)\t\n",
            "Epoch: [54][2/10]\tTime 0.267 (0.212)\tData 0.0496 (0.0487)\tLoss 0.5992 (0.5779)\tAccu 0.6758 (0.7031)\t\n",
            "Epoch: [54][3/10]\tTime 0.267 (0.230)\tData 0.0485 (0.0487)\tLoss 0.5831 (0.5796)\tAccu 0.6992 (0.7018)\t\n",
            "Epoch: [54][4/10]\tTime 0.268 (0.240)\tData 0.0488 (0.0487)\tLoss 0.5512 (0.5725)\tAccu 0.7500 (0.7139)\t\n",
            "Epoch: [54][5/10]\tTime 0.267 (0.245)\tData 0.0526 (0.0495)\tLoss 0.5714 (0.5723)\tAccu 0.7031 (0.7117)\t\n",
            "Epoch: [54][6/10]\tTime 0.266 (0.249)\tData 0.0486 (0.0493)\tLoss 0.6016 (0.5772)\tAccu 0.6797 (0.7064)\t\n",
            "Epoch: [54][7/10]\tTime 0.266 (0.251)\tData 0.0494 (0.0493)\tLoss 0.6036 (0.5809)\tAccu 0.6641 (0.7003)\t\n",
            "Epoch: [54][8/10]\tTime 0.270 (0.253)\tData 0.0502 (0.0494)\tLoss 0.6106 (0.5847)\tAccu 0.6406 (0.6929)\t\n",
            "Epoch: [54][9/10]\tTime 0.266 (0.255)\tData 0.0498 (0.0495)\tLoss 0.5973 (0.5861)\tAccu 0.6836 (0.6918)\t\n",
            "Epoch: [54][10/10]\tTime 0.242 (0.254)\tData 0.0373 (0.0483)\tLoss 0.6254 (0.5892)\tAccu 0.6429 (0.6880)\t\n",
            "Time 0.098\tAccu 0.6440\tLoss 0.6306\t\n",
            "Epoch: [55][1/10]\tTime 0.157 (0.157)\tData 0.0486 (0.0486)\tLoss 0.6116 (0.6116)\tAccu 0.6602 (0.6602)\t\n",
            "Epoch: [55][2/10]\tTime 0.267 (0.212)\tData 0.0506 (0.0496)\tLoss 0.5907 (0.6011)\tAccu 0.6914 (0.6758)\t\n",
            "Epoch: [55][3/10]\tTime 0.266 (0.230)\tData 0.0487 (0.0493)\tLoss 0.5597 (0.5873)\tAccu 0.7266 (0.6927)\t\n",
            "Epoch: [55][4/10]\tTime 0.267 (0.239)\tData 0.0487 (0.0492)\tLoss 0.5959 (0.5895)\tAccu 0.6797 (0.6895)\t\n",
            "Epoch: [55][5/10]\tTime 0.269 (0.245)\tData 0.0478 (0.0489)\tLoss 0.5841 (0.5884)\tAccu 0.6992 (0.6914)\t\n",
            "Epoch: [55][6/10]\tTime 0.267 (0.249)\tData 0.0480 (0.0487)\tLoss 0.5468 (0.5815)\tAccu 0.7227 (0.6966)\t\n",
            "Epoch: [55][7/10]\tTime 0.267 (0.251)\tData 0.0486 (0.0487)\tLoss 0.6146 (0.5862)\tAccu 0.6289 (0.6869)\t\n",
            "Epoch: [55][8/10]\tTime 0.266 (0.253)\tData 0.0482 (0.0487)\tLoss 0.6383 (0.5927)\tAccu 0.6602 (0.6836)\t\n",
            "Epoch: [55][9/10]\tTime 0.268 (0.255)\tData 0.0501 (0.0488)\tLoss 0.5736 (0.5906)\tAccu 0.7227 (0.6879)\t\n",
            "Epoch: [55][10/10]\tTime 0.241 (0.253)\tData 0.0381 (0.0478)\tLoss 0.5773 (0.5895)\tAccu 0.7398 (0.6920)\t\n",
            "Time 0.099\tAccu 0.6500\tLoss 0.6384\t\n",
            "Epoch: [56][1/10]\tTime 0.156 (0.156)\tData 0.0475 (0.0475)\tLoss 0.5814 (0.5814)\tAccu 0.6875 (0.6875)\t\n",
            "Epoch: [56][2/10]\tTime 0.266 (0.211)\tData 0.0538 (0.0506)\tLoss 0.6195 (0.6005)\tAccu 0.6758 (0.6816)\t\n",
            "Epoch: [56][3/10]\tTime 0.267 (0.230)\tData 0.0486 (0.0500)\tLoss 0.5941 (0.5983)\tAccu 0.7031 (0.6888)\t\n",
            "Epoch: [56][4/10]\tTime 0.266 (0.239)\tData 0.0486 (0.0496)\tLoss 0.5828 (0.5945)\tAccu 0.6953 (0.6904)\t\n",
            "Epoch: [56][5/10]\tTime 0.267 (0.244)\tData 0.0485 (0.0494)\tLoss 0.5790 (0.5914)\tAccu 0.7031 (0.6930)\t\n",
            "Epoch: [56][6/10]\tTime 0.267 (0.248)\tData 0.0489 (0.0493)\tLoss 0.5654 (0.5870)\tAccu 0.7500 (0.7025)\t\n",
            "Epoch: [56][7/10]\tTime 0.269 (0.251)\tData 0.0486 (0.0492)\tLoss 0.6058 (0.5897)\tAccu 0.6602 (0.6964)\t\n",
            "Epoch: [56][8/10]\tTime 0.266 (0.253)\tData 0.0489 (0.0492)\tLoss 0.5982 (0.5908)\tAccu 0.6953 (0.6963)\t\n",
            "Epoch: [56][9/10]\tTime 0.269 (0.255)\tData 0.0483 (0.0491)\tLoss 0.6355 (0.5957)\tAccu 0.6367 (0.6897)\t\n",
            "Epoch: [56][10/10]\tTime 0.242 (0.253)\tData 0.0421 (0.0484)\tLoss 0.5683 (0.5936)\tAccu 0.6990 (0.6904)\t\n",
            "Time 0.099\tAccu 0.6480\tLoss 0.6363\t\n",
            "Epoch: [57][1/10]\tTime 0.156 (0.156)\tData 0.0486 (0.0486)\tLoss 0.5815 (0.5815)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [57][2/10]\tTime 0.268 (0.212)\tData 0.0490 (0.0488)\tLoss 0.6038 (0.5927)\tAccu 0.6641 (0.6680)\t\n",
            "Epoch: [57][3/10]\tTime 0.267 (0.230)\tData 0.0500 (0.0492)\tLoss 0.5818 (0.5890)\tAccu 0.6953 (0.6771)\t\n",
            "Epoch: [57][4/10]\tTime 0.267 (0.240)\tData 0.0491 (0.0492)\tLoss 0.5640 (0.5828)\tAccu 0.7148 (0.6865)\t\n",
            "Epoch: [57][5/10]\tTime 0.268 (0.245)\tData 0.0490 (0.0491)\tLoss 0.5898 (0.5842)\tAccu 0.6797 (0.6852)\t\n",
            "Epoch: [57][6/10]\tTime 0.266 (0.249)\tData 0.0499 (0.0493)\tLoss 0.5961 (0.5862)\tAccu 0.6836 (0.6849)\t\n",
            "Epoch: [57][7/10]\tTime 0.268 (0.252)\tData 0.0492 (0.0493)\tLoss 0.5698 (0.5838)\tAccu 0.7070 (0.6881)\t\n",
            "Epoch: [57][8/10]\tTime 0.269 (0.254)\tData 0.0502 (0.0494)\tLoss 0.5694 (0.5820)\tAccu 0.6992 (0.6895)\t\n",
            "Epoch: [57][9/10]\tTime 0.267 (0.255)\tData 0.0513 (0.0496)\tLoss 0.6077 (0.5849)\tAccu 0.7031 (0.6910)\t\n",
            "Epoch: [57][10/10]\tTime 0.247 (0.254)\tData 0.0386 (0.0485)\tLoss 0.5887 (0.5852)\tAccu 0.6888 (0.6908)\t\n",
            "Time 0.099\tAccu 0.6600\tLoss 0.6420\t\n",
            "Epoch: [58][1/10]\tTime 0.156 (0.156)\tData 0.0487 (0.0487)\tLoss 0.5771 (0.5771)\tAccu 0.7031 (0.7031)\t\n",
            "Epoch: [58][2/10]\tTime 0.268 (0.212)\tData 0.0497 (0.0492)\tLoss 0.6408 (0.6090)\tAccu 0.6367 (0.6699)\t\n",
            "Epoch: [58][3/10]\tTime 0.267 (0.230)\tData 0.0490 (0.0491)\tLoss 0.5830 (0.6003)\tAccu 0.6953 (0.6784)\t\n",
            "Epoch: [58][4/10]\tTime 0.267 (0.240)\tData 0.0492 (0.0492)\tLoss 0.5831 (0.5960)\tAccu 0.7070 (0.6855)\t\n",
            "Epoch: [58][5/10]\tTime 0.268 (0.245)\tData 0.0492 (0.0492)\tLoss 0.5649 (0.5898)\tAccu 0.7422 (0.6969)\t\n",
            "Epoch: [58][6/10]\tTime 0.268 (0.249)\tData 0.0496 (0.0492)\tLoss 0.6019 (0.5918)\tAccu 0.6719 (0.6927)\t\n",
            "Epoch: [58][7/10]\tTime 0.268 (0.252)\tData 0.0488 (0.0492)\tLoss 0.5592 (0.5871)\tAccu 0.7031 (0.6942)\t\n",
            "Epoch: [58][8/10]\tTime 0.269 (0.254)\tData 0.0493 (0.0492)\tLoss 0.5389 (0.5811)\tAccu 0.7109 (0.6963)\t\n",
            "Epoch: [58][9/10]\tTime 0.268 (0.256)\tData 0.0496 (0.0492)\tLoss 0.6851 (0.5927)\tAccu 0.6250 (0.6884)\t\n",
            "Epoch: [58][10/10]\tTime 0.242 (0.254)\tData 0.0386 (0.0482)\tLoss 0.5283 (0.5876)\tAccu 0.7551 (0.6936)\t\n",
            "Time 0.099\tAccu 0.6480\tLoss 0.6318\t\n",
            "Epoch: [59][1/10]\tTime 0.155 (0.155)\tData 0.0482 (0.0482)\tLoss 0.5811 (0.5811)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [59][2/10]\tTime 0.269 (0.212)\tData 0.0493 (0.0487)\tLoss 0.5936 (0.5874)\tAccu 0.6758 (0.6855)\t\n",
            "Epoch: [59][3/10]\tTime 0.267 (0.231)\tData 0.0510 (0.0495)\tLoss 0.5925 (0.5891)\tAccu 0.6953 (0.6888)\t\n",
            "Epoch: [59][4/10]\tTime 0.268 (0.240)\tData 0.0499 (0.0496)\tLoss 0.5746 (0.5854)\tAccu 0.6914 (0.6895)\t\n",
            "Epoch: [59][5/10]\tTime 0.268 (0.246)\tData 0.0487 (0.0494)\tLoss 0.5882 (0.5860)\tAccu 0.7031 (0.6922)\t\n",
            "Epoch: [59][6/10]\tTime 0.269 (0.249)\tData 0.0493 (0.0494)\tLoss 0.5937 (0.5873)\tAccu 0.6758 (0.6895)\t\n",
            "Epoch: [59][7/10]\tTime 0.269 (0.252)\tData 0.0506 (0.0496)\tLoss 0.5643 (0.5840)\tAccu 0.7148 (0.6931)\t\n",
            "Epoch: [59][8/10]\tTime 0.269 (0.254)\tData 0.0498 (0.0496)\tLoss 0.5883 (0.5845)\tAccu 0.6641 (0.6895)\t\n",
            "Epoch: [59][9/10]\tTime 0.267 (0.256)\tData 0.0501 (0.0496)\tLoss 0.5660 (0.5825)\tAccu 0.7070 (0.6914)\t\n",
            "Epoch: [59][10/10]\tTime 0.243 (0.255)\tData 0.0398 (0.0487)\tLoss 0.5508 (0.5800)\tAccu 0.7245 (0.6940)\t\n",
            "Time 0.099\tAccu 0.6500\tLoss 0.6297\t\n",
            "Epoch: [60][1/10]\tTime 0.156 (0.156)\tData 0.0490 (0.0490)\tLoss 0.6035 (0.6035)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [60][2/10]\tTime 0.270 (0.213)\tData 0.0503 (0.0497)\tLoss 0.6124 (0.6079)\tAccu 0.6602 (0.6660)\t\n",
            "Epoch: [60][3/10]\tTime 0.268 (0.231)\tData 0.0488 (0.0494)\tLoss 0.5885 (0.6015)\tAccu 0.6836 (0.6719)\t\n",
            "Epoch: [60][4/10]\tTime 0.269 (0.241)\tData 0.0487 (0.0492)\tLoss 0.5716 (0.5940)\tAccu 0.6797 (0.6738)\t\n",
            "Epoch: [60][5/10]\tTime 0.269 (0.246)\tData 0.0486 (0.0491)\tLoss 0.5450 (0.5842)\tAccu 0.7305 (0.6852)\t\n",
            "Epoch: [60][6/10]\tTime 0.267 (0.250)\tData 0.0487 (0.0490)\tLoss 0.5702 (0.5819)\tAccu 0.7031 (0.6882)\t\n",
            "Epoch: [60][7/10]\tTime 0.267 (0.252)\tData 0.0485 (0.0489)\tLoss 0.5586 (0.5785)\tAccu 0.7305 (0.6942)\t\n",
            "Epoch: [60][8/10]\tTime 0.268 (0.254)\tData 0.0485 (0.0489)\tLoss 0.6123 (0.5828)\tAccu 0.6719 (0.6914)\t\n",
            "Epoch: [60][9/10]\tTime 0.267 (0.256)\tData 0.0485 (0.0488)\tLoss 0.5684 (0.5812)\tAccu 0.7227 (0.6949)\t\n",
            "Epoch: [60][10/10]\tTime 0.245 (0.255)\tData 0.0373 (0.0477)\tLoss 0.6124 (0.5836)\tAccu 0.6837 (0.6940)\t\n",
            "Time 0.100\tAccu 0.6500\tLoss 0.6312\t\n",
            "Epoch: [61][1/10]\tTime 0.155 (0.155)\tData 0.0476 (0.0476)\tLoss 0.5882 (0.5882)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [61][2/10]\tTime 0.269 (0.212)\tData 0.0491 (0.0484)\tLoss 0.5640 (0.5761)\tAccu 0.7148 (0.7031)\t\n",
            "Epoch: [61][3/10]\tTime 0.267 (0.231)\tData 0.0499 (0.0489)\tLoss 0.5879 (0.5800)\tAccu 0.6914 (0.6992)\t\n",
            "Epoch: [61][4/10]\tTime 0.269 (0.240)\tData 0.0498 (0.0491)\tLoss 0.5684 (0.5771)\tAccu 0.7109 (0.7021)\t\n",
            "Epoch: [61][5/10]\tTime 0.267 (0.246)\tData 0.0510 (0.0495)\tLoss 0.5731 (0.5763)\tAccu 0.6914 (0.7000)\t\n",
            "Epoch: [61][6/10]\tTime 0.268 (0.249)\tData 0.0488 (0.0494)\tLoss 0.5927 (0.5790)\tAccu 0.7070 (0.7012)\t\n",
            "Epoch: [61][7/10]\tTime 0.268 (0.252)\tData 0.0488 (0.0493)\tLoss 0.6170 (0.5845)\tAccu 0.6250 (0.6903)\t\n",
            "Epoch: [61][8/10]\tTime 0.269 (0.254)\tData 0.0483 (0.0492)\tLoss 0.5818 (0.5841)\tAccu 0.6836 (0.6895)\t\n",
            "Epoch: [61][9/10]\tTime 0.267 (0.256)\tData 0.0485 (0.0491)\tLoss 0.5567 (0.5811)\tAccu 0.7383 (0.6949)\t\n",
            "Epoch: [61][10/10]\tTime 0.242 (0.254)\tData 0.0378 (0.0480)\tLoss 0.5627 (0.5796)\tAccu 0.7041 (0.6956)\t\n",
            "Time 0.099\tAccu 0.6560\tLoss 0.6583\t\n",
            "Epoch: [62][1/10]\tTime 0.159 (0.159)\tData 0.0479 (0.0479)\tLoss 0.5967 (0.5967)\tAccu 0.6914 (0.6914)\t\n",
            "Epoch: [62][2/10]\tTime 0.270 (0.215)\tData 0.0504 (0.0491)\tLoss 0.6017 (0.5992)\tAccu 0.6758 (0.6836)\t\n",
            "Epoch: [62][3/10]\tTime 0.266 (0.232)\tData 0.0497 (0.0493)\tLoss 0.6004 (0.5996)\tAccu 0.6680 (0.6784)\t\n",
            "Epoch: [62][4/10]\tTime 0.268 (0.241)\tData 0.0491 (0.0492)\tLoss 0.5817 (0.5951)\tAccu 0.6914 (0.6816)\t\n",
            "Epoch: [62][5/10]\tTime 0.269 (0.247)\tData 0.0491 (0.0492)\tLoss 0.5751 (0.5911)\tAccu 0.6992 (0.6852)\t\n",
            "Epoch: [62][6/10]\tTime 0.270 (0.250)\tData 0.0490 (0.0492)\tLoss 0.5820 (0.5896)\tAccu 0.6797 (0.6842)\t\n",
            "Epoch: [62][7/10]\tTime 0.267 (0.253)\tData 0.0486 (0.0491)\tLoss 0.5797 (0.5882)\tAccu 0.7305 (0.6908)\t\n",
            "Epoch: [62][8/10]\tTime 0.268 (0.255)\tData 0.0488 (0.0491)\tLoss 0.5686 (0.5857)\tAccu 0.7109 (0.6934)\t\n",
            "Epoch: [62][9/10]\tTime 0.269 (0.256)\tData 0.0491 (0.0491)\tLoss 0.5966 (0.5869)\tAccu 0.7031 (0.6944)\t\n",
            "Epoch: [62][10/10]\tTime 0.243 (0.255)\tData 0.0388 (0.0480)\tLoss 0.6114 (0.5889)\tAccu 0.6531 (0.6912)\t\n",
            "Time 0.099\tAccu 0.6560\tLoss 0.6365\t\n",
            "Epoch: [63][1/10]\tTime 0.154 (0.154)\tData 0.0474 (0.0474)\tLoss 0.5673 (0.5673)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [63][2/10]\tTime 0.269 (0.212)\tData 0.0528 (0.0501)\tLoss 0.5746 (0.5709)\tAccu 0.7031 (0.7051)\t\n",
            "Epoch: [63][3/10]\tTime 0.267 (0.230)\tData 0.0489 (0.0497)\tLoss 0.6030 (0.5816)\tAccu 0.6719 (0.6940)\t\n",
            "Epoch: [63][4/10]\tTime 0.269 (0.240)\tData 0.0493 (0.0496)\tLoss 0.5877 (0.5831)\tAccu 0.7031 (0.6963)\t\n",
            "Epoch: [63][5/10]\tTime 0.268 (0.245)\tData 0.0490 (0.0495)\tLoss 0.5666 (0.5798)\tAccu 0.7148 (0.7000)\t\n",
            "Epoch: [63][6/10]\tTime 0.268 (0.249)\tData 0.0494 (0.0495)\tLoss 0.5956 (0.5825)\tAccu 0.6875 (0.6979)\t\n",
            "Epoch: [63][7/10]\tTime 0.267 (0.252)\tData 0.0488 (0.0494)\tLoss 0.5621 (0.5795)\tAccu 0.7031 (0.6987)\t\n",
            "Epoch: [63][8/10]\tTime 0.269 (0.254)\tData 0.0483 (0.0492)\tLoss 0.5858 (0.5803)\tAccu 0.6875 (0.6973)\t\n",
            "Epoch: [63][9/10]\tTime 0.268 (0.255)\tData 0.0493 (0.0492)\tLoss 0.6055 (0.5831)\tAccu 0.6914 (0.6966)\t\n",
            "Epoch: [63][10/10]\tTime 0.243 (0.254)\tData 0.0379 (0.0481)\tLoss 0.5829 (0.5831)\tAccu 0.6990 (0.6968)\t\n",
            "Time 0.099\tAccu 0.6520\tLoss 0.6269\t\n",
            "Epoch: [64][1/10]\tTime 0.155 (0.155)\tData 0.0476 (0.0476)\tLoss 0.5752 (0.5752)\tAccu 0.7031 (0.7031)\t\n",
            "Epoch: [64][2/10]\tTime 0.274 (0.214)\tData 0.0491 (0.0484)\tLoss 0.5870 (0.5811)\tAccu 0.6914 (0.6973)\t\n",
            "Epoch: [64][3/10]\tTime 0.269 (0.232)\tData 0.0501 (0.0489)\tLoss 0.5784 (0.5802)\tAccu 0.6797 (0.6914)\t\n",
            "Epoch: [64][4/10]\tTime 0.268 (0.241)\tData 0.0490 (0.0490)\tLoss 0.5267 (0.5668)\tAccu 0.7461 (0.7051)\t\n",
            "Epoch: [64][5/10]\tTime 0.268 (0.247)\tData 0.0487 (0.0489)\tLoss 0.6083 (0.5751)\tAccu 0.6641 (0.6969)\t\n",
            "Epoch: [64][6/10]\tTime 0.268 (0.250)\tData 0.0494 (0.0490)\tLoss 0.5753 (0.5752)\tAccu 0.7109 (0.6992)\t\n",
            "Epoch: [64][7/10]\tTime 0.268 (0.253)\tData 0.0489 (0.0490)\tLoss 0.5564 (0.5725)\tAccu 0.7148 (0.7015)\t\n",
            "Epoch: [64][8/10]\tTime 0.271 (0.255)\tData 0.0485 (0.0489)\tLoss 0.5990 (0.5758)\tAccu 0.6875 (0.6997)\t\n",
            "Epoch: [64][9/10]\tTime 0.269 (0.256)\tData 0.0490 (0.0489)\tLoss 0.5812 (0.5764)\tAccu 0.6875 (0.6984)\t\n",
            "Epoch: [64][10/10]\tTime 0.242 (0.255)\tData 0.0372 (0.0477)\tLoss 0.6019 (0.5784)\tAccu 0.6531 (0.6948)\t\n",
            "Time 0.099\tAccu 0.6280\tLoss 0.6456\t\n",
            "Epoch: [65][1/10]\tTime 0.157 (0.157)\tData 0.0488 (0.0488)\tLoss 0.5881 (0.5881)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [65][2/10]\tTime 0.267 (0.212)\tData 0.0512 (0.0500)\tLoss 0.6049 (0.5965)\tAccu 0.6953 (0.6895)\t\n",
            "Epoch: [65][3/10]\tTime 0.268 (0.230)\tData 0.0537 (0.0512)\tLoss 0.5838 (0.5923)\tAccu 0.7070 (0.6953)\t\n",
            "Epoch: [65][4/10]\tTime 0.268 (0.240)\tData 0.0497 (0.0508)\tLoss 0.5293 (0.5765)\tAccu 0.7500 (0.7090)\t\n",
            "Epoch: [65][5/10]\tTime 0.268 (0.245)\tData 0.0489 (0.0505)\tLoss 0.6331 (0.5879)\tAccu 0.6758 (0.7023)\t\n",
            "Epoch: [65][6/10]\tTime 0.269 (0.249)\tData 0.0494 (0.0503)\tLoss 0.6566 (0.5993)\tAccu 0.6680 (0.6966)\t\n",
            "Epoch: [65][7/10]\tTime 0.266 (0.252)\tData 0.0508 (0.0504)\tLoss 0.5551 (0.5930)\tAccu 0.7422 (0.7031)\t\n",
            "Epoch: [65][8/10]\tTime 0.268 (0.254)\tData 0.0490 (0.0502)\tLoss 0.6004 (0.5939)\tAccu 0.6719 (0.6992)\t\n",
            "Epoch: [65][9/10]\tTime 0.269 (0.255)\tData 0.0507 (0.0503)\tLoss 0.5911 (0.5936)\tAccu 0.7266 (0.7023)\t\n",
            "Epoch: [65][10/10]\tTime 0.242 (0.254)\tData 0.0374 (0.0490)\tLoss 0.6013 (0.5942)\tAccu 0.6429 (0.6976)\t\n",
            "Time 0.099\tAccu 0.6600\tLoss 0.6253\t\n",
            "Epoch: [66][1/10]\tTime 0.156 (0.156)\tData 0.0471 (0.0471)\tLoss 0.5742 (0.5742)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [66][2/10]\tTime 0.268 (0.212)\tData 0.0492 (0.0481)\tLoss 0.6193 (0.5968)\tAccu 0.6680 (0.6895)\t\n",
            "Epoch: [66][3/10]\tTime 0.276 (0.233)\tData 0.0484 (0.0482)\tLoss 0.6307 (0.6081)\tAccu 0.6406 (0.6732)\t\n",
            "Epoch: [66][4/10]\tTime 0.267 (0.242)\tData 0.0489 (0.0484)\tLoss 0.5365 (0.5902)\tAccu 0.7383 (0.6895)\t\n",
            "Epoch: [66][5/10]\tTime 0.268 (0.247)\tData 0.0492 (0.0485)\tLoss 0.5971 (0.5916)\tAccu 0.6680 (0.6852)\t\n",
            "Epoch: [66][6/10]\tTime 0.266 (0.250)\tData 0.0491 (0.0486)\tLoss 0.5393 (0.5829)\tAccu 0.7383 (0.6940)\t\n",
            "Epoch: [66][7/10]\tTime 0.268 (0.253)\tData 0.0518 (0.0491)\tLoss 0.5775 (0.5821)\tAccu 0.6953 (0.6942)\t\n",
            "Epoch: [66][8/10]\tTime 0.268 (0.255)\tData 0.0486 (0.0490)\tLoss 0.5943 (0.5836)\tAccu 0.6797 (0.6924)\t\n",
            "Epoch: [66][9/10]\tTime 0.267 (0.256)\tData 0.0493 (0.0490)\tLoss 0.5645 (0.5815)\tAccu 0.6953 (0.6927)\t\n",
            "Epoch: [66][10/10]\tTime 0.242 (0.255)\tData 0.0382 (0.0480)\tLoss 0.5631 (0.5800)\tAccu 0.7092 (0.6940)\t\n",
            "Time 0.098\tAccu 0.6680\tLoss 0.6279\t\n",
            "Epoch: [67][1/10]\tTime 0.156 (0.156)\tData 0.0481 (0.0481)\tLoss 0.5835 (0.5835)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [67][2/10]\tTime 0.267 (0.212)\tData 0.0492 (0.0486)\tLoss 0.5392 (0.5614)\tAccu 0.7383 (0.7227)\t\n",
            "Epoch: [67][3/10]\tTime 0.267 (0.230)\tData 0.0485 (0.0486)\tLoss 0.5622 (0.5616)\tAccu 0.6992 (0.7148)\t\n",
            "Epoch: [67][4/10]\tTime 0.266 (0.239)\tData 0.0527 (0.0496)\tLoss 0.5804 (0.5663)\tAccu 0.6953 (0.7100)\t\n",
            "Epoch: [67][5/10]\tTime 0.268 (0.245)\tData 0.0492 (0.0495)\tLoss 0.5677 (0.5666)\tAccu 0.6914 (0.7063)\t\n",
            "Epoch: [67][6/10]\tTime 0.266 (0.248)\tData 0.0496 (0.0495)\tLoss 0.6013 (0.5724)\tAccu 0.6875 (0.7031)\t\n",
            "Epoch: [67][7/10]\tTime 0.268 (0.251)\tData 0.0482 (0.0494)\tLoss 0.6005 (0.5764)\tAccu 0.6641 (0.6975)\t\n",
            "Epoch: [67][8/10]\tTime 0.268 (0.253)\tData 0.0531 (0.0498)\tLoss 0.5748 (0.5762)\tAccu 0.7109 (0.6992)\t\n",
            "Epoch: [67][9/10]\tTime 0.269 (0.255)\tData 0.0496 (0.0498)\tLoss 0.5834 (0.5770)\tAccu 0.6953 (0.6988)\t\n",
            "Epoch: [67][10/10]\tTime 0.242 (0.254)\tData 0.0376 (0.0486)\tLoss 0.6247 (0.5807)\tAccu 0.6429 (0.6944)\t\n",
            "Time 0.100\tAccu 0.6480\tLoss 0.6621\t\n",
            "Epoch: [68][1/10]\tTime 0.155 (0.155)\tData 0.0484 (0.0484)\tLoss 0.6146 (0.6146)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [68][2/10]\tTime 0.268 (0.212)\tData 0.0500 (0.0492)\tLoss 0.5765 (0.5955)\tAccu 0.6992 (0.6914)\t\n",
            "Epoch: [68][3/10]\tTime 0.267 (0.230)\tData 0.0494 (0.0493)\tLoss 0.5810 (0.5907)\tAccu 0.6953 (0.6927)\t\n",
            "Epoch: [68][4/10]\tTime 0.271 (0.240)\tData 0.0493 (0.0493)\tLoss 0.5643 (0.5841)\tAccu 0.7266 (0.7012)\t\n",
            "Epoch: [68][5/10]\tTime 0.266 (0.245)\tData 0.0496 (0.0493)\tLoss 0.6118 (0.5896)\tAccu 0.6523 (0.6914)\t\n",
            "Epoch: [68][6/10]\tTime 0.268 (0.249)\tData 0.0484 (0.0492)\tLoss 0.5438 (0.5820)\tAccu 0.7500 (0.7012)\t\n",
            "Epoch: [68][7/10]\tTime 0.266 (0.252)\tData 0.0487 (0.0491)\tLoss 0.5620 (0.5791)\tAccu 0.7070 (0.7020)\t\n",
            "Epoch: [68][8/10]\tTime 0.270 (0.254)\tData 0.0496 (0.0492)\tLoss 0.5635 (0.5772)\tAccu 0.6875 (0.7002)\t\n",
            "Epoch: [68][9/10]\tTime 0.267 (0.255)\tData 0.0485 (0.0491)\tLoss 0.5546 (0.5747)\tAccu 0.7109 (0.7014)\t\n",
            "Epoch: [68][10/10]\tTime 0.242 (0.254)\tData 0.0388 (0.0481)\tLoss 0.5819 (0.5752)\tAccu 0.6888 (0.7004)\t\n",
            "Time 0.099\tAccu 0.6620\tLoss 0.6342\t\n",
            "Epoch: [69][1/10]\tTime 0.159 (0.159)\tData 0.0516 (0.0516)\tLoss 0.6023 (0.6023)\tAccu 0.6719 (0.6719)\t\n",
            "Epoch: [69][2/10]\tTime 0.267 (0.213)\tData 0.0492 (0.0504)\tLoss 0.6134 (0.6079)\tAccu 0.6758 (0.6738)\t\n",
            "Epoch: [69][3/10]\tTime 0.267 (0.231)\tData 0.0487 (0.0498)\tLoss 0.5540 (0.5899)\tAccu 0.7148 (0.6875)\t\n",
            "Epoch: [69][4/10]\tTime 0.267 (0.240)\tData 0.0495 (0.0497)\tLoss 0.6087 (0.5946)\tAccu 0.6484 (0.6777)\t\n",
            "Epoch: [69][5/10]\tTime 0.267 (0.246)\tData 0.0494 (0.0497)\tLoss 0.6078 (0.5972)\tAccu 0.6914 (0.6805)\t\n",
            "Epoch: [69][6/10]\tTime 0.270 (0.250)\tData 0.0499 (0.0497)\tLoss 0.5169 (0.5839)\tAccu 0.7656 (0.6947)\t\n",
            "Epoch: [69][7/10]\tTime 0.267 (0.252)\tData 0.0495 (0.0497)\tLoss 0.5992 (0.5860)\tAccu 0.6914 (0.6942)\t\n",
            "Epoch: [69][8/10]\tTime 0.270 (0.254)\tData 0.0497 (0.0497)\tLoss 0.5402 (0.5803)\tAccu 0.7188 (0.6973)\t\n",
            "Epoch: [69][9/10]\tTime 0.268 (0.256)\tData 0.0520 (0.0500)\tLoss 0.5843 (0.5808)\tAccu 0.6797 (0.6953)\t\n",
            "Epoch: [69][10/10]\tTime 0.242 (0.254)\tData 0.0380 (0.0488)\tLoss 0.5574 (0.5789)\tAccu 0.7041 (0.6960)\t\n",
            "Time 0.098\tAccu 0.6420\tLoss 0.6356\t\n",
            "Epoch: [70][1/10]\tTime 0.156 (0.156)\tData 0.0474 (0.0474)\tLoss 0.5507 (0.5507)\tAccu 0.7383 (0.7383)\t\n",
            "Epoch: [70][2/10]\tTime 0.267 (0.211)\tData 0.0503 (0.0489)\tLoss 0.5564 (0.5535)\tAccu 0.7344 (0.7363)\t\n",
            "Epoch: [70][3/10]\tTime 0.266 (0.230)\tData 0.0489 (0.0489)\tLoss 0.5526 (0.5532)\tAccu 0.7266 (0.7331)\t\n",
            "Epoch: [70][4/10]\tTime 0.266 (0.239)\tData 0.0495 (0.0490)\tLoss 0.5889 (0.5621)\tAccu 0.7070 (0.7266)\t\n",
            "Epoch: [70][5/10]\tTime 0.268 (0.244)\tData 0.0489 (0.0490)\tLoss 0.5577 (0.5613)\tAccu 0.7305 (0.7273)\t\n",
            "Epoch: [70][6/10]\tTime 0.266 (0.248)\tData 0.0492 (0.0490)\tLoss 0.6562 (0.5771)\tAccu 0.6523 (0.7148)\t\n",
            "Epoch: [70][7/10]\tTime 0.267 (0.251)\tData 0.0501 (0.0492)\tLoss 0.5449 (0.5725)\tAccu 0.6914 (0.7115)\t\n",
            "Epoch: [70][8/10]\tTime 0.268 (0.253)\tData 0.0489 (0.0491)\tLoss 0.5607 (0.5710)\tAccu 0.7227 (0.7129)\t\n",
            "Epoch: [70][9/10]\tTime 0.270 (0.255)\tData 0.0488 (0.0491)\tLoss 0.5920 (0.5733)\tAccu 0.6875 (0.7101)\t\n",
            "Epoch: [70][10/10]\tTime 0.243 (0.254)\tData 0.0378 (0.0480)\tLoss 0.6259 (0.5775)\tAccu 0.6684 (0.7068)\t\n",
            "Time 0.098\tAccu 0.6520\tLoss 0.6297\t\n",
            "Epoch: [71][1/10]\tTime 0.156 (0.156)\tData 0.0475 (0.0475)\tLoss 0.5909 (0.5909)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [71][2/10]\tTime 0.267 (0.212)\tData 0.0493 (0.0484)\tLoss 0.5744 (0.5827)\tAccu 0.7188 (0.6973)\t\n",
            "Epoch: [71][3/10]\tTime 0.268 (0.230)\tData 0.0488 (0.0485)\tLoss 0.5514 (0.5722)\tAccu 0.7266 (0.7070)\t\n",
            "Epoch: [71][4/10]\tTime 0.266 (0.239)\tData 0.0490 (0.0486)\tLoss 0.5641 (0.5702)\tAccu 0.6914 (0.7031)\t\n",
            "Epoch: [71][5/10]\tTime 0.267 (0.245)\tData 0.0489 (0.0487)\tLoss 0.5682 (0.5698)\tAccu 0.6875 (0.7000)\t\n",
            "Epoch: [71][6/10]\tTime 0.269 (0.249)\tData 0.0491 (0.0488)\tLoss 0.5692 (0.5697)\tAccu 0.7031 (0.7005)\t\n",
            "Epoch: [71][7/10]\tTime 0.267 (0.252)\tData 0.0494 (0.0488)\tLoss 0.5472 (0.5665)\tAccu 0.7305 (0.7048)\t\n",
            "Epoch: [71][8/10]\tTime 0.268 (0.254)\tData 0.0487 (0.0488)\tLoss 0.6047 (0.5713)\tAccu 0.6719 (0.7007)\t\n",
            "Epoch: [71][9/10]\tTime 0.266 (0.255)\tData 0.0488 (0.0488)\tLoss 0.5873 (0.5731)\tAccu 0.6875 (0.6992)\t\n",
            "Epoch: [71][10/10]\tTime 0.242 (0.254)\tData 0.0378 (0.0477)\tLoss 0.5744 (0.5732)\tAccu 0.6939 (0.6988)\t\n",
            "Time 0.098\tAccu 0.6520\tLoss 0.6355\t\n",
            "Epoch: [72][1/10]\tTime 0.154 (0.154)\tData 0.0477 (0.0477)\tLoss 0.5286 (0.5286)\tAccu 0.7500 (0.7500)\t\n",
            "Epoch: [72][2/10]\tTime 0.269 (0.212)\tData 0.0494 (0.0486)\tLoss 0.6063 (0.5674)\tAccu 0.6875 (0.7188)\t\n",
            "Epoch: [72][3/10]\tTime 0.266 (0.230)\tData 0.0526 (0.0499)\tLoss 0.5637 (0.5662)\tAccu 0.7344 (0.7240)\t\n",
            "Epoch: [72][4/10]\tTime 0.268 (0.239)\tData 0.0504 (0.0500)\tLoss 0.6191 (0.5794)\tAccu 0.6523 (0.7061)\t\n",
            "Epoch: [72][5/10]\tTime 0.268 (0.245)\tData 0.0493 (0.0499)\tLoss 0.5853 (0.5806)\tAccu 0.6914 (0.7031)\t\n",
            "Epoch: [72][6/10]\tTime 0.266 (0.249)\tData 0.0493 (0.0498)\tLoss 0.5689 (0.5787)\tAccu 0.7422 (0.7096)\t\n",
            "Epoch: [72][7/10]\tTime 0.267 (0.251)\tData 0.0489 (0.0497)\tLoss 0.6087 (0.5829)\tAccu 0.6758 (0.7048)\t\n",
            "Epoch: [72][8/10]\tTime 0.267 (0.253)\tData 0.0487 (0.0496)\tLoss 0.5918 (0.5841)\tAccu 0.6797 (0.7017)\t\n",
            "Epoch: [72][9/10]\tTime 0.266 (0.255)\tData 0.0478 (0.0494)\tLoss 0.5486 (0.5801)\tAccu 0.7383 (0.7057)\t\n",
            "Epoch: [72][10/10]\tTime 0.241 (0.253)\tData 0.0372 (0.0481)\tLoss 0.5828 (0.5803)\tAccu 0.6837 (0.7040)\t\n",
            "Time 0.098\tAccu 0.6500\tLoss 0.6405\t\n",
            "Epoch: [73][1/10]\tTime 0.153 (0.153)\tData 0.0464 (0.0464)\tLoss 0.5028 (0.5028)\tAccu 0.7656 (0.7656)\t\n",
            "Epoch: [73][2/10]\tTime 0.267 (0.210)\tData 0.0481 (0.0472)\tLoss 0.5706 (0.5367)\tAccu 0.6992 (0.7324)\t\n",
            "Epoch: [73][3/10]\tTime 0.273 (0.231)\tData 0.0477 (0.0474)\tLoss 0.6328 (0.5687)\tAccu 0.6602 (0.7083)\t\n",
            "Epoch: [73][4/10]\tTime 0.266 (0.240)\tData 0.0480 (0.0476)\tLoss 0.6110 (0.5793)\tAccu 0.6641 (0.6973)\t\n",
            "Epoch: [73][5/10]\tTime 0.267 (0.245)\tData 0.0488 (0.0478)\tLoss 0.5592 (0.5753)\tAccu 0.7266 (0.7031)\t\n",
            "Epoch: [73][6/10]\tTime 0.266 (0.249)\tData 0.0494 (0.0481)\tLoss 0.6182 (0.5824)\tAccu 0.6562 (0.6953)\t\n",
            "Epoch: [73][7/10]\tTime 0.268 (0.251)\tData 0.0489 (0.0482)\tLoss 0.5961 (0.5844)\tAccu 0.7109 (0.6975)\t\n",
            "Epoch: [73][8/10]\tTime 0.267 (0.253)\tData 0.0491 (0.0483)\tLoss 0.5670 (0.5822)\tAccu 0.7109 (0.6992)\t\n",
            "Epoch: [73][9/10]\tTime 0.266 (0.255)\tData 0.0487 (0.0483)\tLoss 0.5597 (0.5797)\tAccu 0.6914 (0.6984)\t\n",
            "Epoch: [73][10/10]\tTime 0.241 (0.253)\tData 0.0399 (0.0475)\tLoss 0.5192 (0.5750)\tAccu 0.7551 (0.7028)\t\n",
            "Time 0.099\tAccu 0.6520\tLoss 0.6639\t\n",
            "Epoch: [74][1/10]\tTime 0.155 (0.155)\tData 0.0488 (0.0488)\tLoss 0.5607 (0.5607)\tAccu 0.6992 (0.6992)\t\n",
            "Epoch: [74][2/10]\tTime 0.267 (0.211)\tData 0.0483 (0.0486)\tLoss 0.5904 (0.5756)\tAccu 0.6953 (0.6973)\t\n",
            "Epoch: [74][3/10]\tTime 0.265 (0.229)\tData 0.0480 (0.0484)\tLoss 0.5414 (0.5642)\tAccu 0.6992 (0.6979)\t\n",
            "Epoch: [74][4/10]\tTime 0.265 (0.238)\tData 0.0523 (0.0494)\tLoss 0.5936 (0.5715)\tAccu 0.6875 (0.6953)\t\n",
            "Epoch: [74][5/10]\tTime 0.266 (0.244)\tData 0.0491 (0.0493)\tLoss 0.5600 (0.5692)\tAccu 0.7461 (0.7055)\t\n",
            "Epoch: [74][6/10]\tTime 0.267 (0.248)\tData 0.0487 (0.0492)\tLoss 0.6041 (0.5750)\tAccu 0.6641 (0.6986)\t\n",
            "Epoch: [74][7/10]\tTime 0.267 (0.251)\tData 0.0484 (0.0491)\tLoss 0.5746 (0.5750)\tAccu 0.6914 (0.6975)\t\n",
            "Epoch: [74][8/10]\tTime 0.267 (0.253)\tData 0.0522 (0.0495)\tLoss 0.5594 (0.5730)\tAccu 0.7148 (0.6997)\t\n",
            "Epoch: [74][9/10]\tTime 0.267 (0.254)\tData 0.0483 (0.0493)\tLoss 0.5733 (0.5731)\tAccu 0.7070 (0.7005)\t\n",
            "Epoch: [74][10/10]\tTime 0.242 (0.253)\tData 0.0378 (0.0482)\tLoss 0.5868 (0.5741)\tAccu 0.6837 (0.6992)\t\n",
            "Time 0.100\tAccu 0.6480\tLoss 0.6486\t\n",
            "Epoch: [75][1/10]\tTime 0.155 (0.155)\tData 0.0483 (0.0483)\tLoss 0.5355 (0.5355)\tAccu 0.7188 (0.7188)\t\n",
            "Epoch: [75][2/10]\tTime 0.270 (0.212)\tData 0.0530 (0.0507)\tLoss 0.5360 (0.5357)\tAccu 0.7148 (0.7168)\t\n",
            "Epoch: [75][3/10]\tTime 0.267 (0.231)\tData 0.0495 (0.0503)\tLoss 0.5891 (0.5535)\tAccu 0.7070 (0.7135)\t\n",
            "Epoch: [75][4/10]\tTime 0.273 (0.241)\tData 0.0486 (0.0499)\tLoss 0.5338 (0.5486)\tAccu 0.7461 (0.7217)\t\n",
            "Epoch: [75][5/10]\tTime 0.267 (0.246)\tData 0.0493 (0.0497)\tLoss 0.5657 (0.5520)\tAccu 0.7148 (0.7203)\t\n",
            "Epoch: [75][6/10]\tTime 0.266 (0.250)\tData 0.0489 (0.0496)\tLoss 0.5943 (0.5590)\tAccu 0.7148 (0.7194)\t\n",
            "Epoch: [75][7/10]\tTime 0.267 (0.252)\tData 0.0485 (0.0494)\tLoss 0.6096 (0.5663)\tAccu 0.6719 (0.7126)\t\n",
            "Epoch: [75][8/10]\tTime 0.267 (0.254)\tData 0.0487 (0.0493)\tLoss 0.5737 (0.5672)\tAccu 0.7188 (0.7134)\t\n",
            "Epoch: [75][9/10]\tTime 0.267 (0.256)\tData 0.0507 (0.0495)\tLoss 0.5467 (0.5649)\tAccu 0.7188 (0.7140)\t\n",
            "Epoch: [75][10/10]\tTime 0.242 (0.254)\tData 0.0377 (0.0483)\tLoss 0.5849 (0.5665)\tAccu 0.6684 (0.7104)\t\n",
            "Time 0.098\tAccu 0.6640\tLoss 0.6386\t\n",
            "Epoch: [76][1/10]\tTime 0.159 (0.159)\tData 0.0527 (0.0527)\tLoss 0.5582 (0.5582)\tAccu 0.7188 (0.7188)\t\n",
            "Epoch: [76][2/10]\tTime 0.268 (0.214)\tData 0.0490 (0.0508)\tLoss 0.5951 (0.5766)\tAccu 0.6914 (0.7051)\t\n",
            "Epoch: [76][3/10]\tTime 0.266 (0.231)\tData 0.0491 (0.0502)\tLoss 0.5266 (0.5600)\tAccu 0.7383 (0.7161)\t\n",
            "Epoch: [76][4/10]\tTime 0.267 (0.240)\tData 0.0490 (0.0499)\tLoss 0.5537 (0.5584)\tAccu 0.7031 (0.7129)\t\n",
            "Epoch: [76][5/10]\tTime 0.269 (0.246)\tData 0.0489 (0.0497)\tLoss 0.5990 (0.5665)\tAccu 0.6602 (0.7023)\t\n",
            "Epoch: [76][6/10]\tTime 0.266 (0.249)\tData 0.0489 (0.0496)\tLoss 0.5418 (0.5624)\tAccu 0.7148 (0.7044)\t\n",
            "Epoch: [76][7/10]\tTime 0.269 (0.252)\tData 0.0493 (0.0496)\tLoss 0.5685 (0.5633)\tAccu 0.6914 (0.7026)\t\n",
            "Epoch: [76][8/10]\tTime 0.266 (0.254)\tData 0.0488 (0.0495)\tLoss 0.5402 (0.5604)\tAccu 0.7383 (0.7070)\t\n",
            "Epoch: [76][9/10]\tTime 0.268 (0.255)\tData 0.0521 (0.0498)\tLoss 0.5835 (0.5630)\tAccu 0.6758 (0.7036)\t\n",
            "Epoch: [76][10/10]\tTime 0.241 (0.254)\tData 0.0374 (0.0485)\tLoss 0.5759 (0.5640)\tAccu 0.7194 (0.7048)\t\n",
            "Time 0.098\tAccu 0.6460\tLoss 0.6449\t\n",
            "Epoch: [77][1/10]\tTime 0.157 (0.157)\tData 0.0473 (0.0473)\tLoss 0.5688 (0.5688)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [77][2/10]\tTime 0.266 (0.211)\tData 0.0500 (0.0486)\tLoss 0.5738 (0.5713)\tAccu 0.6953 (0.7051)\t\n",
            "Epoch: [77][3/10]\tTime 0.269 (0.231)\tData 0.0491 (0.0488)\tLoss 0.5585 (0.5671)\tAccu 0.6953 (0.7018)\t\n",
            "Epoch: [77][4/10]\tTime 0.267 (0.240)\tData 0.0494 (0.0489)\tLoss 0.5271 (0.5571)\tAccu 0.7656 (0.7178)\t\n",
            "Epoch: [77][5/10]\tTime 0.266 (0.245)\tData 0.0487 (0.0489)\tLoss 0.5587 (0.5574)\tAccu 0.7109 (0.7164)\t\n",
            "Epoch: [77][6/10]\tTime 0.268 (0.249)\tData 0.0497 (0.0490)\tLoss 0.6050 (0.5653)\tAccu 0.6953 (0.7129)\t\n",
            "Epoch: [77][7/10]\tTime 0.267 (0.251)\tData 0.0491 (0.0490)\tLoss 0.5506 (0.5632)\tAccu 0.7109 (0.7126)\t\n",
            "Epoch: [77][8/10]\tTime 0.266 (0.253)\tData 0.0493 (0.0491)\tLoss 0.5610 (0.5629)\tAccu 0.6992 (0.7109)\t\n",
            "Epoch: [77][9/10]\tTime 0.271 (0.255)\tData 0.0485 (0.0490)\tLoss 0.5600 (0.5626)\tAccu 0.7031 (0.7101)\t\n",
            "Epoch: [77][10/10]\tTime 0.241 (0.254)\tData 0.0380 (0.0479)\tLoss 0.5714 (0.5633)\tAccu 0.7041 (0.7096)\t\n",
            "Time 0.099\tAccu 0.6620\tLoss 0.6449\t\n",
            "Epoch: [78][1/10]\tTime 0.155 (0.155)\tData 0.0480 (0.0480)\tLoss 0.5353 (0.5353)\tAccu 0.7031 (0.7031)\t\n",
            "Epoch: [78][2/10]\tTime 0.270 (0.212)\tData 0.0513 (0.0497)\tLoss 0.5366 (0.5360)\tAccu 0.7461 (0.7246)\t\n",
            "Epoch: [78][3/10]\tTime 0.266 (0.230)\tData 0.0493 (0.0496)\tLoss 0.5823 (0.5514)\tAccu 0.6758 (0.7083)\t\n",
            "Epoch: [78][4/10]\tTime 0.268 (0.240)\tData 0.0486 (0.0493)\tLoss 0.6474 (0.5754)\tAccu 0.6406 (0.6914)\t\n",
            "Epoch: [78][5/10]\tTime 0.267 (0.245)\tData 0.0488 (0.0492)\tLoss 0.5916 (0.5787)\tAccu 0.6875 (0.6906)\t\n",
            "Epoch: [78][6/10]\tTime 0.267 (0.249)\tData 0.0497 (0.0493)\tLoss 0.5808 (0.5790)\tAccu 0.7148 (0.6947)\t\n",
            "Epoch: [78][7/10]\tTime 0.268 (0.251)\tData 0.0485 (0.0492)\tLoss 0.5595 (0.5762)\tAccu 0.6914 (0.6942)\t\n",
            "Epoch: [78][8/10]\tTime 0.268 (0.254)\tData 0.0523 (0.0496)\tLoss 0.5749 (0.5761)\tAccu 0.6953 (0.6943)\t\n",
            "Epoch: [78][9/10]\tTime 0.265 (0.255)\tData 0.0514 (0.0498)\tLoss 0.5441 (0.5725)\tAccu 0.7031 (0.6953)\t\n",
            "Epoch: [78][10/10]\tTime 0.242 (0.253)\tData 0.0384 (0.0487)\tLoss 0.5377 (0.5698)\tAccu 0.7449 (0.6992)\t\n",
            "Time 0.099\tAccu 0.6560\tLoss 0.6423\t\n",
            "Epoch: [79][1/10]\tTime 0.155 (0.155)\tData 0.0476 (0.0476)\tLoss 0.5722 (0.5722)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [79][2/10]\tTime 0.268 (0.212)\tData 0.0493 (0.0484)\tLoss 0.5576 (0.5649)\tAccu 0.7031 (0.6895)\t\n",
            "Epoch: [79][3/10]\tTime 0.267 (0.230)\tData 0.0537 (0.0502)\tLoss 0.5633 (0.5644)\tAccu 0.7109 (0.6966)\t\n",
            "Epoch: [79][4/10]\tTime 0.268 (0.240)\tData 0.0492 (0.0499)\tLoss 0.5834 (0.5691)\tAccu 0.6758 (0.6914)\t\n",
            "Epoch: [79][5/10]\tTime 0.266 (0.245)\tData 0.0492 (0.0498)\tLoss 0.5502 (0.5653)\tAccu 0.7344 (0.7000)\t\n",
            "Epoch: [79][6/10]\tTime 0.268 (0.249)\tData 0.0496 (0.0498)\tLoss 0.5686 (0.5659)\tAccu 0.7070 (0.7012)\t\n",
            "Epoch: [79][7/10]\tTime 0.267 (0.251)\tData 0.0504 (0.0498)\tLoss 0.5361 (0.5616)\tAccu 0.7422 (0.7070)\t\n",
            "Epoch: [79][8/10]\tTime 0.267 (0.253)\tData 0.0489 (0.0497)\tLoss 0.5306 (0.5577)\tAccu 0.7461 (0.7119)\t\n",
            "Epoch: [79][9/10]\tTime 0.269 (0.255)\tData 0.0494 (0.0497)\tLoss 0.5944 (0.5618)\tAccu 0.7109 (0.7118)\t\n",
            "Epoch: [79][10/10]\tTime 0.242 (0.254)\tData 0.0380 (0.0485)\tLoss 0.5621 (0.5618)\tAccu 0.7092 (0.7116)\t\n",
            "Time 0.098\tAccu 0.6540\tLoss 0.6404\t\n",
            "Epoch: [80][1/10]\tTime 0.154 (0.154)\tData 0.0478 (0.0478)\tLoss 0.5526 (0.5526)\tAccu 0.7070 (0.7070)\t\n",
            "Epoch: [80][2/10]\tTime 0.268 (0.211)\tData 0.0501 (0.0489)\tLoss 0.5348 (0.5437)\tAccu 0.7344 (0.7207)\t\n",
            "Epoch: [80][3/10]\tTime 0.270 (0.231)\tData 0.0495 (0.0491)\tLoss 0.5917 (0.5597)\tAccu 0.6719 (0.7044)\t\n",
            "Epoch: [80][4/10]\tTime 0.265 (0.239)\tData 0.0495 (0.0492)\tLoss 0.5989 (0.5695)\tAccu 0.6562 (0.6924)\t\n",
            "Epoch: [80][5/10]\tTime 0.265 (0.244)\tData 0.0493 (0.0492)\tLoss 0.5542 (0.5664)\tAccu 0.7188 (0.6977)\t\n",
            "Epoch: [80][6/10]\tTime 0.267 (0.248)\tData 0.0487 (0.0491)\tLoss 0.5726 (0.5675)\tAccu 0.7188 (0.7012)\t\n",
            "Epoch: [80][7/10]\tTime 0.270 (0.251)\tData 0.0495 (0.0492)\tLoss 0.5645 (0.5670)\tAccu 0.6875 (0.6992)\t\n",
            "Epoch: [80][8/10]\tTime 0.267 (0.253)\tData 0.0491 (0.0492)\tLoss 0.5219 (0.5614)\tAccu 0.7344 (0.7036)\t\n",
            "Epoch: [80][9/10]\tTime 0.266 (0.255)\tData 0.0487 (0.0491)\tLoss 0.5272 (0.5576)\tAccu 0.7109 (0.7044)\t\n",
            "Epoch: [80][10/10]\tTime 0.242 (0.253)\tData 0.0382 (0.0480)\tLoss 0.5430 (0.5564)\tAccu 0.7296 (0.7064)\t\n",
            "Time 0.099\tAccu 0.6500\tLoss 0.6349\t\n",
            "Epoch: [81][1/10]\tTime 0.155 (0.155)\tData 0.0475 (0.0475)\tLoss 0.5590 (0.5590)\tAccu 0.6836 (0.6836)\t\n",
            "Epoch: [81][2/10]\tTime 0.267 (0.211)\tData 0.0499 (0.0487)\tLoss 0.5546 (0.5568)\tAccu 0.7109 (0.6973)\t\n",
            "Epoch: [81][3/10]\tTime 0.268 (0.230)\tData 0.0494 (0.0489)\tLoss 0.5665 (0.5600)\tAccu 0.7422 (0.7122)\t\n",
            "Epoch: [81][4/10]\tTime 0.266 (0.239)\tData 0.0504 (0.0493)\tLoss 0.5345 (0.5536)\tAccu 0.7344 (0.7178)\t\n",
            "Epoch: [81][5/10]\tTime 0.267 (0.245)\tData 0.0496 (0.0493)\tLoss 0.5627 (0.5554)\tAccu 0.7109 (0.7164)\t\n",
            "Epoch: [81][6/10]\tTime 0.267 (0.249)\tData 0.0495 (0.0494)\tLoss 0.5524 (0.5549)\tAccu 0.7422 (0.7207)\t\n",
            "Epoch: [81][7/10]\tTime 0.266 (0.251)\tData 0.0488 (0.0493)\tLoss 0.5521 (0.5545)\tAccu 0.6992 (0.7176)\t\n",
            "Epoch: [81][8/10]\tTime 0.265 (0.253)\tData 0.0499 (0.0494)\tLoss 0.5470 (0.5536)\tAccu 0.7148 (0.7173)\t\n",
            "Epoch: [81][9/10]\tTime 0.267 (0.254)\tData 0.0495 (0.0494)\tLoss 0.5924 (0.5579)\tAccu 0.6875 (0.7140)\t\n",
            "Epoch: [81][10/10]\tTime 0.242 (0.253)\tData 0.0378 (0.0482)\tLoss 0.5596 (0.5580)\tAccu 0.7092 (0.7136)\t\n",
            "Time 0.099\tAccu 0.6580\tLoss 0.6393\t\n",
            "Epoch: [82][1/10]\tTime 0.155 (0.155)\tData 0.0478 (0.0478)\tLoss 0.5324 (0.5324)\tAccu 0.7344 (0.7344)\t\n",
            "Epoch: [82][2/10]\tTime 0.269 (0.212)\tData 0.0489 (0.0484)\tLoss 0.4849 (0.5087)\tAccu 0.7578 (0.7461)\t\n",
            "Epoch: [82][3/10]\tTime 0.266 (0.230)\tData 0.0483 (0.0483)\tLoss 0.6393 (0.5522)\tAccu 0.6484 (0.7135)\t\n",
            "Epoch: [82][4/10]\tTime 0.266 (0.239)\tData 0.0493 (0.0486)\tLoss 0.5713 (0.5570)\tAccu 0.6992 (0.7100)\t\n",
            "Epoch: [82][5/10]\tTime 0.268 (0.245)\tData 0.0487 (0.0486)\tLoss 0.5542 (0.5564)\tAccu 0.7227 (0.7125)\t\n",
            "Epoch: [82][6/10]\tTime 0.266 (0.248)\tData 0.0483 (0.0485)\tLoss 0.5598 (0.5570)\tAccu 0.7500 (0.7188)\t\n",
            "Epoch: [82][7/10]\tTime 0.266 (0.251)\tData 0.0505 (0.0488)\tLoss 0.5620 (0.5577)\tAccu 0.7188 (0.7188)\t\n",
            "Epoch: [82][8/10]\tTime 0.269 (0.253)\tData 0.0487 (0.0488)\tLoss 0.5468 (0.5563)\tAccu 0.7227 (0.7192)\t\n",
            "Epoch: [82][9/10]\tTime 0.266 (0.255)\tData 0.0492 (0.0489)\tLoss 0.5776 (0.5587)\tAccu 0.6914 (0.7161)\t\n",
            "Epoch: [82][10/10]\tTime 0.241 (0.253)\tData 0.0380 (0.0478)\tLoss 0.5806 (0.5604)\tAccu 0.6786 (0.7132)\t\n",
            "Time 0.099\tAccu 0.6700\tLoss 0.6536\t\n",
            "Epoch: [83][1/10]\tTime 0.160 (0.160)\tData 0.0490 (0.0490)\tLoss 0.6037 (0.6037)\tAccu 0.6641 (0.6641)\t\n",
            "Epoch: [83][2/10]\tTime 0.266 (0.213)\tData 0.0501 (0.0495)\tLoss 0.5268 (0.5652)\tAccu 0.7500 (0.7070)\t\n",
            "Epoch: [83][3/10]\tTime 0.267 (0.231)\tData 0.0490 (0.0494)\tLoss 0.5830 (0.5712)\tAccu 0.6953 (0.7031)\t\n",
            "Epoch: [83][4/10]\tTime 0.266 (0.240)\tData 0.0510 (0.0498)\tLoss 0.5734 (0.5717)\tAccu 0.7266 (0.7090)\t\n",
            "Epoch: [83][5/10]\tTime 0.267 (0.245)\tData 0.0508 (0.0500)\tLoss 0.5529 (0.5680)\tAccu 0.7305 (0.7133)\t\n",
            "Epoch: [83][6/10]\tTime 0.266 (0.249)\tData 0.0498 (0.0499)\tLoss 0.5471 (0.5645)\tAccu 0.7148 (0.7135)\t\n",
            "Epoch: [83][7/10]\tTime 0.267 (0.251)\tData 0.0495 (0.0499)\tLoss 0.5636 (0.5643)\tAccu 0.6914 (0.7104)\t\n",
            "Epoch: [83][8/10]\tTime 0.270 (0.254)\tData 0.0497 (0.0499)\tLoss 0.5753 (0.5657)\tAccu 0.6953 (0.7085)\t\n",
            "Epoch: [83][9/10]\tTime 0.266 (0.255)\tData 0.0492 (0.0498)\tLoss 0.5337 (0.5622)\tAccu 0.7383 (0.7118)\t\n",
            "Epoch: [83][10/10]\tTime 0.241 (0.254)\tData 0.0379 (0.0486)\tLoss 0.5489 (0.5611)\tAccu 0.7245 (0.7128)\t\n",
            "Time 0.098\tAccu 0.6560\tLoss 0.6321\t\n",
            "Epoch: [84][1/10]\tTime 0.155 (0.155)\tData 0.0492 (0.0492)\tLoss 0.5451 (0.5451)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [84][2/10]\tTime 0.268 (0.212)\tData 0.0508 (0.0500)\tLoss 0.5623 (0.5537)\tAccu 0.6953 (0.6953)\t\n",
            "Epoch: [84][3/10]\tTime 0.266 (0.230)\tData 0.0493 (0.0498)\tLoss 0.5657 (0.5577)\tAccu 0.7266 (0.7057)\t\n",
            "Epoch: [84][4/10]\tTime 0.267 (0.239)\tData 0.0483 (0.0494)\tLoss 0.5456 (0.5547)\tAccu 0.7422 (0.7148)\t\n",
            "Epoch: [84][5/10]\tTime 0.269 (0.245)\tData 0.0500 (0.0495)\tLoss 0.5600 (0.5557)\tAccu 0.7070 (0.7133)\t\n",
            "Epoch: [84][6/10]\tTime 0.265 (0.248)\tData 0.0507 (0.0497)\tLoss 0.5787 (0.5596)\tAccu 0.6719 (0.7064)\t\n",
            "Epoch: [84][7/10]\tTime 0.267 (0.251)\tData 0.0490 (0.0496)\tLoss 0.5622 (0.5599)\tAccu 0.6836 (0.7031)\t\n",
            "Epoch: [84][8/10]\tTime 0.267 (0.253)\tData 0.0492 (0.0496)\tLoss 0.5202 (0.5550)\tAccu 0.7383 (0.7075)\t\n",
            "Epoch: [84][9/10]\tTime 0.267 (0.255)\tData 0.0499 (0.0496)\tLoss 0.5834 (0.5581)\tAccu 0.6914 (0.7057)\t\n",
            "Epoch: [84][10/10]\tTime 0.242 (0.253)\tData 0.0380 (0.0485)\tLoss 0.4888 (0.5527)\tAccu 0.8214 (0.7148)\t\n",
            "Time 0.099\tAccu 0.6520\tLoss 0.6337\t\n",
            "Epoch: [85][1/10]\tTime 0.155 (0.155)\tData 0.0478 (0.0478)\tLoss 0.5480 (0.5480)\tAccu 0.7266 (0.7266)\t\n",
            "Epoch: [85][2/10]\tTime 0.273 (0.214)\tData 0.0492 (0.0485)\tLoss 0.5284 (0.5382)\tAccu 0.7188 (0.7227)\t\n",
            "Epoch: [85][3/10]\tTime 0.267 (0.232)\tData 0.0488 (0.0486)\tLoss 0.5602 (0.5455)\tAccu 0.6992 (0.7148)\t\n",
            "Epoch: [85][4/10]\tTime 0.268 (0.241)\tData 0.0488 (0.0487)\tLoss 0.5814 (0.5545)\tAccu 0.6914 (0.7090)\t\n",
            "Epoch: [85][5/10]\tTime 0.268 (0.246)\tData 0.0500 (0.0489)\tLoss 0.5611 (0.5558)\tAccu 0.6875 (0.7047)\t\n",
            "Epoch: [85][6/10]\tTime 0.266 (0.250)\tData 0.0492 (0.0490)\tLoss 0.5435 (0.5538)\tAccu 0.7305 (0.7090)\t\n",
            "Epoch: [85][7/10]\tTime 0.267 (0.252)\tData 0.0490 (0.0490)\tLoss 0.5517 (0.5535)\tAccu 0.7266 (0.7115)\t\n",
            "Epoch: [85][8/10]\tTime 0.267 (0.254)\tData 0.0495 (0.0490)\tLoss 0.5884 (0.5578)\tAccu 0.6875 (0.7085)\t\n",
            "Epoch: [85][9/10]\tTime 0.266 (0.255)\tData 0.0491 (0.0490)\tLoss 0.5049 (0.5520)\tAccu 0.7578 (0.7140)\t\n",
            "Epoch: [85][10/10]\tTime 0.241 (0.254)\tData 0.0377 (0.0479)\tLoss 0.5827 (0.5544)\tAccu 0.6837 (0.7116)\t\n",
            "Time 0.099\tAccu 0.6540\tLoss 0.6385\t\n",
            "Epoch: [86][1/10]\tTime 0.155 (0.155)\tData 0.0476 (0.0476)\tLoss 0.5623 (0.5623)\tAccu 0.7109 (0.7109)\t\n",
            "Epoch: [86][2/10]\tTime 0.267 (0.211)\tData 0.0496 (0.0486)\tLoss 0.5657 (0.5640)\tAccu 0.6875 (0.6992)\t\n",
            "Epoch: [86][3/10]\tTime 0.269 (0.230)\tData 0.0488 (0.0486)\tLoss 0.5570 (0.5617)\tAccu 0.7148 (0.7044)\t\n",
            "Epoch: [86][4/10]\tTime 0.268 (0.240)\tData 0.0485 (0.0486)\tLoss 0.5227 (0.5519)\tAccu 0.7461 (0.7148)\t\n",
            "Epoch: [86][5/10]\tTime 0.265 (0.245)\tData 0.0495 (0.0488)\tLoss 0.4976 (0.5411)\tAccu 0.7539 (0.7227)\t\n",
            "Epoch: [86][6/10]\tTime 0.268 (0.249)\tData 0.0500 (0.0490)\tLoss 0.5456 (0.5418)\tAccu 0.7266 (0.7233)\t\n",
            "Epoch: [86][7/10]\tTime 0.268 (0.251)\tData 0.0520 (0.0494)\tLoss 0.6009 (0.5503)\tAccu 0.6602 (0.7143)\t\n",
            "Epoch: [86][8/10]\tTime 0.266 (0.253)\tData 0.0489 (0.0494)\tLoss 0.5968 (0.5561)\tAccu 0.6758 (0.7095)\t\n",
            "Epoch: [86][9/10]\tTime 0.269 (0.255)\tData 0.0486 (0.0493)\tLoss 0.5423 (0.5546)\tAccu 0.7227 (0.7109)\t\n",
            "Epoch: [86][10/10]\tTime 0.244 (0.254)\tData 0.0382 (0.0482)\tLoss 0.5843 (0.5569)\tAccu 0.7092 (0.7108)\t\n",
            "Time 0.098\tAccu 0.6300\tLoss 0.6370\t\n",
            "Epoch: [87][1/10]\tTime 0.154 (0.154)\tData 0.0470 (0.0470)\tLoss 0.5888 (0.5888)\tAccu 0.6758 (0.6758)\t\n",
            "Epoch: [87][2/10]\tTime 0.269 (0.212)\tData 0.0495 (0.0482)\tLoss 0.5686 (0.5787)\tAccu 0.7109 (0.6934)\t\n",
            "Epoch: [87][3/10]\tTime 0.275 (0.233)\tData 0.0499 (0.0488)\tLoss 0.5231 (0.5602)\tAccu 0.7734 (0.7201)\t\n",
            "Epoch: [87][4/10]\tTime 0.267 (0.242)\tData 0.0505 (0.0492)\tLoss 0.5495 (0.5575)\tAccu 0.7148 (0.7188)\t\n",
            "Epoch: [87][5/10]\tTime 0.267 (0.247)\tData 0.0491 (0.0492)\tLoss 0.5810 (0.5622)\tAccu 0.6758 (0.7102)\t\n",
            "Epoch: [87][6/10]\tTime 0.267 (0.250)\tData 0.0488 (0.0491)\tLoss 0.5788 (0.5650)\tAccu 0.6797 (0.7051)\t\n",
            "Epoch: [87][7/10]\tTime 0.267 (0.252)\tData 0.0488 (0.0491)\tLoss 0.5604 (0.5643)\tAccu 0.7109 (0.7059)\t\n",
            "Epoch: [87][8/10]\tTime 0.266 (0.254)\tData 0.0491 (0.0491)\tLoss 0.6035 (0.5692)\tAccu 0.6406 (0.6978)\t\n",
            "Epoch: [87][9/10]\tTime 0.268 (0.256)\tData 0.0496 (0.0491)\tLoss 0.5254 (0.5643)\tAccu 0.7656 (0.7053)\t\n",
            "Epoch: [87][10/10]\tTime 0.241 (0.254)\tData 0.0382 (0.0480)\tLoss 0.5099 (0.5601)\tAccu 0.7449 (0.7084)\t\n",
            "Time 0.100\tAccu 0.6560\tLoss 0.6988\t\n",
            "Epoch: [88][1/10]\tTime 0.155 (0.155)\tData 0.0482 (0.0482)\tLoss 0.6204 (0.6204)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [88][2/10]\tTime 0.267 (0.211)\tData 0.0507 (0.0494)\tLoss 0.5947 (0.6076)\tAccu 0.6797 (0.6797)\t\n",
            "Epoch: [88][3/10]\tTime 0.266 (0.229)\tData 0.0498 (0.0496)\tLoss 0.5478 (0.5877)\tAccu 0.7031 (0.6875)\t\n",
            "Epoch: [88][4/10]\tTime 0.266 (0.238)\tData 0.0496 (0.0496)\tLoss 0.5049 (0.5670)\tAccu 0.7617 (0.7061)\t\n",
            "Epoch: [88][5/10]\tTime 0.266 (0.244)\tData 0.0512 (0.0499)\tLoss 0.5722 (0.5680)\tAccu 0.7188 (0.7086)\t\n",
            "Epoch: [88][6/10]\tTime 0.266 (0.248)\tData 0.0483 (0.0496)\tLoss 0.5795 (0.5699)\tAccu 0.7227 (0.7109)\t\n",
            "Epoch: [88][7/10]\tTime 0.265 (0.250)\tData 0.0494 (0.0496)\tLoss 0.5476 (0.5667)\tAccu 0.7500 (0.7165)\t\n",
            "Epoch: [88][8/10]\tTime 0.267 (0.252)\tData 0.0512 (0.0498)\tLoss 0.5423 (0.5637)\tAccu 0.7227 (0.7173)\t\n",
            "Epoch: [88][9/10]\tTime 0.266 (0.254)\tData 0.0486 (0.0497)\tLoss 0.5928 (0.5669)\tAccu 0.6875 (0.7140)\t\n",
            "Epoch: [88][10/10]\tTime 0.243 (0.253)\tData 0.0379 (0.0485)\tLoss 0.5649 (0.5668)\tAccu 0.7092 (0.7136)\t\n",
            "Time 0.100\tAccu 0.6580\tLoss 0.6391\t\n",
            "Epoch: [89][1/10]\tTime 0.156 (0.156)\tData 0.0484 (0.0484)\tLoss 0.5861 (0.5861)\tAccu 0.7031 (0.7031)\t\n",
            "Epoch: [89][2/10]\tTime 0.269 (0.212)\tData 0.0492 (0.0488)\tLoss 0.5495 (0.5678)\tAccu 0.7461 (0.7246)\t\n",
            "Epoch: [89][3/10]\tTime 0.267 (0.231)\tData 0.0497 (0.0491)\tLoss 0.5982 (0.5779)\tAccu 0.7031 (0.7174)\t\n",
            "Epoch: [89][4/10]\tTime 0.267 (0.240)\tData 0.0489 (0.0491)\tLoss 0.5542 (0.5720)\tAccu 0.7305 (0.7207)\t\n",
            "Epoch: [89][5/10]\tTime 0.267 (0.245)\tData 0.0487 (0.0490)\tLoss 0.5468 (0.5670)\tAccu 0.7305 (0.7227)\t\n",
            "Epoch: [89][6/10]\tTime 0.268 (0.249)\tData 0.0489 (0.0490)\tLoss 0.5092 (0.5573)\tAccu 0.7500 (0.7272)\t\n",
            "Epoch: [89][7/10]\tTime 0.267 (0.251)\tData 0.0490 (0.0490)\tLoss 0.5778 (0.5603)\tAccu 0.7188 (0.7260)\t\n",
            "Epoch: [89][8/10]\tTime 0.273 (0.254)\tData 0.0492 (0.0490)\tLoss 0.5445 (0.5583)\tAccu 0.7188 (0.7251)\t\n",
            "Epoch: [89][9/10]\tTime 0.268 (0.256)\tData 0.0489 (0.0490)\tLoss 0.5667 (0.5592)\tAccu 0.7109 (0.7235)\t\n",
            "Epoch: [89][10/10]\tTime 0.241 (0.254)\tData 0.0381 (0.0479)\tLoss 0.5244 (0.5565)\tAccu 0.7602 (0.7264)\t\n",
            "Time 0.098\tAccu 0.6360\tLoss 0.6503\t\n",
            "Epoch: [90][1/10]\tTime 0.160 (0.160)\tData 0.0525 (0.0525)\tLoss 0.5489 (0.5489)\tAccu 0.7188 (0.7188)\t\n",
            "Epoch: [90][2/10]\tTime 0.267 (0.214)\tData 0.0503 (0.0514)\tLoss 0.5469 (0.5479)\tAccu 0.7266 (0.7227)\t\n",
            "Epoch: [90][3/10]\tTime 0.264 (0.231)\tData 0.0484 (0.0504)\tLoss 0.5637 (0.5531)\tAccu 0.7344 (0.7266)\t\n",
            "Epoch: [90][4/10]\tTime 0.266 (0.239)\tData 0.0483 (0.0499)\tLoss 0.5823 (0.5604)\tAccu 0.6914 (0.7178)\t\n",
            "Epoch: [90][5/10]\tTime 0.266 (0.245)\tData 0.0492 (0.0497)\tLoss 0.5441 (0.5572)\tAccu 0.7148 (0.7172)\t\n",
            "Epoch: [90][6/10]\tTime 0.266 (0.248)\tData 0.0486 (0.0496)\tLoss 0.5058 (0.5486)\tAccu 0.7461 (0.7220)\t\n",
            "Epoch: [90][7/10]\tTime 0.267 (0.251)\tData 0.0482 (0.0494)\tLoss 0.5419 (0.5476)\tAccu 0.7148 (0.7210)\t\n",
            "Epoch: [90][8/10]\tTime 0.265 (0.253)\tData 0.0481 (0.0492)\tLoss 0.5529 (0.5483)\tAccu 0.7148 (0.7202)\t\n",
            "Epoch: [90][9/10]\tTime 0.268 (0.254)\tData 0.0506 (0.0494)\tLoss 0.5294 (0.5462)\tAccu 0.7422 (0.7227)\t\n",
            "Epoch: [90][10/10]\tTime 0.241 (0.253)\tData 0.0377 (0.0482)\tLoss 0.5112 (0.5435)\tAccu 0.7092 (0.7216)\t\n",
            "Time 0.098\tAccu 0.6720\tLoss 0.6525\t\n",
            "Epoch: [91][1/10]\tTime 0.154 (0.154)\tData 0.0480 (0.0480)\tLoss 0.4919 (0.4919)\tAccu 0.7734 (0.7734)\t\n",
            "Epoch: [91][2/10]\tTime 0.268 (0.211)\tData 0.0535 (0.0508)\tLoss 0.5182 (0.5050)\tAccu 0.7188 (0.7461)\t\n",
            "Epoch: [91][3/10]\tTime 0.266 (0.229)\tData 0.0500 (0.0505)\tLoss 0.5540 (0.5214)\tAccu 0.7109 (0.7344)\t\n",
            "Epoch: [91][4/10]\tTime 0.267 (0.239)\tData 0.0488 (0.0501)\tLoss 0.5488 (0.5282)\tAccu 0.7383 (0.7354)\t\n",
            "Epoch: [91][5/10]\tTime 0.267 (0.244)\tData 0.0486 (0.0498)\tLoss 0.5280 (0.5282)\tAccu 0.7461 (0.7375)\t\n",
            "Epoch: [91][6/10]\tTime 0.267 (0.248)\tData 0.0495 (0.0497)\tLoss 0.5489 (0.5316)\tAccu 0.7344 (0.7370)\t\n",
            "Epoch: [91][7/10]\tTime 0.268 (0.251)\tData 0.0488 (0.0496)\tLoss 0.6009 (0.5415)\tAccu 0.6797 (0.7288)\t\n",
            "Epoch: [91][8/10]\tTime 0.266 (0.253)\tData 0.0493 (0.0496)\tLoss 0.5252 (0.5395)\tAccu 0.7266 (0.7285)\t\n",
            "Epoch: [91][9/10]\tTime 0.268 (0.254)\tData 0.0482 (0.0494)\tLoss 0.5269 (0.5381)\tAccu 0.7461 (0.7305)\t\n",
            "Epoch: [91][10/10]\tTime 0.242 (0.253)\tData 0.0382 (0.0483)\tLoss 0.5497 (0.5390)\tAccu 0.6990 (0.7280)\t\n",
            "Time 0.099\tAccu 0.6400\tLoss 0.6481\t\n",
            "Epoch: [92][1/10]\tTime 0.155 (0.155)\tData 0.0477 (0.0477)\tLoss 0.5144 (0.5144)\tAccu 0.7578 (0.7578)\t\n",
            "Epoch: [92][2/10]\tTime 0.272 (0.213)\tData 0.0495 (0.0486)\tLoss 0.5018 (0.5081)\tAccu 0.7695 (0.7637)\t\n",
            "Epoch: [92][3/10]\tTime 0.266 (0.231)\tData 0.0479 (0.0484)\tLoss 0.5743 (0.5302)\tAccu 0.6992 (0.7422)\t\n",
            "Epoch: [92][4/10]\tTime 0.269 (0.241)\tData 0.0483 (0.0483)\tLoss 0.5267 (0.5293)\tAccu 0.7344 (0.7402)\t\n",
            "Epoch: [92][5/10]\tTime 0.267 (0.246)\tData 0.0490 (0.0485)\tLoss 0.5529 (0.5340)\tAccu 0.7539 (0.7430)\t\n",
            "Epoch: [92][6/10]\tTime 0.267 (0.249)\tData 0.0490 (0.0486)\tLoss 0.5246 (0.5325)\tAccu 0.7383 (0.7422)\t\n",
            "Epoch: [92][7/10]\tTime 0.268 (0.252)\tData 0.0484 (0.0485)\tLoss 0.5770 (0.5388)\tAccu 0.6914 (0.7349)\t\n",
            "Epoch: [92][8/10]\tTime 0.265 (0.254)\tData 0.0482 (0.0485)\tLoss 0.5093 (0.5351)\tAccu 0.7617 (0.7383)\t\n",
            "Epoch: [92][9/10]\tTime 0.268 (0.255)\tData 0.0488 (0.0485)\tLoss 0.5337 (0.5350)\tAccu 0.7031 (0.7344)\t\n",
            "Epoch: [92][10/10]\tTime 0.243 (0.254)\tData 0.0377 (0.0474)\tLoss 0.5177 (0.5336)\tAccu 0.7194 (0.7332)\t\n",
            "Time 0.099\tAccu 0.6440\tLoss 0.6581\t\n",
            "Epoch: [93][1/10]\tTime 0.154 (0.154)\tData 0.0470 (0.0470)\tLoss 0.5102 (0.5102)\tAccu 0.7422 (0.7422)\t\n",
            "Epoch: [93][2/10]\tTime 0.268 (0.211)\tData 0.0491 (0.0481)\tLoss 0.5639 (0.5371)\tAccu 0.7305 (0.7363)\t\n",
            "Epoch: [93][3/10]\tTime 0.268 (0.230)\tData 0.0500 (0.0487)\tLoss 0.5454 (0.5398)\tAccu 0.7070 (0.7266)\t\n",
            "Epoch: [93][4/10]\tTime 0.267 (0.239)\tData 0.0496 (0.0489)\tLoss 0.5247 (0.5360)\tAccu 0.7500 (0.7324)\t\n",
            "Epoch: [93][5/10]\tTime 0.269 (0.245)\tData 0.0499 (0.0491)\tLoss 0.5183 (0.5325)\tAccu 0.7617 (0.7383)\t\n",
            "Epoch: [93][6/10]\tTime 0.268 (0.249)\tData 0.0487 (0.0490)\tLoss 0.5225 (0.5308)\tAccu 0.7305 (0.7370)\t\n",
            "Epoch: [93][7/10]\tTime 0.267 (0.251)\tData 0.0506 (0.0493)\tLoss 0.5443 (0.5328)\tAccu 0.7188 (0.7344)\t\n",
            "Epoch: [93][8/10]\tTime 0.268 (0.253)\tData 0.0492 (0.0493)\tLoss 0.5191 (0.5310)\tAccu 0.7305 (0.7339)\t\n",
            "Epoch: [93][9/10]\tTime 0.268 (0.255)\tData 0.0544 (0.0498)\tLoss 0.5452 (0.5326)\tAccu 0.7266 (0.7331)\t\n",
            "Epoch: [93][10/10]\tTime 0.244 (0.254)\tData 0.0380 (0.0486)\tLoss 0.5335 (0.5327)\tAccu 0.7551 (0.7348)\t\n",
            "Time 0.099\tAccu 0.6520\tLoss 0.6578\t\n",
            "Epoch: [94][1/10]\tTime 0.156 (0.156)\tData 0.0474 (0.0474)\tLoss 0.5180 (0.5180)\tAccu 0.7188 (0.7188)\t\n",
            "Epoch: [94][2/10]\tTime 0.268 (0.212)\tData 0.0510 (0.0492)\tLoss 0.4964 (0.5072)\tAccu 0.7422 (0.7305)\t\n",
            "Epoch: [94][3/10]\tTime 0.275 (0.233)\tData 0.0486 (0.0490)\tLoss 0.4923 (0.5022)\tAccu 0.7539 (0.7383)\t\n",
            "Epoch: [94][4/10]\tTime 0.267 (0.242)\tData 0.0486 (0.0489)\tLoss 0.5275 (0.5086)\tAccu 0.7578 (0.7432)\t\n",
            "Epoch: [94][5/10]\tTime 0.268 (0.247)\tData 0.0486 (0.0488)\tLoss 0.5227 (0.5114)\tAccu 0.7188 (0.7383)\t\n",
            "Epoch: [94][6/10]\tTime 0.266 (0.250)\tData 0.0492 (0.0489)\tLoss 0.5186 (0.5126)\tAccu 0.7617 (0.7422)\t\n",
            "Epoch: [94][7/10]\tTime 0.271 (0.253)\tData 0.0492 (0.0489)\tLoss 0.5388 (0.5163)\tAccu 0.7266 (0.7400)\t\n",
            "Epoch: [94][8/10]\tTime 0.267 (0.255)\tData 0.0489 (0.0489)\tLoss 0.5583 (0.5216)\tAccu 0.6914 (0.7339)\t\n",
            "Epoch: [94][9/10]\tTime 0.268 (0.256)\tData 0.0495 (0.0490)\tLoss 0.5517 (0.5249)\tAccu 0.7188 (0.7322)\t\n",
            "Epoch: [94][10/10]\tTime 0.242 (0.255)\tData 0.0384 (0.0479)\tLoss 0.5774 (0.5290)\tAccu 0.7092 (0.7304)\t\n",
            "Time 0.098\tAccu 0.6140\tLoss 0.6613\t\n",
            "Epoch: [95][1/10]\tTime 0.157 (0.157)\tData 0.0473 (0.0473)\tLoss 0.5135 (0.5135)\tAccu 0.7656 (0.7656)\t\n",
            "Epoch: [95][2/10]\tTime 0.265 (0.211)\tData 0.0505 (0.0489)\tLoss 0.5208 (0.5172)\tAccu 0.6914 (0.7285)\t\n",
            "Epoch: [95][3/10]\tTime 0.268 (0.230)\tData 0.0494 (0.0491)\tLoss 0.5688 (0.5344)\tAccu 0.6875 (0.7148)\t\n",
            "Epoch: [95][4/10]\tTime 0.268 (0.239)\tData 0.0510 (0.0496)\tLoss 0.5456 (0.5372)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [95][5/10]\tTime 0.267 (0.245)\tData 0.0489 (0.0494)\tLoss 0.5389 (0.5375)\tAccu 0.7188 (0.7156)\t\n",
            "Epoch: [95][6/10]\tTime 0.268 (0.249)\tData 0.0493 (0.0494)\tLoss 0.5377 (0.5376)\tAccu 0.7422 (0.7201)\t\n",
            "Epoch: [95][7/10]\tTime 0.268 (0.251)\tData 0.0504 (0.0495)\tLoss 0.5343 (0.5371)\tAccu 0.7148 (0.7193)\t\n",
            "Epoch: [95][8/10]\tTime 0.268 (0.254)\tData 0.0508 (0.0497)\tLoss 0.5481 (0.5385)\tAccu 0.7227 (0.7197)\t\n",
            "Epoch: [95][9/10]\tTime 0.269 (0.255)\tData 0.0484 (0.0496)\tLoss 0.5140 (0.5357)\tAccu 0.7461 (0.7227)\t\n",
            "Epoch: [95][10/10]\tTime 0.242 (0.254)\tData 0.0379 (0.0484)\tLoss 0.5315 (0.5354)\tAccu 0.6939 (0.7204)\t\n",
            "Time 0.100\tAccu 0.6540\tLoss 0.6375\t\n",
            "Epoch: [96][1/10]\tTime 0.153 (0.153)\tData 0.0478 (0.0478)\tLoss 0.5279 (0.5279)\tAccu 0.7305 (0.7305)\t\n",
            "Epoch: [96][2/10]\tTime 0.269 (0.211)\tData 0.0496 (0.0487)\tLoss 0.5367 (0.5323)\tAccu 0.7383 (0.7344)\t\n",
            "Epoch: [96][3/10]\tTime 0.269 (0.231)\tData 0.0492 (0.0489)\tLoss 0.5570 (0.5405)\tAccu 0.7148 (0.7279)\t\n",
            "Epoch: [96][4/10]\tTime 0.272 (0.241)\tData 0.0492 (0.0489)\tLoss 0.4843 (0.5265)\tAccu 0.7617 (0.7363)\t\n",
            "Epoch: [96][5/10]\tTime 0.266 (0.246)\tData 0.0488 (0.0489)\tLoss 0.4889 (0.5190)\tAccu 0.7812 (0.7453)\t\n",
            "Epoch: [96][6/10]\tTime 0.269 (0.250)\tData 0.0505 (0.0492)\tLoss 0.5883 (0.5305)\tAccu 0.6758 (0.7337)\t\n",
            "Epoch: [96][7/10]\tTime 0.268 (0.252)\tData 0.0489 (0.0491)\tLoss 0.5219 (0.5293)\tAccu 0.7344 (0.7338)\t\n",
            "Epoch: [96][8/10]\tTime 0.268 (0.254)\tData 0.0491 (0.0491)\tLoss 0.5273 (0.5291)\tAccu 0.7461 (0.7354)\t\n",
            "Epoch: [96][9/10]\tTime 0.267 (0.256)\tData 0.0499 (0.0492)\tLoss 0.5801 (0.5347)\tAccu 0.6797 (0.7292)\t\n",
            "Epoch: [96][10/10]\tTime 0.241 (0.254)\tData 0.0375 (0.0480)\tLoss 0.5258 (0.5340)\tAccu 0.7194 (0.7284)\t\n",
            "Time 0.099\tAccu 0.6480\tLoss 0.6724\t\n",
            "Epoch: [97][1/10]\tTime 0.155 (0.155)\tData 0.0490 (0.0490)\tLoss 0.4659 (0.4659)\tAccu 0.8164 (0.8164)\t\n",
            "Epoch: [97][2/10]\tTime 0.270 (0.212)\tData 0.0495 (0.0492)\tLoss 0.5150 (0.4904)\tAccu 0.7109 (0.7637)\t\n",
            "Epoch: [97][3/10]\tTime 0.266 (0.230)\tData 0.0487 (0.0491)\tLoss 0.5631 (0.5146)\tAccu 0.7070 (0.7448)\t\n",
            "Epoch: [97][4/10]\tTime 0.267 (0.239)\tData 0.0479 (0.0488)\tLoss 0.5350 (0.5197)\tAccu 0.7383 (0.7432)\t\n",
            "Epoch: [97][5/10]\tTime 0.268 (0.245)\tData 0.0480 (0.0486)\tLoss 0.5063 (0.5171)\tAccu 0.7461 (0.7438)\t\n",
            "Epoch: [97][6/10]\tTime 0.267 (0.249)\tData 0.0519 (0.0492)\tLoss 0.4973 (0.5138)\tAccu 0.7422 (0.7435)\t\n",
            "Epoch: [97][7/10]\tTime 0.268 (0.252)\tData 0.0490 (0.0491)\tLoss 0.5891 (0.5245)\tAccu 0.7148 (0.7394)\t\n",
            "Epoch: [97][8/10]\tTime 0.267 (0.253)\tData 0.0493 (0.0492)\tLoss 0.5610 (0.5291)\tAccu 0.7266 (0.7378)\t\n",
            "Epoch: [97][9/10]\tTime 0.267 (0.255)\tData 0.0517 (0.0494)\tLoss 0.5382 (0.5301)\tAccu 0.7344 (0.7374)\t\n",
            "Epoch: [97][10/10]\tTime 0.242 (0.254)\tData 0.0381 (0.0483)\tLoss 0.5576 (0.5323)\tAccu 0.7092 (0.7352)\t\n",
            "Time 0.099\tAccu 0.6560\tLoss 0.6496\t\n",
            "Epoch: [98][1/10]\tTime 0.155 (0.155)\tData 0.0476 (0.0476)\tLoss 0.5188 (0.5188)\tAccu 0.7500 (0.7500)\t\n",
            "Epoch: [98][2/10]\tTime 0.268 (0.212)\tData 0.0493 (0.0484)\tLoss 0.5985 (0.5587)\tAccu 0.6797 (0.7148)\t\n",
            "Epoch: [98][3/10]\tTime 0.268 (0.231)\tData 0.0495 (0.0488)\tLoss 0.5232 (0.5468)\tAccu 0.7305 (0.7201)\t\n",
            "Epoch: [98][4/10]\tTime 0.268 (0.240)\tData 0.0491 (0.0489)\tLoss 0.4933 (0.5335)\tAccu 0.7500 (0.7275)\t\n",
            "Epoch: [98][5/10]\tTime 0.268 (0.246)\tData 0.0487 (0.0488)\tLoss 0.5300 (0.5328)\tAccu 0.7344 (0.7289)\t\n",
            "Epoch: [98][6/10]\tTime 0.269 (0.249)\tData 0.0491 (0.0489)\tLoss 0.5474 (0.5352)\tAccu 0.7227 (0.7279)\t\n",
            "Epoch: [98][7/10]\tTime 0.265 (0.252)\tData 0.0500 (0.0490)\tLoss 0.4907 (0.5289)\tAccu 0.7578 (0.7321)\t\n",
            "Epoch: [98][8/10]\tTime 0.269 (0.254)\tData 0.0482 (0.0489)\tLoss 0.5821 (0.5355)\tAccu 0.6875 (0.7266)\t\n",
            "Epoch: [98][9/10]\tTime 0.270 (0.256)\tData 0.0494 (0.0490)\tLoss 0.5369 (0.5357)\tAccu 0.7188 (0.7257)\t\n",
            "Epoch: [98][10/10]\tTime 0.242 (0.254)\tData 0.0379 (0.0479)\tLoss 0.4992 (0.5328)\tAccu 0.7704 (0.7292)\t\n",
            "Time 0.098\tAccu 0.6180\tLoss 0.6511\t\n",
            "Epoch: [99][1/10]\tTime 0.155 (0.155)\tData 0.0475 (0.0475)\tLoss 0.5493 (0.5493)\tAccu 0.7148 (0.7148)\t\n",
            "Epoch: [99][2/10]\tTime 0.268 (0.211)\tData 0.0497 (0.0486)\tLoss 0.4968 (0.5231)\tAccu 0.7617 (0.7383)\t\n",
            "Epoch: [99][3/10]\tTime 0.268 (0.230)\tData 0.0485 (0.0486)\tLoss 0.5499 (0.5320)\tAccu 0.7188 (0.7318)\t\n",
            "Epoch: [99][4/10]\tTime 0.267 (0.239)\tData 0.0487 (0.0486)\tLoss 0.5757 (0.5429)\tAccu 0.7461 (0.7354)\t\n",
            "Epoch: [99][5/10]\tTime 0.266 (0.245)\tData 0.0490 (0.0487)\tLoss 0.5091 (0.5362)\tAccu 0.7422 (0.7367)\t\n",
            "Epoch: [99][6/10]\tTime 0.267 (0.249)\tData 0.0488 (0.0487)\tLoss 0.5501 (0.5385)\tAccu 0.7305 (0.7357)\t\n",
            "Epoch: [99][7/10]\tTime 0.268 (0.251)\tData 0.0489 (0.0487)\tLoss 0.5528 (0.5405)\tAccu 0.6992 (0.7305)\t\n",
            "Epoch: [99][8/10]\tTime 0.269 (0.253)\tData 0.0487 (0.0487)\tLoss 0.4679 (0.5315)\tAccu 0.7852 (0.7373)\t\n",
            "Epoch: [99][9/10]\tTime 0.268 (0.255)\tData 0.0494 (0.0488)\tLoss 0.5122 (0.5293)\tAccu 0.7188 (0.7352)\t\n",
            "Epoch: [99][10/10]\tTime 0.241 (0.254)\tData 0.0416 (0.0481)\tLoss 0.5646 (0.5321)\tAccu 0.6786 (0.7308)\t\n",
            "Time 0.098\tAccu 0.6420\tLoss 0.6506\t\n",
            "Epoch: [100][1/10]\tTime 0.155 (0.155)\tData 0.0472 (0.0472)\tLoss 0.5296 (0.5296)\tAccu 0.7539 (0.7539)\t\n",
            "Epoch: [100][2/10]\tTime 0.267 (0.211)\tData 0.0493 (0.0483)\tLoss 0.5201 (0.5248)\tAccu 0.7383 (0.7461)\t\n",
            "Epoch: [100][3/10]\tTime 0.268 (0.230)\tData 0.0521 (0.0495)\tLoss 0.5378 (0.5292)\tAccu 0.7852 (0.7591)\t\n",
            "Epoch: [100][4/10]\tTime 0.266 (0.239)\tData 0.0491 (0.0494)\tLoss 0.5173 (0.5262)\tAccu 0.7695 (0.7617)\t\n",
            "Epoch: [100][5/10]\tTime 0.268 (0.245)\tData 0.0490 (0.0493)\tLoss 0.5145 (0.5238)\tAccu 0.7422 (0.7578)\t\n",
            "Epoch: [100][6/10]\tTime 0.267 (0.248)\tData 0.0487 (0.0492)\tLoss 0.5207 (0.5233)\tAccu 0.7070 (0.7493)\t\n",
            "Epoch: [100][7/10]\tTime 0.268 (0.251)\tData 0.0489 (0.0492)\tLoss 0.5378 (0.5254)\tAccu 0.7344 (0.7472)\t\n",
            "Epoch: [100][8/10]\tTime 0.268 (0.253)\tData 0.0484 (0.0491)\tLoss 0.5379 (0.5270)\tAccu 0.7383 (0.7461)\t\n",
            "Epoch: [100][9/10]\tTime 0.266 (0.255)\tData 0.0495 (0.0491)\tLoss 0.5237 (0.5266)\tAccu 0.7344 (0.7448)\t\n",
            "Epoch: [100][10/10]\tTime 0.248 (0.254)\tData 0.0378 (0.0480)\tLoss 0.5220 (0.5262)\tAccu 0.7245 (0.7432)\t\n",
            "Time 0.099\tAccu 0.6540\tLoss 0.6647\t\n",
            "0.625336845963236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg5AF5kq9ydF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "f9f55b19-04c7-4fbe-c1de-958a794d4fc0"
      },
      "source": [
        "plt.plot(range(len(train_loss)), train_loss, label='Training Loss')\n",
        "plt.plot(range(len(val_loss)), val_loss, label='Validation Loss')\n",
        "\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(range(len(train_acc)), train_acc, label='Train Accuracy')\n",
        "plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy')\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfUGyf7CV9-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "f1e8f0ca-1070-4e28-ba16-c69ef1c5a342"
      },
      "source": [
        "# write your code in this cell to test your best model with the test dataset\n",
        "model = ConvNet()\n",
        "model.load_state_dict(torch.load('CNN.pth'))\n",
        "model.to(device)\n",
        "\n",
        "batch_time = AverageMeter()\n",
        "accuracies = AverageMeter()\n",
        "losses = AverageMeter()\n",
        "test_loader = DataLoader(test_set_cnn, test_batch)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "\n",
        "TN = 0\n",
        "TP = 0\n",
        "FN = 0\n",
        "FP = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    end = time.time()\n",
        "    for batch_idx, (data, labels) in enumerate(test_loader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        data = data.permute(0,3,1,2)\n",
        "        output = model(data.float())\n",
        "        # print(output[:10])\n",
        "        # output = torch.round(output)\n",
        "        # output = torch.flatten(output)\n",
        "        # print(output[:10])\n",
        "\n",
        "        loss = criterion(output, labels.unsqueeze(1)) \n",
        "        output = torch.round(torch.sigmoid(output))\n",
        "        output = torch.flatten(output)  \n",
        "        prec = accuracy(output, labels)\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            if output[i] == 0 and labels[i] == 0:\n",
        "                TN += 1\n",
        "            if output[i] == 0 and labels[i] == 1:\n",
        "                FN += 1\n",
        "            if output[i] == 1 and labels[i] == 0:\n",
        "                FP += 1\n",
        "            if output[i] == 1 and labels[i] == 1:\n",
        "                TP += 1\n",
        "\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        accuracies.update(prec, data.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        \n",
        "    print('Time {batch_time.avg:.3f}\\t'\n",
        "            'Accu {acc.avg:.4f}\\t'\n",
        "            'Loss {loss.avg:.4f}\\t'.format(\n",
        "            batch_time=batch_time, \n",
        "            acc=accuracies,\n",
        "            loss=losses))\n",
        "    \n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(\"TP = \" + str(TP) + \"   FP = \" + str(FP))\n",
        "    print(\"FN = \" + str(FN) + \"  TN = \" + str(TN))\n",
        "\n",
        "    precision = TP/(TP + FP)\n",
        "    recall = TP/(TP + FN)\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)    \n",
        "    \n",
        "    print(\"Precision = \" + str(precision))\n",
        "    print(\"Recall = \" + str(recall))\n",
        "    print(\"F1-score = \" + str(F1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time 0.123\tAccu 0.6600\tLoss 0.6335\t\n",
            "Confusion Matrix:\n",
            "TP = 7   FP = 13\n",
            "FN = 157  TN = 323\n",
            "Precision = 0.35\n",
            "Recall = 0.042682926829268296\n",
            "F1-score = 0.07608695652173914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylI0Fri5uVTB",
        "colab_type": "text"
      },
      "source": [
        "Looking at the results of MLP and CNN on the test set (confusion matrix, accuracy, F1 score, precision, recall), we can see that CNN produced slighty better results. The reason behind this difference is probably due to CNN's ability at automatically detecting image features when compared to MLP which benefits from human specified feature. The graphs, however, show that CNN benefitting from learning earlier (after less epochs) in comparision to the MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCDpcMJTHD0h",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Interpretation [10 pts.]\n",
        "\n",
        "Explicitly discuss the results that you have obtained in Question 2. <ul>\n",
        "    > Among MLP and CNN , which one do you think is better? <br>\n",
        "    > What are the weaknesses and strengths of each method?<br>\n",
        "    > Why do we use max pooling layers for CNN? What would happen if we used average pooling instead? <br>\n",
        "    > How can we interpret the weights of convolutional layers? <br>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nqh5qQDwJMK",
        "colab_type": "text"
      },
      "source": [
        "* Given a problem with images as the data input, CNNs generally perform better than a simpler MLPs, are computationally more efficient and are able to automatically detect features (no need for feature selection).\n",
        "* CNN is usually computationally more effecient (faster and uses less memory). It maintains the 2D representation of the image, guaranteeing spacial locality is maintained. It is able to extract relevant features and performs well on images. Whereas MLP is simpler to build and implement, has less components, works better on simpler data (such as sequential/vectorized data - althougth CNN can be applied to a 1D vector) \n",
        "* Max pooling is used in CNN to reduce the size on the inner convolutional layers. If pooling is not used, the number of neurons would become extrememly large which would drastically reduce computational efficiency.\n",
        "* The weights of the convolutional layers can be interpreted as weights applied to a window of the image. As the number of layers progresses, the weights become more related to relevant features of the image that the network independently identified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q8jg7nfFUe-",
        "colab_type": "text"
      },
      "source": [
        "##References\n",
        "\n",
        "Ocular Disease Recognition - ODIR5k Dataset (https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k)\n",
        "\n"
      ]
    }
  ]
}